{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42316,"status":"ok","timestamp":1688045366483,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"ky78QJ-u_r-L","outputId":"20c6ecd2-0a02-4734-e244-b182e5621db6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1688045366487,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"qhecyK14zfeW","outputId":"75643ff9-c682-4eee-e177-518698841082"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive\n"]}],"source":["cd gdrive/MyDrive/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1688045382739,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"scTKH4_j-Vsq","outputId":"1ef883d2-f12b-4fdd-ef4e-4a31d5011a87"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/lightweighted-cnn\n"]}],"source":["cd lightweighted-cnn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11955,"status":"ok","timestamp":1687888561631,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"yV8qFpgBybjX","outputId":"ea3a5efb-a362-4c56-aac5-2d71461e9d47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Updating files: 100% (974/974), done.\n","HEAD is now at 4b96eae add gitignore\n","From https://github.com/happyb321d/lightweighted-cnn\n"," * branch            HEAD       -> FETCH_HEAD\n","Updating 4b96eae..9d04b47\n","Fast-forward\n"," .gitignore                   |  1 \u001b[32m+\u001b[m\n"," train/helper/data_helpers.py | 12 \u001b[32m++++++\u001b[m\u001b[31m------\u001b[m\n"," 2 files changed, 7 insertions(+), 6 deletions(-)\n"]}],"source":["# !git reset --hard\n","# !git pull https://github.com/happyb321d/lightweighted-cnn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16203,"status":"ok","timestamp":1687977553501,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"O7zjPm87A3Je","outputId":"2af0bbde-74ad-45b8-d64a-f2d4a8367335"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6-minimal\n","Suggested packages:\n","  python3.6-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6 python3.6-minimal\n","0 upgraded, 4 newly installed, 0 to remove and 13 not upgraded.\n","Need to get 4,294 kB of archives.\n","After this operation, 22.1 MB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-minimal amd64 3.6.15-1+focal3 [569 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-minimal amd64 3.6.15-1+focal3 [1,718 kB]\n","Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-stdlib amd64 3.6.15-1+focal3 [1,758 kB]\n","Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6 amd64 3.6.15-1+focal3 [248 kB]\n","Fetched 4,294 kB in 8s (561 kB/s)\n","Selecting previously unselected package libpython3.6-minimal:amd64.\n","(Reading database ... 123069 files and directories currently installed.)\n","Preparing to unpack .../libpython3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-minimal.\n","Preparing to unpack .../python3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6-minimal (3.6.15-1+focal3) ...\n","Selecting previously unselected package libpython3.6-stdlib:amd64.\n","Preparing to unpack .../libpython3.6-stdlib_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6.\n","Preparing to unpack .../python3.6_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6 (3.6.15-1+focal3) ...\n","Setting up libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6-minimal (3.6.15-1+focal3) ...\n","Setting up libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6 (3.6.15-1+focal3) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n"]}],"source":["!apt-get install python3.6\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5294,"status":"ok","timestamp":1687977558790,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"ONhdtbYLCp7d","outputId":"8ab70417-ca8f-4c89-9eba-69ee70c11fcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3.6-lib2to3\n","The following NEW packages will be installed:\n","  python3.6-distutils python3.6-lib2to3\n","0 upgraded, 2 newly installed, 0 to remove and 13 not upgraded.\n","Need to get 308 kB of archives.\n","After this operation, 1,232 kB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-lib2to3 all 3.6.15-1+focal3 [122 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-distutils all 3.6.15-1+focal3 [187 kB]\n","Fetched 308 kB in 2s (135 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python3.6-lib2to3.\n","(Reading database ... 123680 files and directories currently installed.)\n","Preparing to unpack .../python3.6-lib2to3_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-distutils.\n","Preparing to unpack .../python3.6-distutils_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-distutils (3.6.15-1+focal3) ...\n","Setting up python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Setting up python3.6-distutils (3.6.15-1+focal3) ...\n"]}],"source":["!sudo apt-get install python3.6-distutils"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1687977558791,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"gpKisFFrG7v8","outputId":"b138c10e-baab-40a7-e318-4400c44cbc69"},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 2108k  100 2108k    0     0  11.7M      0 --:--:-- --:--:-- --:--:-- 11.7M\n"]}],"source":["!curl https://bootstrap.pypa.io/pip/3.6/get-pip.py -o get-pip.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7510,"status":"ok","timestamp":1687977566295,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"Uk00mHWkHFyz","outputId":"5881fd5d-d524-4c4c-dcf7-cb2561f7ff24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pip<22.0\n","  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n","     |████████████████████████████████| 1.7 MB 3.8 MB/s            \n","\u001b[?25hCollecting setuptools\n","  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n","     |████████████████████████████████| 952 kB 46.9 MB/s            \n","\u001b[?25hCollecting wheel\n","  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Installing collected packages: wheel, setuptools, pip\n","Successfully installed pip-21.3.1 setuptools-59.6.0 wheel-0.37.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!python3.6 get-pip.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42189,"status":"ok","timestamp":1687977608478,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"G1XZuJcHJpOf","outputId":"b9c42dd0-5294-46ad-a306-299bcabec56f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==1.14.0\n","  Downloading tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2 MB)\n","     |████████████████████████████████| 109.2 MB 7.9 kB/s            \n","\u001b[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting keras-preprocessing>=1.0.5\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","     |████████████████████████████████| 42 kB 1.4 MB/s             \n","\u001b[?25hCollecting google-pasta>=0.1.6\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","     |████████████████████████████████| 57 kB 5.0 MB/s             \n","\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n","  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n","     |████████████████████████████████| 3.1 MB 73.5 MB/s            \n","\u001b[?25hCollecting absl-py>=0.7.0\n","  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n","     |████████████████████████████████| 126 kB 66.6 MB/s            \n","\u001b[?25hCollecting gast>=0.2.0\n","  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n","Collecting numpy<2.0,>=1.14.5\n","  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n","     |████████████████████████████████| 14.8 MB 41.5 MB/s            \n","\u001b[?25hCollecting astor>=0.6.0\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Collecting six>=1.10.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting wrapt>=1.11.1\n","  Downloading wrapt-1.15.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","     |████████████████████████████████| 75 kB 4.6 MB/s             \n","\u001b[?25hCollecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","     |████████████████████████████████| 50 kB 4.9 MB/s             \n","\u001b[?25hCollecting grpcio>=1.8.6\n","  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","     |████████████████████████████████| 4.6 MB 54.9 MB/s            \n","\u001b[?25hCollecting protobuf>=3.6.1\n","  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","     |████████████████████████████████| 1.1 MB 65.9 MB/s            \n","\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n","     |████████████████████████████████| 488 kB 71.7 MB/s            \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.37.1)\n","Collecting h5py\n","  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n","     |████████████████████████████████| 4.0 MB 59.9 MB/s            \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","     |████████████████████████████████| 97 kB 7.9 MB/s             \n","\u001b[?25hCollecting werkzeug>=0.11.15\n","  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n","     |████████████████████████████████| 289 kB 73.1 MB/s            \n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (59.6.0)\n","Collecting importlib-metadata>=4.4\n","  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n","Collecting dataclasses\n","  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n","Collecting cached-property\n","  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n","Collecting typing-extensions>=3.6.4\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Building wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=451229b51889f5330949a04112240b1e9b6f612e588743eeb459bc83b47d9d73\n","  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n","Successfully built termcolor\n","Installing collected packages: zipp, typing-extensions, six, numpy, importlib-metadata, dataclasses, cached-property, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n","Successfully installed absl-py-1.4.0 astor-0.8.1 cached-property-1.5.2 dataclasses-0.8 gast-0.5.4 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 importlib-metadata-4.8.3 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 numpy-1.19.5 protobuf-3.19.6 six-1.16.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 typing-extensions-4.1.1 werkzeug-2.0.3 wrapt-1.15.0 zipp-3.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip3.6 install tensorflow==1.14.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14835,"status":"ok","timestamp":1687977623282,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"f7XlgaFqKHvc","outputId":"6888650d-746e-4b05-c3e8-c27d2fc53999"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn\n","  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n","     |████████████████████████████████| 22.2 MB 1.2 MB/s            \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n","Collecting scipy>=0.19.1\n","  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n","     |████████████████████████████████| 25.9 MB 1.2 MB/s             \n","\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n","Collecting joblib>=0.11\n","  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n","     |████████████████████████████████| 309 kB 66.3 MB/s            \n","\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n","Successfully installed joblib-1.1.1 scikit-learn-0.24.2 scipy-1.5.4 threadpoolctl-3.1.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip3.6 install scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUSFVhQaxEWe"},"outputs":[],"source":["# !pip3.6 install keras==2.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkG0X-7vKxee","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687977630280,"user_tz":-210,"elapsed":4350,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"}},"outputId":"3a7e890f-dfdf-4194-df65-6ddc6a32fefc"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}],"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","# print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CeC0PpNiNvdv","outputId":"7a884e5e-671d-431c-d9d7-692fc296da8e","executionInfo":{"status":"ok","timestamp":1687977631874,"user_tz":-210,"elapsed":1613,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyyaml\n","  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n","\u001b[?25l\r\r     |▌                               | 10 kB 26.9 MB/s eta 0:00:01\r     |█                               | 20 kB 5.1 MB/s eta 0:00:01 \r     |█▋                              | 30 kB 7.4 MB/s eta 0:00:01 \r     |██▏                             | 40 kB 3.4 MB/s eta 0:00:01 \r     |██▊                             | 51 kB 3.6 MB/s eta 0:00:01 \r     |███▎                            | 61 kB 4.2 MB/s eta 0:00:01 \r     |███▉                            | 71 kB 4.4 MB/s eta 0:00:01 \r     |████▍                           | 81 kB 4.7 MB/s eta 0:00:01 \r     |████▉                           | 92 kB 5.2 MB/s eta 0:00:01 \r     |█████▍                          | 102 kB 4.0 MB/s eta 0:00:01\r     |██████                          | 112 kB 4.0 MB/s eta 0:00:01\r     |██████▌                         | 122 kB 4.0 MB/s eta 0:00:01\r     |███████                         | 133 kB 4.0 MB/s eta 0:00:01\r     |███████▋                        | 143 kB 4.0 MB/s eta 0:00:01\r     |████████▏                       | 153 kB 4.0 MB/s eta 0:00:01\r     |████████▊                       | 163 kB 4.0 MB/s eta 0:00:01\r     |█████████▎                      | 174 kB 4.0 MB/s eta 0:00:01\r     |█████████▊                      | 184 kB 4.0 MB/s eta 0:00:01\r     |██████████▎                     | 194 kB 4.0 MB/s eta 0:00:01\r     |██████████▉                     | 204 kB 4.0 MB/s eta 0:00:01\r     |███████████▍                    | 215 kB 4.0 MB/s eta 0:00:01\r     |████████████                    | 225 kB 4.0 MB/s eta 0:00:01\r     |████████████▌                   | 235 kB 4.0 MB/s eta 0:00:01\r     |█████████████                   | 245 kB 4.0 MB/s eta 0:00:01\r     |█████████████▋                  | 256 kB 4.0 MB/s eta 0:00:01\r     |██████████████                  | 266 kB 4.0 MB/s eta 0:00:01\r     |██████████████▋                 | 276 kB 4.0 MB/s eta 0:00:01\r     |███████████████▏                | 286 kB 4.0 MB/s eta 0:00:01\r     |███████████████▊                | 296 kB 4.0 MB/s eta 0:00:01\r     |████████████████▎               | 307 kB 4.0 MB/s eta 0:00:01\r     |████████████████▉               | 317 kB 4.0 MB/s eta 0:00:01\r     |█████████████████▍              | 327 kB 4.0 MB/s eta 0:00:01\r     |██████████████████              | 337 kB 4.0 MB/s eta 0:00:01\r     |██████████████████▌             | 348 kB 4.0 MB/s eta 0:00:01\r     |███████████████████             | 358 kB 4.0 MB/s eta 0:00:01\r     |███████████████████▌            | 368 kB 4.0 MB/s eta 0:00:01\r     |████████████████████            | 378 kB 4.0 MB/s eta 0:00:01\r     |████████████████████▋           | 389 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████▏          | 399 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████▊          | 409 kB 4.0 MB/s eta 0:00:01\r     |██████████████████████▎         | 419 kB 4.0 MB/s eta 0:00:01\r     |██████████████████████▉         | 430 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████▎        | 440 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████▉        | 450 kB 4.0 MB/s eta 0:00:01\r     |████████████████████████▍       | 460 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████████       | 471 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████████▌      | 481 kB 4.0 MB/s eta 0:00:01\r     |██████████████████████████      | 491 kB 4.0 MB/s eta 0:00:01\r     |██████████████████████████▋     | 501 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████████▏    | 512 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████████▊    | 522 kB 4.0 MB/s eta 0:00:01\r     |████████████████████████████▏   | 532 kB 4.0 MB/s eta 0:00:01\r     |████████████████████████████▊   | 542 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████████████▎  | 552 kB 4.0 MB/s eta 0:00:01\r     |█████████████████████████████▉  | 563 kB 4.0 MB/s eta 0:00:01\r     |██████████████████████████████▍ | 573 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████████████ | 583 kB 4.0 MB/s eta 0:00:01\r     |███████████████████████████████▌| 593 kB 4.0 MB/s eta 0:00:01\r     |████████████████████████████████| 603 kB 4.0 MB/s            \n","\u001b[?25hInstalling collected packages: pyyaml\n","Successfully installed pyyaml-6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip3.6 install pyyaml"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1687979123664,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"2rw0iYdU1ZyQ","outputId":"f76d3444-98b4-42ef-cc00-81c2cbce3047"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/lightweighted-cnn/train\n"]}],"source":["cd train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":586572,"status":"ok","timestamp":1687991278400,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"ribTiwheKJNe","outputId":"f959449a-df0e-4212-fd24-1e917aea6e4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","***********************YML_PATH /content/gdrive/MyDrive/lightweighted-cnn/train/helper/config.yml\n","Loading data...\n","*************data_path /content/gdrive/MyDrive/lightweighted-cnn/data/tobacco-data/\n","['1.txt', '6.txt', '8.txt']\n","WARNING:tensorflow:From Nadamax_optmized_train.py:79: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use tensorflow/transform or tf.data.\n","Vocabulary Size: 18284\n","Train/Dev split: 12993/3248\n","WARNING:tensorflow:From /content/gdrive/MyDrive/lightweighted-cnn/train/text_cnn/opt_text_cnn.py:64: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Writing to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696\n","\n","2023-06-28 22:18:16.342880: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","Trainning input set: x_train, y_train 12993\n","*********Trainable PARAMETERS*********** 2495395\n","/content/gdrive/MyDrive/lightweighted-cnn/train/helper/data_helpers.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  data = np.array(data)\n","2023-06-28T22:18:16.756088: step 0, loss 1.70453, acc 0.5625, learning_rate 0.004\n","2023-06-28T22:18:16.865975: step 1, loss 7.94275, acc 0.15625, learning_rate 0.00399616\n","2023-06-28T22:18:16.963438: step 2, loss 1.7556, acc 0.53125, learning_rate 0.00399232\n","2023-06-28T22:18:17.094391: step 3, loss 3.02745, acc 0.375, learning_rate 0.00398849\n","2023-06-28T22:18:17.216309: step 4, loss 3.68296, acc 0.25, learning_rate 0.00398466\n","2023-06-28T22:18:17.323766: step 5, loss 2.88811, acc 0.46875, learning_rate 0.00398084\n","2023-06-28T22:18:17.425168: step 6, loss 2.20541, acc 0.53125, learning_rate 0.00397702\n","2023-06-28T22:18:17.526019: step 7, loss 1.48191, acc 0.5625, learning_rate 0.0039732\n","2023-06-28T22:18:17.626564: step 8, loss 1.25965, acc 0.625, learning_rate 0.00396938\n","2023-06-28T22:18:17.728038: step 9, loss 2.23684, acc 0.4375, learning_rate 0.00396557\n","2023-06-28T22:18:17.829431: step 10, loss 2.22324, acc 0.46875, learning_rate 0.00396177\n","2023-06-28T22:18:17.926355: step 11, loss 1.46738, acc 0.5625, learning_rate 0.00395797\n","2023-06-28T22:18:18.027147: step 12, loss 1.7945, acc 0.53125, learning_rate 0.00395417\n","2023-06-28T22:18:18.142042: step 13, loss 0.802418, acc 0.6875, learning_rate 0.00395037\n","2023-06-28T22:18:18.252711: step 14, loss 1.8046, acc 0.5, learning_rate 0.00394658\n","2023-06-28T22:18:18.356772: step 15, loss 1.47262, acc 0.5, learning_rate 0.00394279\n","2023-06-28T22:18:18.455020: step 16, loss 1.85514, acc 0.375, learning_rate 0.00393901\n","2023-06-28T22:18:18.551525: step 17, loss 2.01117, acc 0.34375, learning_rate 0.00393523\n","2023-06-28T22:18:18.654960: step 18, loss 1.2914, acc 0.65625, learning_rate 0.00393145\n","2023-06-28T22:18:18.753538: step 19, loss 2.03433, acc 0.5, learning_rate 0.00392768\n","2023-06-28T22:18:18.855273: step 20, loss 1.89907, acc 0.375, learning_rate 0.00392391\n","2023-06-28T22:18:18.952392: step 21, loss 0.705735, acc 0.75, learning_rate 0.00392015\n","2023-06-28T22:18:19.050415: step 22, loss 1.30235, acc 0.59375, learning_rate 0.00391638\n","2023-06-28T22:18:19.163991: step 23, loss 1.12927, acc 0.65625, learning_rate 0.00391263\n","2023-06-28T22:18:19.262032: step 24, loss 1.53059, acc 0.625, learning_rate 0.00390887\n","2023-06-28T22:18:19.369254: step 25, loss 1.40509, acc 0.5625, learning_rate 0.00390512\n","2023-06-28T22:18:19.472224: step 26, loss 1.39392, acc 0.5, learning_rate 0.00390137\n","2023-06-28T22:18:19.573353: step 27, loss 1.63316, acc 0.375, learning_rate 0.00389763\n","2023-06-28T22:18:19.671768: step 28, loss 0.615245, acc 0.71875, learning_rate 0.00389389\n","2023-06-28T22:18:19.767985: step 29, loss 1.2394, acc 0.625, learning_rate 0.00389016\n","2023-06-28T22:18:19.865514: step 30, loss 1.45225, acc 0.5625, learning_rate 0.00388642\n","2023-06-28T22:18:19.970619: step 31, loss 0.867248, acc 0.6875, learning_rate 0.0038827\n","2023-06-28T22:18:20.068929: step 32, loss 0.837645, acc 0.65625, learning_rate 0.00387897\n","2023-06-28T22:18:20.177706: step 33, loss 1.29638, acc 0.6875, learning_rate 0.00387525\n","2023-06-28T22:18:20.280714: step 34, loss 1.07105, acc 0.59375, learning_rate 0.00387153\n","2023-06-28T22:18:20.380461: step 35, loss 0.847077, acc 0.6875, learning_rate 0.00386782\n","2023-06-28T22:18:20.483311: step 36, loss 1.37174, acc 0.5625, learning_rate 0.00386411\n","2023-06-28T22:18:20.583870: step 37, loss 0.888098, acc 0.59375, learning_rate 0.0038604\n","2023-06-28T22:18:20.678483: step 38, loss 1.66533, acc 0.46875, learning_rate 0.0038567\n","2023-06-28T22:18:20.775499: step 39, loss 0.852727, acc 0.6875, learning_rate 0.003853\n","2023-06-28T22:18:20.877813: step 40, loss 1.26508, acc 0.5, learning_rate 0.00384931\n","2023-06-28T22:18:20.978601: step 41, loss 0.897643, acc 0.6875, learning_rate 0.00384561\n","2023-06-28T22:18:21.079983: step 42, loss 1.26265, acc 0.625, learning_rate 0.00384193\n","2023-06-28T22:18:21.191037: step 43, loss 0.899416, acc 0.6875, learning_rate 0.00383824\n","2023-06-28T22:18:21.304916: step 44, loss 1.23587, acc 0.65625, learning_rate 0.00383456\n","2023-06-28T22:18:21.405729: step 45, loss 1.20252, acc 0.65625, learning_rate 0.00383088\n","2023-06-28T22:18:21.508576: step 46, loss 1.60418, acc 0.59375, learning_rate 0.00382721\n","2023-06-28T22:18:21.610774: step 47, loss 1.48453, acc 0.65625, learning_rate 0.00382354\n","2023-06-28T22:18:21.705581: step 48, loss 1.12576, acc 0.5625, learning_rate 0.00381987\n","2023-06-28T22:18:21.810500: step 49, loss 1.33261, acc 0.5, learning_rate 0.00381621\n","2023-06-28T22:18:21.905858: step 50, loss 1.6644, acc 0.4375, learning_rate 0.00381255\n","2023-06-28T22:18:22.006261: step 51, loss 1.22278, acc 0.46875, learning_rate 0.0038089\n","2023-06-28T22:18:22.108755: step 52, loss 1.27395, acc 0.59375, learning_rate 0.00380524\n","2023-06-28T22:18:22.226201: step 53, loss 1.78176, acc 0.4375, learning_rate 0.0038016\n","2023-06-28T22:18:22.330377: step 54, loss 1.25738, acc 0.5625, learning_rate 0.00379795\n","2023-06-28T22:18:22.436976: step 55, loss 1.06281, acc 0.71875, learning_rate 0.00379431\n","2023-06-28T22:18:22.537024: step 56, loss 0.671729, acc 0.65625, learning_rate 0.00379067\n","2023-06-28T22:18:22.641023: step 57, loss 1.65999, acc 0.5, learning_rate 0.00378704\n","2023-06-28T22:18:22.748275: step 58, loss 1.44108, acc 0.53125, learning_rate 0.00378341\n","2023-06-28T22:18:22.850141: step 59, loss 0.953805, acc 0.59375, learning_rate 0.00377978\n","2023-06-28T22:18:22.952836: step 60, loss 0.74862, acc 0.65625, learning_rate 0.00377616\n","2023-06-28T22:18:23.048513: step 61, loss 1.06732, acc 0.65625, learning_rate 0.00377254\n","2023-06-28T22:18:23.145343: step 62, loss 0.741743, acc 0.71875, learning_rate 0.00376892\n","2023-06-28T22:18:23.257116: step 63, loss 0.844077, acc 0.71875, learning_rate 0.00376531\n","2023-06-28T22:18:23.358291: step 64, loss 0.742091, acc 0.71875, learning_rate 0.0037617\n","2023-06-28T22:18:23.455026: step 65, loss 0.786154, acc 0.59375, learning_rate 0.00375809\n","2023-06-28T22:18:23.562878: step 66, loss 1.16942, acc 0.5, learning_rate 0.00375449\n","2023-06-28T22:18:23.654366: step 67, loss 0.959157, acc 0.78125, learning_rate 0.00375089\n","2023-06-28T22:18:23.747019: step 68, loss 0.835136, acc 0.625, learning_rate 0.0037473\n","2023-06-28T22:18:23.847360: step 69, loss 0.983791, acc 0.625, learning_rate 0.00374371\n","2023-06-28T22:18:23.947721: step 70, loss 1.41378, acc 0.625, learning_rate 0.00374012\n","2023-06-28T22:18:24.040705: step 71, loss 1.36923, acc 0.4375, learning_rate 0.00373653\n","2023-06-28T22:18:24.145904: step 72, loss 1.05519, acc 0.625, learning_rate 0.00373295\n","2023-06-28T22:18:24.246688: step 73, loss 1.00123, acc 0.59375, learning_rate 0.00372938\n","2023-06-28T22:18:24.348973: step 74, loss 0.632301, acc 0.78125, learning_rate 0.0037258\n","2023-06-28T22:18:24.447424: step 75, loss 0.648813, acc 0.75, learning_rate 0.00372223\n","2023-06-28T22:18:24.549529: step 76, loss 1.28341, acc 0.5625, learning_rate 0.00371867\n","2023-06-28T22:18:24.652255: step 77, loss 0.83581, acc 0.75, learning_rate 0.0037151\n","2023-06-28T22:18:24.751736: step 78, loss 1.19477, acc 0.59375, learning_rate 0.00371154\n","2023-06-28T22:18:24.844509: step 79, loss 0.964122, acc 0.625, learning_rate 0.00370799\n","2023-06-28T22:18:24.956479: step 80, loss 0.849791, acc 0.71875, learning_rate 0.00370443\n","2023-06-28T22:18:25.108009: step 81, loss 0.968911, acc 0.71875, learning_rate 0.00370089\n","2023-06-28T22:18:25.285729: step 82, loss 0.773552, acc 0.6875, learning_rate 0.00369734\n","2023-06-28T22:18:25.453825: step 83, loss 1.28207, acc 0.59375, learning_rate 0.0036938\n","2023-06-28T22:18:25.625987: step 84, loss 0.912739, acc 0.625, learning_rate 0.00369026\n","2023-06-28T22:18:25.795084: step 85, loss 0.69304, acc 0.8125, learning_rate 0.00368672\n","2023-06-28T22:18:25.962298: step 86, loss 0.707195, acc 0.625, learning_rate 0.00368319\n","2023-06-28T22:18:26.143210: step 87, loss 1.27022, acc 0.5625, learning_rate 0.00367966\n","2023-06-28T22:18:26.323037: step 88, loss 0.843164, acc 0.6875, learning_rate 0.00367614\n","2023-06-28T22:18:26.496839: step 89, loss 0.903358, acc 0.8125, learning_rate 0.00367262\n","2023-06-28T22:18:26.655772: step 90, loss 0.72334, acc 0.6875, learning_rate 0.0036691\n","2023-06-28T22:18:26.820581: step 91, loss 0.820005, acc 0.59375, learning_rate 0.00366559\n","2023-06-28T22:18:26.989034: step 92, loss 0.894232, acc 0.6875, learning_rate 0.00366207\n","2023-06-28T22:18:27.179852: step 93, loss 0.923424, acc 0.6875, learning_rate 0.00365857\n","2023-06-28T22:18:27.366150: step 94, loss 1.0656, acc 0.6875, learning_rate 0.00365506\n","2023-06-28T22:18:27.549088: step 95, loss 0.820465, acc 0.71875, learning_rate 0.00365156\n","2023-06-28T22:18:27.732479: step 96, loss 0.93749, acc 0.6875, learning_rate 0.00364807\n","2023-06-28T22:18:27.900100: step 97, loss 0.873921, acc 0.59375, learning_rate 0.00364457\n","2023-06-28T22:18:28.081959: step 98, loss 0.596162, acc 0.6875, learning_rate 0.00364108\n","2023-06-28T22:18:28.257602: step 99, loss 0.624146, acc 0.71875, learning_rate 0.00363759\n","\n","Evaluation:\n","2023-06-28 22:18:28.313428: W tensorflow/core/framework/allocator.cc:107] Allocation of 66519040 exceeds 10% of system memory.\n","2023-06-28 22:18:28.355984: W tensorflow/core/framework/allocator.cc:107] Allocation of 60802560 exceeds 10% of system memory.\n","2023-06-28 22:18:28.356229: W tensorflow/core/framework/allocator.cc:107] Allocation of 56125440 exceeds 10% of system memory.\n","2023-06-28 22:18:28.964487: W tensorflow/core/framework/allocator.cc:107] Allocation of 59243520 exceeds 10% of system memory.\n","2023-06-28T22:18:29.589534: step 100, loss 0.659132, acc 0.740148\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-100\n","\n","2023-06-28T22:18:29.906704: step 100, loss 0.675153, acc 0.75, learning_rate 0.00363411\n","2023-06-28T22:18:30.054719: step 101, loss 0.714439, acc 0.75, learning_rate 0.00363063\n","2023-06-28T22:18:30.226901: step 102, loss 0.953098, acc 0.6875, learning_rate 0.00362716\n","2023-06-28T22:18:30.376645: step 103, loss 1.15878, acc 0.6875, learning_rate 0.00362368\n","2023-06-28T22:18:30.534917: step 104, loss 0.744362, acc 0.75, learning_rate 0.00362021\n","2023-06-28T22:18:30.713216: step 105, loss 1.34977, acc 0.5, learning_rate 0.00361675\n","2023-06-28T22:18:30.912824: step 106, loss 1.18972, acc 0.53125, learning_rate 0.00361328\n","2023-06-28T22:18:31.100587: step 107, loss 1.26742, acc 0.5, learning_rate 0.00360982\n","2023-06-28T22:18:31.268027: step 108, loss 0.632205, acc 0.78125, learning_rate 0.00360637\n","2023-06-28T22:18:31.464888: step 109, loss 0.80758, acc 0.5625, learning_rate 0.00360292\n","2023-06-28T22:18:31.675272: step 110, loss 0.967556, acc 0.65625, learning_rate 0.00359947\n","2023-06-28T22:18:31.866779: step 111, loss 0.949611, acc 0.65625, learning_rate 0.00359602\n","2023-06-28T22:18:32.095575: step 112, loss 0.720439, acc 0.6875, learning_rate 0.00359258\n","2023-06-28T22:18:32.278756: step 113, loss 1.45455, acc 0.5, learning_rate 0.00358914\n","2023-06-28T22:18:32.454954: step 114, loss 0.920027, acc 0.625, learning_rate 0.0035857\n","2023-06-28T22:18:32.684327: step 115, loss 0.657634, acc 0.78125, learning_rate 0.00358227\n","2023-06-28T22:18:32.869451: step 116, loss 0.595982, acc 0.78125, learning_rate 0.00357884\n","2023-06-28T22:18:33.121625: step 117, loss 0.666131, acc 0.75, learning_rate 0.00357542\n","2023-06-28T22:18:33.326051: step 118, loss 0.974748, acc 0.6875, learning_rate 0.00357199\n","2023-06-28T22:18:33.523378: step 119, loss 0.693756, acc 0.6875, learning_rate 0.00356858\n","2023-06-28T22:18:33.713060: step 120, loss 0.63413, acc 0.78125, learning_rate 0.00356516\n","2023-06-28T22:18:33.935622: step 121, loss 1.08469, acc 0.5625, learning_rate 0.00356175\n","2023-06-28T22:18:34.119610: step 122, loss 1.07645, acc 0.65625, learning_rate 0.00355834\n","2023-06-28T22:18:34.348934: step 123, loss 1.10827, acc 0.625, learning_rate 0.00355493\n","2023-06-28T22:18:34.534504: step 124, loss 0.863027, acc 0.71875, learning_rate 0.00355153\n","2023-06-28T22:18:34.758813: step 125, loss 0.728638, acc 0.78125, learning_rate 0.00354813\n","2023-06-28T22:18:34.955742: step 126, loss 0.955925, acc 0.71875, learning_rate 0.00354474\n","2023-06-28T22:18:35.137003: step 127, loss 0.687112, acc 0.75, learning_rate 0.00354135\n","2023-06-28T22:18:35.388758: step 128, loss 0.581221, acc 0.75, learning_rate 0.00353796\n","2023-06-28T22:18:35.571913: step 129, loss 0.871004, acc 0.6875, learning_rate 0.00353457\n","2023-06-28T22:18:35.762506: step 130, loss 0.967816, acc 0.6875, learning_rate 0.00353119\n","2023-06-28T22:18:35.939060: step 131, loss 0.80792, acc 0.78125, learning_rate 0.00352781\n","2023-06-28T22:18:36.120444: step 132, loss 0.614155, acc 0.75, learning_rate 0.00352444\n","2023-06-28T22:18:36.305777: step 133, loss 0.657766, acc 0.78125, learning_rate 0.00352107\n","2023-06-28T22:18:36.489225: step 134, loss 0.749611, acc 0.65625, learning_rate 0.0035177\n","2023-06-28T22:18:36.655071: step 135, loss 0.671257, acc 0.65625, learning_rate 0.00351433\n","2023-06-28T22:18:36.855869: step 136, loss 1.17853, acc 0.6875, learning_rate 0.00351097\n","2023-06-28T22:18:37.030156: step 137, loss 0.804513, acc 0.6875, learning_rate 0.00350761\n","2023-06-28T22:18:37.232343: step 138, loss 0.66595, acc 0.6875, learning_rate 0.00350426\n","2023-06-28T22:18:37.421913: step 139, loss 1.27461, acc 0.6875, learning_rate 0.0035009\n","2023-06-28T22:18:37.601560: step 140, loss 0.84006, acc 0.59375, learning_rate 0.00349756\n","2023-06-28T22:18:37.781375: step 141, loss 0.489485, acc 0.84375, learning_rate 0.00349421\n","2023-06-28T22:18:37.955115: step 142, loss 0.760107, acc 0.625, learning_rate 0.00349087\n","2023-06-28T22:18:38.131040: step 143, loss 0.875476, acc 0.6875, learning_rate 0.00348753\n","2023-06-28T22:18:38.296547: step 144, loss 0.763211, acc 0.6875, learning_rate 0.00348419\n","2023-06-28T22:18:38.456900: step 145, loss 0.520085, acc 0.84375, learning_rate 0.00348086\n","2023-06-28T22:18:38.634701: step 146, loss 0.604071, acc 0.78125, learning_rate 0.00347753\n","2023-06-28T22:18:38.814343: step 147, loss 0.756695, acc 0.6875, learning_rate 0.00347421\n","2023-06-28T22:18:39.035170: step 148, loss 0.553938, acc 0.6875, learning_rate 0.00347088\n","2023-06-28T22:18:39.223175: step 149, loss 0.549736, acc 0.75, learning_rate 0.00346756\n","2023-06-28T22:18:39.391830: step 150, loss 0.991665, acc 0.625, learning_rate 0.00346425\n","2023-06-28T22:18:39.555244: step 151, loss 0.511421, acc 0.8125, learning_rate 0.00346094\n","2023-06-28T22:18:39.704489: step 152, loss 0.766388, acc 0.71875, learning_rate 0.00345763\n","2023-06-28T22:18:39.856685: step 153, loss 0.679176, acc 0.75, learning_rate 0.00345432\n","2023-06-28T22:18:40.000051: step 154, loss 0.914019, acc 0.6875, learning_rate 0.00345102\n","2023-06-28T22:18:40.162130: step 155, loss 0.537396, acc 0.875, learning_rate 0.00344772\n","2023-06-28T22:18:40.325286: step 156, loss 1.35741, acc 0.6875, learning_rate 0.00344442\n","2023-06-28T22:18:40.486511: step 157, loss 0.962783, acc 0.6875, learning_rate 0.00344113\n","2023-06-28T22:18:40.645019: step 158, loss 0.722238, acc 0.75, learning_rate 0.00343784\n","2023-06-28T22:18:40.819024: step 159, loss 0.652215, acc 0.78125, learning_rate 0.00343455\n","2023-06-28T22:18:40.983057: step 160, loss 0.621729, acc 0.71875, learning_rate 0.00343127\n","2023-06-28T22:18:41.156995: step 161, loss 0.969768, acc 0.59375, learning_rate 0.00342799\n","2023-06-28T22:18:41.315355: step 162, loss 1.09527, acc 0.71875, learning_rate 0.00342471\n","2023-06-28T22:18:41.487597: step 163, loss 0.38576, acc 0.875, learning_rate 0.00342144\n","2023-06-28T22:18:41.648037: step 164, loss 0.982522, acc 0.78125, learning_rate 0.00341817\n","2023-06-28T22:18:41.796292: step 165, loss 0.529722, acc 0.84375, learning_rate 0.0034149\n","2023-06-28T22:18:41.953830: step 166, loss 0.569257, acc 0.75, learning_rate 0.00341164\n","2023-06-28T22:18:42.114948: step 167, loss 0.574546, acc 0.75, learning_rate 0.00340838\n","2023-06-28T22:18:42.265542: step 168, loss 0.852919, acc 0.625, learning_rate 0.00340512\n","2023-06-28T22:18:42.436043: step 169, loss 0.610499, acc 0.75, learning_rate 0.00340186\n","2023-06-28T22:18:42.589136: step 170, loss 0.820597, acc 0.71875, learning_rate 0.00339861\n","2023-06-28T22:18:42.755344: step 171, loss 0.573917, acc 0.75, learning_rate 0.00339536\n","2023-06-28T22:18:42.919014: step 172, loss 0.820919, acc 0.6875, learning_rate 0.00339212\n","2023-06-28T22:18:43.076100: step 173, loss 0.853117, acc 0.65625, learning_rate 0.00338888\n","2023-06-28T22:18:43.233182: step 174, loss 0.983563, acc 0.71875, learning_rate 0.00338564\n","2023-06-28T22:18:43.401284: step 175, loss 0.317322, acc 0.875, learning_rate 0.0033824\n","2023-06-28T22:18:43.568942: step 176, loss 0.88565, acc 0.78125, learning_rate 0.00337917\n","2023-06-28T22:18:43.723932: step 177, loss 0.747256, acc 0.78125, learning_rate 0.00337594\n","2023-06-28T22:18:43.896483: step 178, loss 0.73736, acc 0.71875, learning_rate 0.00337272\n","2023-06-28T22:18:44.059291: step 179, loss 0.339191, acc 0.875, learning_rate 0.00336949\n","2023-06-28T22:18:44.219250: step 180, loss 0.717716, acc 0.71875, learning_rate 0.00336628\n","2023-06-28T22:18:44.373504: step 181, loss 0.704606, acc 0.75, learning_rate 0.00336306\n","2023-06-28T22:18:44.524324: step 182, loss 0.712562, acc 0.71875, learning_rate 0.00335985\n","2023-06-28T22:18:44.675285: step 183, loss 0.730263, acc 0.65625, learning_rate 0.00335664\n","2023-06-28T22:18:44.770594: step 184, loss 0.662111, acc 0.75, learning_rate 0.00335343\n","2023-06-28T22:18:44.879903: step 185, loss 1.08094, acc 0.625, learning_rate 0.00335023\n","2023-06-28T22:18:44.975648: step 186, loss 0.558726, acc 0.78125, learning_rate 0.00334703\n","2023-06-28T22:18:45.067886: step 187, loss 1.07923, acc 0.5625, learning_rate 0.00334383\n","2023-06-28T22:18:45.167220: step 188, loss 0.613603, acc 0.75, learning_rate 0.00334063\n","2023-06-28T22:18:45.264670: step 189, loss 0.819205, acc 0.71875, learning_rate 0.00333744\n","2023-06-28T22:18:45.364459: step 190, loss 0.805891, acc 0.65625, learning_rate 0.00333426\n","2023-06-28T22:18:45.472792: step 191, loss 0.770527, acc 0.84375, learning_rate 0.00333107\n","2023-06-28T22:18:45.567325: step 192, loss 0.412899, acc 0.8125, learning_rate 0.00332789\n","2023-06-28T22:18:45.659578: step 193, loss 0.475814, acc 0.8125, learning_rate 0.00332471\n","2023-06-28T22:18:45.757782: step 194, loss 0.746043, acc 0.75, learning_rate 0.00332154\n","2023-06-28T22:18:45.853795: step 195, loss 0.800582, acc 0.65625, learning_rate 0.00331836\n","2023-06-28T22:18:45.958601: step 196, loss 0.5815, acc 0.75, learning_rate 0.00331519\n","2023-06-28T22:18:46.065952: step 197, loss 0.780354, acc 0.53125, learning_rate 0.00331203\n","2023-06-28T22:18:46.166077: step 198, loss 0.664213, acc 0.6875, learning_rate 0.00330887\n","2023-06-28T22:18:46.267300: step 199, loss 0.84957, acc 0.65625, learning_rate 0.00330571\n","\n","Evaluation:\n","2023-06-28 22:18:46.271760: W tensorflow/core/framework/allocator.cc:107] Allocation of 66519040 exceeds 10% of system memory.\n","2023-06-28T22:18:47.017059: step 200, loss 0.601031, acc 0.769089\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-200\n","\n","2023-06-28T22:18:47.202542: step 200, loss 0.352047, acc 0.875, learning_rate 0.00330255\n","2023-06-28T22:18:47.311949: step 201, loss 0.736103, acc 0.71875, learning_rate 0.0032994\n","2023-06-28T22:18:47.437614: step 202, loss 1.33865, acc 0.6875, learning_rate 0.00329625\n","2023-06-28T22:18:47.535152: step 203, loss 0.455085, acc 0.8125, learning_rate 0.0032931\n","2023-06-28T22:18:47.636768: step 204, loss 1.25327, acc 0.59375, learning_rate 0.00328995\n","2023-06-28T22:18:47.736632: step 205, loss 0.822491, acc 0.75, learning_rate 0.00328681\n","2023-06-28T22:18:47.838016: step 206, loss 0.76367, acc 0.75, learning_rate 0.00328368\n","2023-06-28T22:18:47.936788: step 207, loss 0.984716, acc 0.5625, learning_rate 0.00328054\n","2023-06-28T22:18:48.036948: step 208, loss 0.496766, acc 0.84375, learning_rate 0.00327741\n","2023-06-28T22:18:48.141351: step 209, loss 0.765813, acc 0.65625, learning_rate 0.00327428\n","2023-06-28T22:18:48.240662: step 210, loss 0.777981, acc 0.71875, learning_rate 0.00327116\n","2023-06-28T22:18:48.335053: step 211, loss 0.771335, acc 0.6875, learning_rate 0.00326803\n","2023-06-28T22:18:48.444996: step 212, loss 0.695992, acc 0.78125, learning_rate 0.00326491\n","2023-06-28T22:18:48.549115: step 213, loss 0.873913, acc 0.6875, learning_rate 0.0032618\n","2023-06-28T22:18:48.648471: step 214, loss 1.15771, acc 0.59375, learning_rate 0.00325868\n","2023-06-28T22:18:48.748426: step 215, loss 0.297387, acc 0.875, learning_rate 0.00325557\n","2023-06-28T22:18:48.844981: step 216, loss 0.930076, acc 0.6875, learning_rate 0.00325247\n","2023-06-28T22:18:48.945970: step 217, loss 0.472012, acc 0.875, learning_rate 0.00324936\n","2023-06-28T22:18:49.046088: step 218, loss 0.735631, acc 0.71875, learning_rate 0.00324626\n","2023-06-28T22:18:49.150976: step 219, loss 0.825422, acc 0.6875, learning_rate 0.00324316\n","2023-06-28T22:18:49.259225: step 220, loss 0.440581, acc 0.8125, learning_rate 0.00324007\n","2023-06-28T22:18:49.421252: step 221, loss 0.43808, acc 0.84375, learning_rate 0.00323698\n","2023-06-28T22:18:49.601116: step 222, loss 0.775019, acc 0.6875, learning_rate 0.00323389\n","2023-06-28T22:18:49.774140: step 223, loss 0.873087, acc 0.6875, learning_rate 0.0032308\n","2023-06-28T22:18:49.954462: step 224, loss 1.09779, acc 0.65625, learning_rate 0.00322772\n","2023-06-28T22:18:50.143094: step 225, loss 0.752594, acc 0.65625, learning_rate 0.00322464\n","2023-06-28T22:18:50.305043: step 226, loss 0.442318, acc 0.875, learning_rate 0.00322156\n","2023-06-28T22:18:50.475924: step 227, loss 0.655013, acc 0.78125, learning_rate 0.00321849\n","2023-06-28T22:18:50.646181: step 228, loss 0.745842, acc 0.84375, learning_rate 0.00321542\n","2023-06-28T22:18:50.813579: step 229, loss 0.912185, acc 0.78125, learning_rate 0.00321235\n","2023-06-28T22:18:50.984331: step 230, loss 0.66647, acc 0.71875, learning_rate 0.00320929\n","2023-06-28T22:18:51.151866: step 231, loss 0.635067, acc 0.71875, learning_rate 0.00320622\n","2023-06-28T22:18:51.353768: step 232, loss 0.603378, acc 0.75, learning_rate 0.00320317\n","2023-06-28T22:18:51.527260: step 233, loss 0.624871, acc 0.78125, learning_rate 0.00320011\n","2023-06-28T22:18:51.689317: step 234, loss 0.943379, acc 0.75, learning_rate 0.00319706\n","2023-06-28T22:18:51.849272: step 235, loss 1.35258, acc 0.6875, learning_rate 0.00319401\n","2023-06-28T22:18:52.005407: step 236, loss 0.847209, acc 0.71875, learning_rate 0.00319096\n","2023-06-28T22:18:52.173477: step 237, loss 1.14972, acc 0.59375, learning_rate 0.00318792\n","2023-06-28T22:18:52.340520: step 238, loss 0.761451, acc 0.6875, learning_rate 0.00318488\n","2023-06-28T22:18:52.507423: step 239, loss 0.881561, acc 0.71875, learning_rate 0.00318184\n","2023-06-28T22:18:52.678636: step 240, loss 0.602811, acc 0.71875, learning_rate 0.00317881\n","2023-06-28T22:18:52.850303: step 241, loss 0.464121, acc 0.875, learning_rate 0.00317577\n","2023-06-28T22:18:53.019555: step 242, loss 0.626025, acc 0.8125, learning_rate 0.00317274\n","2023-06-28T22:18:53.194463: step 243, loss 0.872362, acc 0.59375, learning_rate 0.00316972\n","2023-06-28T22:18:53.368115: step 244, loss 0.451639, acc 0.78125, learning_rate 0.0031667\n","2023-06-28T22:18:53.538719: step 245, loss 0.467994, acc 0.8125, learning_rate 0.00316368\n","2023-06-28T22:18:53.698919: step 246, loss 0.812654, acc 0.65625, learning_rate 0.00316066\n","2023-06-28T22:18:53.852505: step 247, loss 0.487752, acc 0.78125, learning_rate 0.00315765\n","2023-06-28T22:18:54.056295: step 248, loss 0.678136, acc 0.75, learning_rate 0.00315464\n","2023-06-28T22:18:54.197212: step 249, loss 0.719787, acc 0.6875, learning_rate 0.00315163\n","2023-06-28T22:18:54.351058: step 250, loss 0.745494, acc 0.6875, learning_rate 0.00314862\n","2023-06-28T22:18:54.505915: step 251, loss 0.690452, acc 0.625, learning_rate 0.00314562\n","2023-06-28T22:18:54.653942: step 252, loss 0.761305, acc 0.6875, learning_rate 0.00314262\n","2023-06-28T22:18:54.823629: step 253, loss 0.711688, acc 0.71875, learning_rate 0.00313963\n","2023-06-28T22:18:54.986202: step 254, loss 0.645021, acc 0.625, learning_rate 0.00313663\n","2023-06-28T22:18:55.151444: step 255, loss 0.851886, acc 0.71875, learning_rate 0.00313364\n","2023-06-28T22:18:55.315344: step 256, loss 0.750842, acc 0.6875, learning_rate 0.00313066\n","2023-06-28T22:18:55.469942: step 257, loss 0.871232, acc 0.65625, learning_rate 0.00312767\n","2023-06-28T22:18:55.618104: step 258, loss 0.334862, acc 0.875, learning_rate 0.00312469\n","2023-06-28T22:18:55.764700: step 259, loss 0.883483, acc 0.71875, learning_rate 0.00312171\n","2023-06-28T22:18:55.922482: step 260, loss 0.819506, acc 0.71875, learning_rate 0.00311874\n","2023-06-28T22:18:56.081223: step 261, loss 0.88419, acc 0.71875, learning_rate 0.00311576\n","2023-06-28T22:18:56.254370: step 262, loss 0.697611, acc 0.6875, learning_rate 0.0031128\n","2023-06-28T22:18:56.412635: step 263, loss 0.547222, acc 0.75, learning_rate 0.00310983\n","2023-06-28T22:18:56.576557: step 264, loss 0.972078, acc 0.5625, learning_rate 0.00310687\n","2023-06-28T22:18:56.728245: step 265, loss 0.720133, acc 0.6875, learning_rate 0.0031039\n","2023-06-28T22:18:56.888224: step 266, loss 0.694166, acc 0.8125, learning_rate 0.00310095\n","2023-06-28T22:18:57.061727: step 267, loss 0.582567, acc 0.625, learning_rate 0.00309799\n","2023-06-28T22:18:57.236814: step 268, loss 0.964516, acc 0.65625, learning_rate 0.00309504\n","2023-06-28T22:18:57.402128: step 269, loss 1.13558, acc 0.625, learning_rate 0.00309209\n","2023-06-28T22:18:57.557472: step 270, loss 0.658048, acc 0.75, learning_rate 0.00308914\n","2023-06-28T22:18:57.714809: step 271, loss 0.657442, acc 0.8125, learning_rate 0.0030862\n","2023-06-28T22:18:57.872362: step 272, loss 0.579028, acc 0.78125, learning_rate 0.00308326\n","2023-06-28T22:18:58.021080: step 273, loss 0.442783, acc 0.75, learning_rate 0.00308032\n","2023-06-28T22:18:58.186945: step 274, loss 0.641901, acc 0.78125, learning_rate 0.00307739\n","2023-06-28T22:18:58.339920: step 275, loss 0.624611, acc 0.78125, learning_rate 0.00307446\n","2023-06-28T22:18:58.494747: step 276, loss 0.648693, acc 0.8125, learning_rate 0.00307153\n","2023-06-28T22:18:58.660406: step 277, loss 0.761788, acc 0.625, learning_rate 0.0030686\n","2023-06-28T22:18:58.811229: step 278, loss 0.646096, acc 0.8125, learning_rate 0.00306568\n","2023-06-28T22:18:58.945926: step 279, loss 0.646927, acc 0.71875, learning_rate 0.00306276\n","2023-06-28T22:18:59.094150: step 280, loss 0.387812, acc 0.875, learning_rate 0.00305984\n","2023-06-28T22:18:59.253319: step 281, loss 0.814368, acc 0.71875, learning_rate 0.00305693\n","2023-06-28T22:18:59.410946: step 282, loss 0.607699, acc 0.75, learning_rate 0.00305402\n","2023-06-28T22:18:59.571226: step 283, loss 0.720249, acc 0.78125, learning_rate 0.00305111\n","2023-06-28T22:18:59.727012: step 284, loss 1.07558, acc 0.625, learning_rate 0.0030482\n","2023-06-28T22:18:59.831294: step 285, loss 0.960736, acc 0.78125, learning_rate 0.0030453\n","2023-06-28T22:18:59.924434: step 286, loss 0.530127, acc 0.8125, learning_rate 0.0030424\n","2023-06-28T22:19:00.021528: step 287, loss 0.721222, acc 0.75, learning_rate 0.0030395\n","2023-06-28T22:19:00.120702: step 288, loss 0.660857, acc 0.75, learning_rate 0.00303661\n","2023-06-28T22:19:00.215680: step 289, loss 0.651646, acc 0.71875, learning_rate 0.00303371\n","2023-06-28T22:19:00.318600: step 290, loss 0.533275, acc 0.78125, learning_rate 0.00303083\n","2023-06-28T22:19:00.409805: step 291, loss 0.587525, acc 0.75, learning_rate 0.00302794\n","2023-06-28T22:19:00.505612: step 292, loss 0.589203, acc 0.75, learning_rate 0.00302506\n","2023-06-28T22:19:00.620632: step 293, loss 0.538687, acc 0.78125, learning_rate 0.00302218\n","2023-06-28T22:19:00.732723: step 294, loss 0.410116, acc 0.78125, learning_rate 0.0030193\n","2023-06-28T22:19:00.846208: step 295, loss 0.584538, acc 0.75, learning_rate 0.00301642\n","2023-06-28T22:19:00.952119: step 296, loss 0.943325, acc 0.59375, learning_rate 0.00301355\n","2023-06-28T22:19:01.050243: step 297, loss 0.406134, acc 0.84375, learning_rate 0.00301068\n","2023-06-28T22:19:01.148470: step 298, loss 1.00419, acc 0.71875, learning_rate 0.00300782\n","2023-06-28T22:19:01.246509: step 299, loss 0.438339, acc 0.84375, learning_rate 0.00300496\n","\n","Evaluation:\n","2023-06-28T22:19:01.961728: step 300, loss 0.558313, acc 0.769704\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-300\n","\n","2023-06-28T22:19:02.149524: step 300, loss 0.813433, acc 0.625, learning_rate 0.00300209\n","2023-06-28T22:19:02.245328: step 301, loss 0.895249, acc 0.59375, learning_rate 0.00299924\n","2023-06-28T22:19:02.347067: step 302, loss 0.839796, acc 0.625, learning_rate 0.00299638\n","2023-06-28T22:19:02.446938: step 303, loss 1.23686, acc 0.5625, learning_rate 0.00299353\n","2023-06-28T22:19:02.549989: step 304, loss 0.684544, acc 0.78125, learning_rate 0.00299068\n","2023-06-28T22:19:02.646342: step 305, loss 0.696132, acc 0.65625, learning_rate 0.00298783\n","2023-06-28T22:19:02.745574: step 306, loss 0.815441, acc 0.6875, learning_rate 0.00298499\n","2023-06-28T22:19:02.843504: step 307, loss 0.494466, acc 0.78125, learning_rate 0.00298215\n","2023-06-28T22:19:02.953101: step 308, loss 1.00945, acc 0.78125, learning_rate 0.00297931\n","2023-06-28T22:19:03.045367: step 309, loss 0.624359, acc 0.71875, learning_rate 0.00297648\n","2023-06-28T22:19:03.151440: step 310, loss 0.786426, acc 0.75, learning_rate 0.00297365\n","2023-06-28T22:19:03.248647: step 311, loss 0.709409, acc 0.71875, learning_rate 0.00297082\n","2023-06-28T22:19:03.353592: step 312, loss 0.540382, acc 0.8125, learning_rate 0.00296799\n","2023-06-28T22:19:03.448661: step 313, loss 0.692317, acc 0.6875, learning_rate 0.00296516\n","2023-06-28T22:19:03.547569: step 314, loss 0.539807, acc 0.75, learning_rate 0.00296234\n","2023-06-28T22:19:03.662210: step 315, loss 0.703551, acc 0.6875, learning_rate 0.00295953\n","2023-06-28T22:19:03.756793: step 316, loss 0.618471, acc 0.78125, learning_rate 0.00295671\n","2023-06-28T22:19:03.852945: step 317, loss 0.471897, acc 0.78125, learning_rate 0.0029539\n","2023-06-28T22:19:03.959756: step 318, loss 0.825409, acc 0.6875, learning_rate 0.00295109\n","2023-06-28T22:19:04.056525: step 319, loss 1.42251, acc 0.5, learning_rate 0.00294828\n","2023-06-28T22:19:04.158230: step 320, loss 0.657373, acc 0.75, learning_rate 0.00294547\n","2023-06-28T22:19:04.252537: step 321, loss 0.640751, acc 0.75, learning_rate 0.00294267\n","2023-06-28T22:19:04.351208: step 322, loss 0.854982, acc 0.6875, learning_rate 0.00293987\n","2023-06-28T22:19:04.445946: step 323, loss 0.592331, acc 0.84375, learning_rate 0.00293708\n","2023-06-28T22:19:04.547629: step 324, loss 0.643484, acc 0.75, learning_rate 0.00293428\n","2023-06-28T22:19:04.651304: step 325, loss 0.789075, acc 0.65625, learning_rate 0.00293149\n","2023-06-28T22:19:04.749048: step 326, loss 0.428456, acc 0.875, learning_rate 0.0029287\n","2023-06-28T22:19:04.846355: step 327, loss 0.898003, acc 0.5625, learning_rate 0.00292592\n","2023-06-28T22:19:04.952860: step 328, loss 0.618231, acc 0.71875, learning_rate 0.00292314\n","2023-06-28T22:19:05.047187: step 329, loss 0.478942, acc 0.75, learning_rate 0.00292036\n","2023-06-28T22:19:05.152428: step 330, loss 0.431078, acc 0.84375, learning_rate 0.00291758\n","2023-06-28T22:19:05.254569: step 331, loss 0.977581, acc 0.71875, learning_rate 0.00291481\n","2023-06-28T22:19:05.353674: step 332, loss 0.767166, acc 0.71875, learning_rate 0.00291203\n","2023-06-28T22:19:05.450823: step 333, loss 0.725459, acc 0.75, learning_rate 0.00290927\n","2023-06-28T22:19:05.551569: step 334, loss 0.713902, acc 0.65625, learning_rate 0.0029065\n","2023-06-28T22:19:05.650015: step 335, loss 0.632049, acc 0.75, learning_rate 0.00290374\n","2023-06-28T22:19:05.764414: step 336, loss 0.768625, acc 0.75, learning_rate 0.00290097\n","2023-06-28T22:19:05.860344: step 337, loss 0.541793, acc 0.8125, learning_rate 0.00289822\n","2023-06-28T22:19:05.965920: step 338, loss 0.92204, acc 0.71875, learning_rate 0.00289546\n","2023-06-28T22:19:06.070068: step 339, loss 0.3962, acc 0.8125, learning_rate 0.00289271\n","2023-06-28T22:19:06.171044: step 340, loss 0.666406, acc 0.6875, learning_rate 0.00288996\n","2023-06-28T22:19:06.268271: step 341, loss 0.897586, acc 0.5625, learning_rate 0.00288721\n","2023-06-28T22:19:06.374226: step 342, loss 0.766958, acc 0.625, learning_rate 0.00288447\n","2023-06-28T22:19:06.470634: step 343, loss 0.708713, acc 0.65625, learning_rate 0.00288173\n","2023-06-28T22:19:06.572813: step 344, loss 0.847656, acc 0.71875, learning_rate 0.00287899\n","2023-06-28T22:19:06.674668: step 345, loss 0.654288, acc 0.78125, learning_rate 0.00287625\n","2023-06-28T22:19:06.796890: step 346, loss 0.669954, acc 0.6875, learning_rate 0.00287352\n","2023-06-28T22:19:06.894293: step 347, loss 0.701841, acc 0.78125, learning_rate 0.00287079\n","2023-06-28T22:19:06.998259: step 348, loss 0.450654, acc 0.71875, learning_rate 0.00286806\n","2023-06-28T22:19:07.105325: step 349, loss 0.500868, acc 0.78125, learning_rate 0.00286533\n","2023-06-28T22:19:07.201884: step 350, loss 0.534713, acc 0.78125, learning_rate 0.00286261\n","2023-06-28T22:19:07.296987: step 351, loss 0.560033, acc 0.8125, learning_rate 0.00285989\n","2023-06-28T22:19:07.403574: step 352, loss 0.652881, acc 0.75, learning_rate 0.00285717\n","2023-06-28T22:19:07.499515: step 353, loss 0.632589, acc 0.8125, learning_rate 0.00285446\n","2023-06-28T22:19:07.594183: step 354, loss 0.695251, acc 0.8125, learning_rate 0.00285174\n","2023-06-28T22:19:07.700271: step 355, loss 0.57405, acc 0.84375, learning_rate 0.00284903\n","2023-06-28T22:19:07.794326: step 356, loss 0.592432, acc 0.78125, learning_rate 0.00284633\n","2023-06-28T22:19:07.889965: step 357, loss 0.604001, acc 0.75, learning_rate 0.00284362\n","2023-06-28T22:19:07.985006: step 358, loss 0.716782, acc 0.71875, learning_rate 0.00284092\n","2023-06-28T22:19:08.088554: step 359, loss 0.666331, acc 0.6875, learning_rate 0.00283822\n","2023-06-28T22:19:08.181479: step 360, loss 0.597411, acc 0.8125, learning_rate 0.00283553\n","2023-06-28T22:19:08.279431: step 361, loss 0.579911, acc 0.71875, learning_rate 0.00283283\n","2023-06-28T22:19:08.376854: step 362, loss 0.669159, acc 0.71875, learning_rate 0.00283014\n","2023-06-28T22:19:08.475106: step 363, loss 0.926006, acc 0.6875, learning_rate 0.00282745\n","2023-06-28T22:19:08.576307: step 364, loss 0.424686, acc 0.84375, learning_rate 0.00282477\n","2023-06-28T22:19:08.669045: step 365, loss 0.760183, acc 0.71875, learning_rate 0.00282209\n","2023-06-28T22:19:08.763985: step 366, loss 0.759377, acc 0.71875, learning_rate 0.00281941\n","2023-06-28T22:19:08.860944: step 367, loss 0.608064, acc 0.78125, learning_rate 0.00281673\n","2023-06-28T22:19:08.985688: step 368, loss 0.792988, acc 0.78125, learning_rate 0.00281405\n","2023-06-28T22:19:09.095298: step 369, loss 0.681783, acc 0.75, learning_rate 0.00281138\n","2023-06-28T22:19:09.192568: step 370, loss 0.461628, acc 0.875, learning_rate 0.00280871\n","2023-06-28T22:19:09.298474: step 371, loss 0.658673, acc 0.6875, learning_rate 0.00280604\n","2023-06-28T22:19:09.396459: step 372, loss 0.443659, acc 0.8125, learning_rate 0.00280338\n","2023-06-28T22:19:09.499060: step 373, loss 0.703552, acc 0.6875, learning_rate 0.00280072\n","2023-06-28T22:19:09.598132: step 374, loss 0.49289, acc 0.75, learning_rate 0.00279806\n","2023-06-28T22:19:09.693671: step 375, loss 0.782789, acc 0.78125, learning_rate 0.0027954\n","2023-06-28T22:19:09.831302: step 376, loss 0.514807, acc 0.84375, learning_rate 0.00279275\n","2023-06-28T22:19:10.009120: step 377, loss 0.82063, acc 0.6875, learning_rate 0.0027901\n","2023-06-28T22:19:10.200943: step 378, loss 0.8697, acc 0.71875, learning_rate 0.00278745\n","2023-06-28T22:19:10.371781: step 379, loss 0.41766, acc 0.90625, learning_rate 0.0027848\n","2023-06-28T22:19:10.540091: step 380, loss 0.493864, acc 0.8125, learning_rate 0.00278216\n","2023-06-28T22:19:10.710832: step 381, loss 0.908362, acc 0.71875, learning_rate 0.00277952\n","2023-06-28T22:19:10.883432: step 382, loss 0.578644, acc 0.8125, learning_rate 0.00277688\n","2023-06-28T22:19:11.054880: step 383, loss 0.564285, acc 0.78125, learning_rate 0.00277424\n","2023-06-28T22:19:11.230169: step 384, loss 0.617352, acc 0.71875, learning_rate 0.00277161\n","2023-06-28T22:19:11.401698: step 385, loss 0.50834, acc 0.8125, learning_rate 0.00276898\n","2023-06-28T22:19:11.576714: step 386, loss 0.271682, acc 0.96875, learning_rate 0.00276635\n","2023-06-28T22:19:11.740089: step 387, loss 0.884461, acc 0.6875, learning_rate 0.00276372\n","2023-06-28T22:19:11.914911: step 388, loss 0.576433, acc 0.8125, learning_rate 0.0027611\n","2023-06-28T22:19:12.079349: step 389, loss 0.807436, acc 0.6875, learning_rate 0.00275848\n","2023-06-28T22:19:12.268749: step 390, loss 0.627255, acc 0.6875, learning_rate 0.00275586\n","2023-06-28T22:19:12.435813: step 391, loss 0.745801, acc 0.75, learning_rate 0.00275325\n","2023-06-28T22:19:12.582309: step 392, loss 0.861366, acc 0.5625, learning_rate 0.00275064\n","2023-06-28T22:19:12.775935: step 393, loss 0.858895, acc 0.65625, learning_rate 0.00274803\n","2023-06-28T22:19:12.947891: step 394, loss 0.842091, acc 0.75, learning_rate 0.00274542\n","2023-06-28T22:19:13.118977: step 395, loss 0.654857, acc 0.75, learning_rate 0.00274281\n","2023-06-28T22:19:13.291121: step 396, loss 0.984751, acc 0.65625, learning_rate 0.00274021\n","2023-06-28T22:19:13.472111: step 397, loss 0.68948, acc 0.65625, learning_rate 0.00273761\n","2023-06-28T22:19:13.633647: step 398, loss 0.509313, acc 0.78125, learning_rate 0.00273501\n","2023-06-28T22:19:13.822608: step 399, loss 0.630079, acc 0.78125, learning_rate 0.00273242\n","\n","Evaluation:\n","2023-06-28T22:19:15.080069: step 400, loss 0.530208, acc 0.787562\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-400\n","\n","2023-06-28T22:19:15.381632: step 400, loss 0.462378, acc 0.84375, learning_rate 0.00272983\n","2023-06-28T22:19:15.537084: step 401, loss 0.339728, acc 0.90625, learning_rate 0.00272724\n","2023-06-28T22:19:15.686491: step 402, loss 0.792781, acc 0.71875, learning_rate 0.00272465\n","2023-06-28T22:19:15.845073: step 403, loss 0.782728, acc 0.71875, learning_rate 0.00272207\n","2023-06-28T22:19:16.003115: step 404, loss 0.488625, acc 0.78125, learning_rate 0.00271948\n","2023-06-28T22:19:16.164677: step 405, loss 0.56014, acc 0.75, learning_rate 0.00271691\n","2023-06-28T22:19:16.277923: step 406, loss 0.0100693, acc 1, learning_rate 0.00271433\n","2023-06-28T22:19:16.427262: step 407, loss 0.390632, acc 0.84375, learning_rate 0.00271175\n","2023-06-28T22:19:16.598987: step 408, loss 0.985822, acc 0.625, learning_rate 0.00270918\n","2023-06-28T22:19:16.768065: step 409, loss 0.772356, acc 0.65625, learning_rate 0.00270661\n","2023-06-28T22:19:16.931422: step 410, loss 0.73864, acc 0.78125, learning_rate 0.00270405\n","2023-06-28T22:19:17.094013: step 411, loss 0.696159, acc 0.6875, learning_rate 0.00270148\n","2023-06-28T22:19:17.260054: step 412, loss 0.583366, acc 0.78125, learning_rate 0.00269892\n","2023-06-28T22:19:17.420626: step 413, loss 0.351067, acc 0.875, learning_rate 0.00269636\n","2023-06-28T22:19:17.585837: step 414, loss 0.778042, acc 0.625, learning_rate 0.00269381\n","2023-06-28T22:19:17.731938: step 415, loss 0.332076, acc 0.84375, learning_rate 0.00269125\n","2023-06-28T22:19:17.899509: step 416, loss 0.640634, acc 0.875, learning_rate 0.0026887\n","2023-06-28T22:19:18.077930: step 417, loss 0.694021, acc 0.59375, learning_rate 0.00268615\n","2023-06-28T22:19:18.243431: step 418, loss 0.455282, acc 0.84375, learning_rate 0.0026836\n","2023-06-28T22:19:18.401737: step 419, loss 0.690626, acc 0.6875, learning_rate 0.00268106\n","2023-06-28T22:19:18.551179: step 420, loss 0.585444, acc 0.8125, learning_rate 0.00267852\n","2023-06-28T22:19:18.700226: step 421, loss 0.318177, acc 0.875, learning_rate 0.00267598\n","2023-06-28T22:19:18.850754: step 422, loss 0.548886, acc 0.71875, learning_rate 0.00267344\n","2023-06-28T22:19:19.016514: step 423, loss 0.566356, acc 0.78125, learning_rate 0.00267091\n","2023-06-28T22:19:19.184660: step 424, loss 0.490338, acc 0.875, learning_rate 0.00266838\n","2023-06-28T22:19:19.336886: step 425, loss 0.614163, acc 0.78125, learning_rate 0.00266585\n","2023-06-28T22:19:19.494068: step 426, loss 0.982027, acc 0.625, learning_rate 0.00266332\n","2023-06-28T22:19:19.659802: step 427, loss 0.403409, acc 0.75, learning_rate 0.0026608\n","2023-06-28T22:19:19.812709: step 428, loss 0.242058, acc 0.9375, learning_rate 0.00265828\n","2023-06-28T22:19:19.970296: step 429, loss 0.601937, acc 0.65625, learning_rate 0.00265576\n","2023-06-28T22:19:20.162917: step 430, loss 0.40953, acc 0.875, learning_rate 0.00265324\n","2023-06-28T22:19:20.288571: step 431, loss 0.420707, acc 0.84375, learning_rate 0.00265073\n","2023-06-28T22:19:20.384079: step 432, loss 0.474502, acc 0.8125, learning_rate 0.00264822\n","2023-06-28T22:19:20.488343: step 433, loss 0.310409, acc 0.90625, learning_rate 0.00264571\n","2023-06-28T22:19:20.598605: step 434, loss 0.371233, acc 0.84375, learning_rate 0.0026432\n","2023-06-28T22:19:20.703413: step 435, loss 0.50895, acc 0.84375, learning_rate 0.0026407\n","2023-06-28T22:19:20.803000: step 436, loss 0.5168, acc 0.78125, learning_rate 0.00263819\n","2023-06-28T22:19:20.904009: step 437, loss 0.411559, acc 0.8125, learning_rate 0.0026357\n","2023-06-28T22:19:21.005168: step 438, loss 0.534676, acc 0.8125, learning_rate 0.0026332\n","2023-06-28T22:19:21.114657: step 439, loss 0.423493, acc 0.84375, learning_rate 0.0026307\n","2023-06-28T22:19:21.221941: step 440, loss 0.698642, acc 0.71875, learning_rate 0.00262821\n","2023-06-28T22:19:21.324573: step 441, loss 0.651477, acc 0.625, learning_rate 0.00262572\n","2023-06-28T22:19:21.423553: step 442, loss 0.389912, acc 0.875, learning_rate 0.00262324\n","2023-06-28T22:19:21.542725: step 443, loss 0.556618, acc 0.78125, learning_rate 0.00262075\n","2023-06-28T22:19:21.647414: step 444, loss 0.371831, acc 0.875, learning_rate 0.00261827\n","2023-06-28T22:19:21.751495: step 445, loss 0.498139, acc 0.875, learning_rate 0.00261579\n","2023-06-28T22:19:21.887500: step 446, loss 0.351354, acc 0.84375, learning_rate 0.00261331\n","2023-06-28T22:19:21.989445: step 447, loss 0.419981, acc 0.8125, learning_rate 0.00261084\n","2023-06-28T22:19:22.089586: step 448, loss 0.595509, acc 0.625, learning_rate 0.00260837\n","2023-06-28T22:19:22.189865: step 449, loss 0.499677, acc 0.78125, learning_rate 0.0026059\n","2023-06-28T22:19:22.290124: step 450, loss 0.932044, acc 0.65625, learning_rate 0.00260343\n","2023-06-28T22:19:22.396308: step 451, loss 0.446354, acc 0.84375, learning_rate 0.00260096\n","2023-06-28T22:19:22.495460: step 452, loss 0.326728, acc 0.84375, learning_rate 0.0025985\n","2023-06-28T22:19:22.629904: step 453, loss 0.424907, acc 0.78125, learning_rate 0.00259604\n","2023-06-28T22:19:22.730540: step 454, loss 0.866553, acc 0.75, learning_rate 0.00259358\n","2023-06-28T22:19:22.829535: step 455, loss 0.542397, acc 0.75, learning_rate 0.00259113\n","2023-06-28T22:19:22.932194: step 456, loss 0.771111, acc 0.65625, learning_rate 0.00258867\n","2023-06-28T22:19:23.033507: step 457, loss 0.385057, acc 0.8125, learning_rate 0.00258622\n","2023-06-28T22:19:23.138017: step 458, loss 0.215438, acc 0.9375, learning_rate 0.00258378\n","2023-06-28T22:19:23.239463: step 459, loss 0.636131, acc 0.78125, learning_rate 0.00258133\n","2023-06-28T22:19:23.339563: step 460, loss 0.479015, acc 0.75, learning_rate 0.00257889\n","2023-06-28T22:19:23.452596: step 461, loss 0.53471, acc 0.75, learning_rate 0.00257645\n","2023-06-28T22:19:23.561209: step 462, loss 0.379598, acc 0.78125, learning_rate 0.00257401\n","2023-06-28T22:19:23.661013: step 463, loss 0.335343, acc 0.75, learning_rate 0.00257157\n","2023-06-28T22:19:23.752259: step 464, loss 0.467207, acc 0.8125, learning_rate 0.00256914\n","2023-06-28T22:19:23.847195: step 465, loss 0.462656, acc 0.8125, learning_rate 0.00256671\n","2023-06-28T22:19:23.952064: step 466, loss 0.687136, acc 0.71875, learning_rate 0.00256428\n","2023-06-28T22:19:24.045669: step 467, loss 0.47631, acc 0.78125, learning_rate 0.00256185\n","2023-06-28T22:19:24.142915: step 468, loss 0.752456, acc 0.6875, learning_rate 0.00255943\n","2023-06-28T22:19:24.239047: step 469, loss 0.51662, acc 0.75, learning_rate 0.00255701\n","2023-06-28T22:19:24.335686: step 470, loss 0.421958, acc 0.8125, learning_rate 0.00255459\n","2023-06-28T22:19:24.431348: step 471, loss 0.362274, acc 0.84375, learning_rate 0.00255217\n","2023-06-28T22:19:24.536105: step 472, loss 0.145054, acc 0.9375, learning_rate 0.00254975\n","2023-06-28T22:19:24.640895: step 473, loss 0.399488, acc 0.8125, learning_rate 0.00254734\n","2023-06-28T22:19:24.748965: step 474, loss 0.29209, acc 0.9375, learning_rate 0.00254493\n","2023-06-28T22:19:24.849841: step 475, loss 0.661091, acc 0.75, learning_rate 0.00254252\n","2023-06-28T22:19:24.946136: step 476, loss 0.563822, acc 0.8125, learning_rate 0.00254012\n","2023-06-28T22:19:25.042588: step 477, loss 0.536275, acc 0.875, learning_rate 0.00253772\n","2023-06-28T22:19:25.149916: step 478, loss 0.751379, acc 0.84375, learning_rate 0.00253532\n","2023-06-28T22:19:25.247969: step 479, loss 0.462564, acc 0.78125, learning_rate 0.00253292\n","2023-06-28T22:19:25.347030: step 480, loss 0.623247, acc 0.78125, learning_rate 0.00253052\n","2023-06-28T22:19:25.446319: step 481, loss 0.790873, acc 0.71875, learning_rate 0.00252813\n","2023-06-28T22:19:25.544147: step 482, loss 0.557855, acc 0.71875, learning_rate 0.00252574\n","2023-06-28T22:19:25.650403: step 483, loss 0.631944, acc 0.78125, learning_rate 0.00252335\n","2023-06-28T22:19:25.745463: step 484, loss 0.661525, acc 0.75, learning_rate 0.00252096\n","2023-06-28T22:19:25.842599: step 485, loss 0.321305, acc 0.875, learning_rate 0.00251858\n","2023-06-28T22:19:25.937633: step 486, loss 0.426702, acc 0.8125, learning_rate 0.0025162\n","2023-06-28T22:19:26.037083: step 487, loss 0.317973, acc 0.875, learning_rate 0.00251382\n","2023-06-28T22:19:26.138272: step 488, loss 0.273059, acc 0.84375, learning_rate 0.00251144\n","2023-06-28T22:19:26.235538: step 489, loss 0.50763, acc 0.71875, learning_rate 0.00250907\n","2023-06-28T22:19:26.339307: step 490, loss 0.471675, acc 0.8125, learning_rate 0.0025067\n","2023-06-28T22:19:26.436057: step 491, loss 0.489592, acc 0.75, learning_rate 0.00250433\n","2023-06-28T22:19:26.550471: step 492, loss 0.830087, acc 0.78125, learning_rate 0.00250196\n","2023-06-28T22:19:26.658771: step 493, loss 0.336231, acc 0.84375, learning_rate 0.00249959\n","2023-06-28T22:19:26.754970: step 494, loss 0.381145, acc 0.875, learning_rate 0.00249723\n","2023-06-28T22:19:26.848728: step 495, loss 0.584074, acc 0.6875, learning_rate 0.00249487\n","2023-06-28T22:19:26.950229: step 496, loss 0.425071, acc 0.78125, learning_rate 0.00249251\n","2023-06-28T22:19:27.041964: step 497, loss 0.465744, acc 0.78125, learning_rate 0.00249016\n","2023-06-28T22:19:27.143199: step 498, loss 0.496895, acc 0.78125, learning_rate 0.0024878\n","2023-06-28T22:19:27.239593: step 499, loss 0.405683, acc 0.875, learning_rate 0.00248545\n","\n","Evaluation:\n","2023-06-28T22:19:27.938501: step 500, loss 0.517949, acc 0.791256\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-500\n","\n","2023-06-28T22:19:28.126286: step 500, loss 0.354062, acc 0.90625, learning_rate 0.0024831\n","2023-06-28T22:19:28.237065: step 501, loss 0.426792, acc 0.8125, learning_rate 0.00248076\n","2023-06-28T22:19:28.331779: step 502, loss 0.633507, acc 0.78125, learning_rate 0.00247841\n","2023-06-28T22:19:28.429206: step 503, loss 0.265348, acc 0.90625, learning_rate 0.00247607\n","2023-06-28T22:19:28.526140: step 504, loss 0.566979, acc 0.75, learning_rate 0.00247373\n","2023-06-28T22:19:28.622028: step 505, loss 0.310769, acc 0.875, learning_rate 0.00247139\n","2023-06-28T22:19:28.728715: step 506, loss 0.507866, acc 0.75, learning_rate 0.00246906\n","2023-06-28T22:19:28.829941: step 507, loss 0.600473, acc 0.75, learning_rate 0.00246673\n","2023-06-28T22:19:28.929258: step 508, loss 0.350416, acc 0.9375, learning_rate 0.0024644\n","2023-06-28T22:19:29.030526: step 509, loss 0.477014, acc 0.90625, learning_rate 0.00246207\n","2023-06-28T22:19:29.128263: step 510, loss 0.812557, acc 0.71875, learning_rate 0.00245974\n","2023-06-28T22:19:29.225326: step 511, loss 0.366417, acc 0.84375, learning_rate 0.00245742\n","2023-06-28T22:19:29.324325: step 512, loss 0.386606, acc 0.8125, learning_rate 0.0024551\n","2023-06-28T22:19:29.444675: step 513, loss 0.490961, acc 0.75, learning_rate 0.00245278\n","2023-06-28T22:19:29.541765: step 514, loss 0.643199, acc 0.8125, learning_rate 0.00245046\n","2023-06-28T22:19:29.635948: step 515, loss 0.317108, acc 0.90625, learning_rate 0.00244815\n","2023-06-28T22:19:29.764731: step 516, loss 0.789753, acc 0.8125, learning_rate 0.00244583\n","2023-06-28T22:19:29.860657: step 517, loss 0.498777, acc 0.78125, learning_rate 0.00244352\n","2023-06-28T22:19:29.957375: step 518, loss 0.373204, acc 0.84375, learning_rate 0.00244122\n","2023-06-28T22:19:30.054853: step 519, loss 0.431491, acc 0.84375, learning_rate 0.00243891\n","2023-06-28T22:19:30.150040: step 520, loss 0.47129, acc 0.75, learning_rate 0.00243661\n","2023-06-28T22:19:30.260340: step 521, loss 0.703191, acc 0.78125, learning_rate 0.00243431\n","2023-06-28T22:19:30.412665: step 522, loss 0.6841, acc 0.75, learning_rate 0.00243201\n","2023-06-28T22:19:30.560324: step 523, loss 0.446883, acc 0.90625, learning_rate 0.00242971\n","2023-06-28T22:19:30.717507: step 524, loss 0.508477, acc 0.84375, learning_rate 0.00242742\n","2023-06-28T22:19:30.884486: step 525, loss 0.346049, acc 0.84375, learning_rate 0.00242513\n","2023-06-28T22:19:31.052282: step 526, loss 0.673116, acc 0.75, learning_rate 0.00242284\n","2023-06-28T22:19:31.217715: step 527, loss 0.498947, acc 0.84375, learning_rate 0.00242055\n","2023-06-28T22:19:31.399929: step 528, loss 0.478021, acc 0.78125, learning_rate 0.00241827\n","2023-06-28T22:19:31.579313: step 529, loss 0.631606, acc 0.78125, learning_rate 0.00241598\n","2023-06-28T22:19:31.743837: step 530, loss 0.611717, acc 0.75, learning_rate 0.0024137\n","2023-06-28T22:19:31.924975: step 531, loss 0.533206, acc 0.75, learning_rate 0.00241142\n","2023-06-28T22:19:32.082940: step 532, loss 0.507441, acc 0.78125, learning_rate 0.00240915\n","2023-06-28T22:19:32.238985: step 533, loss 0.255321, acc 0.90625, learning_rate 0.00240688\n","2023-06-28T22:19:32.401161: step 534, loss 0.389602, acc 0.84375, learning_rate 0.0024046\n","2023-06-28T22:19:32.552541: step 535, loss 0.670203, acc 0.625, learning_rate 0.00240233\n","2023-06-28T22:19:32.722585: step 536, loss 0.723035, acc 0.6875, learning_rate 0.00240007\n","2023-06-28T22:19:32.897064: step 537, loss 0.575739, acc 0.78125, learning_rate 0.0023978\n","2023-06-28T22:19:33.055071: step 538, loss 0.480882, acc 0.78125, learning_rate 0.00239554\n","2023-06-28T22:19:33.227752: step 539, loss 0.346647, acc 0.875, learning_rate 0.00239328\n","2023-06-28T22:19:33.389951: step 540, loss 0.534266, acc 0.71875, learning_rate 0.00239102\n","2023-06-28T22:19:33.568161: step 541, loss 0.235462, acc 0.90625, learning_rate 0.00238877\n","2023-06-28T22:19:33.738770: step 542, loss 0.550654, acc 0.78125, learning_rate 0.00238651\n","2023-06-28T22:19:33.912267: step 543, loss 0.713457, acc 0.78125, learning_rate 0.00238426\n","2023-06-28T22:19:34.105737: step 544, loss 0.344319, acc 0.8125, learning_rate 0.00238201\n","2023-06-28T22:19:34.281123: step 545, loss 0.544817, acc 0.71875, learning_rate 0.00237976\n","2023-06-28T22:19:34.440350: step 546, loss 0.369215, acc 0.8125, learning_rate 0.00237752\n","2023-06-28T22:19:34.602888: step 547, loss 0.699836, acc 0.75, learning_rate 0.00237528\n","2023-06-28T22:19:34.762823: step 548, loss 0.461662, acc 0.71875, learning_rate 0.00237304\n","2023-06-28T22:19:34.929568: step 549, loss 0.626065, acc 0.6875, learning_rate 0.0023708\n","2023-06-28T22:19:35.100582: step 550, loss 0.621953, acc 0.65625, learning_rate 0.00236856\n","2023-06-28T22:19:35.263750: step 551, loss 0.418053, acc 0.84375, learning_rate 0.00236633\n","2023-06-28T22:19:35.425354: step 552, loss 0.683057, acc 0.71875, learning_rate 0.0023641\n","2023-06-28T22:19:35.598329: step 553, loss 0.56847, acc 0.75, learning_rate 0.00236187\n","2023-06-28T22:19:35.759848: step 554, loss 0.372244, acc 0.84375, learning_rate 0.00235964\n","2023-06-28T22:19:35.924006: step 555, loss 0.531885, acc 0.75, learning_rate 0.00235742\n","2023-06-28T22:19:36.090358: step 556, loss 0.361907, acc 0.875, learning_rate 0.00235519\n","2023-06-28T22:19:36.260873: step 557, loss 0.572451, acc 0.75, learning_rate 0.00235297\n","2023-06-28T22:19:36.421147: step 558, loss 0.568137, acc 0.75, learning_rate 0.00235075\n","2023-06-28T22:19:36.583351: step 559, loss 0.333484, acc 0.8125, learning_rate 0.00234854\n","2023-06-28T22:19:36.743502: step 560, loss 0.457347, acc 0.875, learning_rate 0.00234632\n","2023-06-28T22:19:36.906925: step 561, loss 0.339214, acc 0.90625, learning_rate 0.00234411\n","2023-06-28T22:19:37.060331: step 562, loss 0.865804, acc 0.6875, learning_rate 0.0023419\n","2023-06-28T22:19:37.222203: step 563, loss 0.485225, acc 0.8125, learning_rate 0.00233969\n","2023-06-28T22:19:37.389546: step 564, loss 0.409273, acc 0.84375, learning_rate 0.00233749\n","2023-06-28T22:19:37.558435: step 565, loss 0.583387, acc 0.75, learning_rate 0.00233529\n","2023-06-28T22:19:37.720865: step 566, loss 0.209625, acc 0.875, learning_rate 0.00233308\n","2023-06-28T22:19:37.890128: step 567, loss 0.517704, acc 0.84375, learning_rate 0.00233089\n","2023-06-28T22:19:38.033656: step 568, loss 0.435819, acc 0.8125, learning_rate 0.00232869\n","2023-06-28T22:19:38.208650: step 569, loss 0.524527, acc 0.84375, learning_rate 0.00232649\n","2023-06-28T22:19:38.373584: step 570, loss 0.558994, acc 0.78125, learning_rate 0.0023243\n","2023-06-28T22:19:38.519737: step 571, loss 0.899625, acc 0.75, learning_rate 0.00232211\n","2023-06-28T22:19:38.672748: step 572, loss 0.45443, acc 0.78125, learning_rate 0.00231992\n","2023-06-28T22:19:38.835354: step 573, loss 0.331984, acc 0.875, learning_rate 0.00231774\n","2023-06-28T22:19:39.035267: step 574, loss 0.432669, acc 0.8125, learning_rate 0.00231555\n","2023-06-28T22:19:39.217083: step 575, loss 0.773499, acc 0.78125, learning_rate 0.00231337\n","2023-06-28T22:19:39.384012: step 576, loss 0.559999, acc 0.71875, learning_rate 0.00231119\n","2023-06-28T22:19:39.552751: step 577, loss 0.411539, acc 0.78125, learning_rate 0.00230902\n","2023-06-28T22:19:39.706359: step 578, loss 0.441848, acc 0.78125, learning_rate 0.00230684\n","2023-06-28T22:19:39.878812: step 579, loss 0.261778, acc 0.90625, learning_rate 0.00230467\n","2023-06-28T22:19:40.033997: step 580, loss 0.398134, acc 0.8125, learning_rate 0.0023025\n","2023-06-28T22:19:40.211048: step 581, loss 0.230708, acc 0.96875, learning_rate 0.00230033\n","2023-06-28T22:19:40.360315: step 582, loss 0.412189, acc 0.90625, learning_rate 0.00229816\n","2023-06-28T22:19:40.527868: step 583, loss 0.612283, acc 0.84375, learning_rate 0.002296\n","2023-06-28T22:19:40.687696: step 584, loss 0.385429, acc 0.9375, learning_rate 0.00229384\n","2023-06-28T22:19:40.853149: step 585, loss 0.205367, acc 0.96875, learning_rate 0.00229168\n","2023-06-28T22:19:40.954140: step 586, loss 0.51359, acc 0.84375, learning_rate 0.00228952\n","2023-06-28T22:19:41.053393: step 587, loss 0.395349, acc 0.875, learning_rate 0.00228736\n","2023-06-28T22:19:41.153459: step 588, loss 0.741497, acc 0.6875, learning_rate 0.00228521\n","2023-06-28T22:19:41.258097: step 589, loss 0.583276, acc 0.71875, learning_rate 0.00228306\n","2023-06-28T22:19:41.364502: step 590, loss 0.341262, acc 0.84375, learning_rate 0.00228091\n","2023-06-28T22:19:41.465128: step 591, loss 0.189799, acc 0.9375, learning_rate 0.00227876\n","2023-06-28T22:19:41.560334: step 592, loss 0.629592, acc 0.75, learning_rate 0.00227661\n","2023-06-28T22:19:41.658312: step 593, loss 0.558958, acc 0.75, learning_rate 0.00227447\n","2023-06-28T22:19:41.748846: step 594, loss 0.508134, acc 0.875, learning_rate 0.00227233\n","2023-06-28T22:19:41.844483: step 595, loss 0.402226, acc 0.875, learning_rate 0.00227019\n","2023-06-28T22:19:41.938896: step 596, loss 0.497696, acc 0.8125, learning_rate 0.00226805\n","2023-06-28T22:19:42.039523: step 597, loss 0.270115, acc 0.90625, learning_rate 0.00226592\n","2023-06-28T22:19:42.143236: step 598, loss 0.204086, acc 0.90625, learning_rate 0.00226379\n","2023-06-28T22:19:42.254476: step 599, loss 0.869795, acc 0.75, learning_rate 0.00226166\n","\n","Evaluation:\n","2023-06-28T22:19:42.953847: step 600, loss 0.554717, acc 0.800493\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-600\n","\n","2023-06-28T22:19:43.168640: step 600, loss 0.331043, acc 0.90625, learning_rate 0.00225953\n","2023-06-28T22:19:43.272337: step 601, loss 0.40331, acc 0.84375, learning_rate 0.0022574\n","2023-06-28T22:19:43.365935: step 602, loss 0.43391, acc 0.75, learning_rate 0.00225528\n","2023-06-28T22:19:43.463135: step 603, loss 0.389952, acc 0.84375, learning_rate 0.00225315\n","2023-06-28T22:19:43.562065: step 604, loss 0.462722, acc 0.8125, learning_rate 0.00225103\n","2023-06-28T22:19:43.654191: step 605, loss 0.789082, acc 0.71875, learning_rate 0.00224892\n","2023-06-28T22:19:43.755486: step 606, loss 0.419445, acc 0.8125, learning_rate 0.0022468\n","2023-06-28T22:19:43.853898: step 607, loss 1.00005, acc 0.71875, learning_rate 0.00224469\n","2023-06-28T22:19:43.955679: step 608, loss 0.297854, acc 0.84375, learning_rate 0.00224257\n","2023-06-28T22:19:44.055918: step 609, loss 0.388947, acc 0.875, learning_rate 0.00224046\n","2023-06-28T22:19:44.153655: step 610, loss 0.60086, acc 0.71875, learning_rate 0.00223836\n","2023-06-28T22:19:44.249186: step 611, loss 0.338705, acc 0.84375, learning_rate 0.00223625\n","2023-06-28T22:19:44.357745: step 612, loss 0.397263, acc 0.875, learning_rate 0.00223415\n","2023-06-28T22:19:44.451229: step 613, loss 0.504808, acc 0.6875, learning_rate 0.00223205\n","2023-06-28T22:19:44.550602: step 614, loss 0.393251, acc 0.8125, learning_rate 0.00222995\n","2023-06-28T22:19:44.652723: step 615, loss 0.484224, acc 0.78125, learning_rate 0.00222785\n","2023-06-28T22:19:44.750726: step 616, loss 0.398406, acc 0.84375, learning_rate 0.00222575\n","2023-06-28T22:19:44.848718: step 617, loss 0.220831, acc 0.90625, learning_rate 0.00222366\n","2023-06-28T22:19:44.943962: step 618, loss 0.504148, acc 0.75, learning_rate 0.00222157\n","2023-06-28T22:19:45.050827: step 619, loss 0.532487, acc 0.84375, learning_rate 0.00221948\n","2023-06-28T22:19:45.150546: step 620, loss 0.617421, acc 0.84375, learning_rate 0.00221739\n","2023-06-28T22:19:45.250594: step 621, loss 0.435193, acc 0.84375, learning_rate 0.00221531\n","2023-06-28T22:19:45.362026: step 622, loss 0.570705, acc 0.875, learning_rate 0.00221323\n","2023-06-28T22:19:45.463575: step 623, loss 0.540929, acc 0.84375, learning_rate 0.00221115\n","2023-06-28T22:19:45.556659: step 624, loss 0.681115, acc 0.75, learning_rate 0.00220907\n","2023-06-28T22:19:45.663701: step 625, loss 0.391439, acc 0.8125, learning_rate 0.00220699\n","2023-06-28T22:19:45.765926: step 626, loss 0.445299, acc 0.84375, learning_rate 0.00220492\n","2023-06-28T22:19:45.866374: step 627, loss 0.763761, acc 0.8125, learning_rate 0.00220284\n","2023-06-28T22:19:45.965071: step 628, loss 0.803678, acc 0.75, learning_rate 0.00220077\n","2023-06-28T22:19:46.058457: step 629, loss 0.615029, acc 0.71875, learning_rate 0.0021987\n","2023-06-28T22:19:46.155606: step 630, loss 0.376099, acc 0.875, learning_rate 0.00219664\n","2023-06-28T22:19:46.245861: step 631, loss 0.611719, acc 0.78125, learning_rate 0.00219457\n","2023-06-28T22:19:46.350054: step 632, loss 0.430804, acc 0.8125, learning_rate 0.00219251\n","2023-06-28T22:19:46.450814: step 633, loss 0.735936, acc 0.6875, learning_rate 0.00219045\n","2023-06-28T22:19:46.545952: step 634, loss 0.39994, acc 0.84375, learning_rate 0.00218839\n","2023-06-28T22:19:46.649306: step 635, loss 0.577748, acc 0.78125, learning_rate 0.00218634\n","2023-06-28T22:19:46.744569: step 636, loss 0.380369, acc 0.875, learning_rate 0.00218428\n","2023-06-28T22:19:46.845908: step 637, loss 0.897858, acc 0.71875, learning_rate 0.00218223\n","2023-06-28T22:19:46.944464: step 638, loss 0.386522, acc 0.84375, learning_rate 0.00218018\n","2023-06-28T22:19:47.037551: step 639, loss 0.275438, acc 0.875, learning_rate 0.00217813\n","2023-06-28T22:19:47.133771: step 640, loss 0.349153, acc 0.84375, learning_rate 0.00217608\n","2023-06-28T22:19:47.228869: step 641, loss 0.336734, acc 0.90625, learning_rate 0.00217404\n","2023-06-28T22:19:47.330795: step 642, loss 0.32459, acc 0.8125, learning_rate 0.002172\n","2023-06-28T22:19:47.433672: step 643, loss 0.473192, acc 0.78125, learning_rate 0.00216996\n","2023-06-28T22:19:47.531763: step 644, loss 0.787917, acc 0.65625, learning_rate 0.00216792\n","2023-06-28T22:19:47.629058: step 645, loss 0.302117, acc 0.875, learning_rate 0.00216588\n","2023-06-28T22:19:47.726115: step 646, loss 0.436102, acc 0.78125, learning_rate 0.00216385\n","2023-06-28T22:19:47.832038: step 647, loss 0.716348, acc 0.6875, learning_rate 0.00216182\n","2023-06-28T22:19:47.929597: step 648, loss 0.448645, acc 0.8125, learning_rate 0.00215979\n","2023-06-28T22:19:48.033210: step 649, loss 0.690226, acc 0.8125, learning_rate 0.00215776\n","2023-06-28T22:19:48.141824: step 650, loss 0.855211, acc 0.71875, learning_rate 0.00215573\n","2023-06-28T22:19:48.250915: step 651, loss 0.562396, acc 0.8125, learning_rate 0.00215371\n","2023-06-28T22:19:48.348783: step 652, loss 0.23944, acc 0.875, learning_rate 0.00215169\n","2023-06-28T22:19:48.454649: step 653, loss 0.305819, acc 0.90625, learning_rate 0.00214966\n","2023-06-28T22:19:48.559709: step 654, loss 0.39493, acc 0.84375, learning_rate 0.00214765\n","2023-06-28T22:19:48.658836: step 655, loss 0.452506, acc 0.8125, learning_rate 0.00214563\n","2023-06-28T22:19:48.762943: step 656, loss 0.576514, acc 0.78125, learning_rate 0.00214362\n","2023-06-28T22:19:48.856633: step 657, loss 0.307044, acc 0.875, learning_rate 0.0021416\n","2023-06-28T22:19:48.973691: step 658, loss 0.622971, acc 0.75, learning_rate 0.00213959\n","2023-06-28T22:19:49.074944: step 659, loss 0.471821, acc 0.8125, learning_rate 0.00213759\n","2023-06-28T22:19:49.168263: step 660, loss 0.288342, acc 0.875, learning_rate 0.00213558\n","2023-06-28T22:19:49.263297: step 661, loss 0.478642, acc 0.6875, learning_rate 0.00213357\n","2023-06-28T22:19:49.357072: step 662, loss 0.605641, acc 0.8125, learning_rate 0.00213157\n","2023-06-28T22:19:49.470817: step 663, loss 0.773386, acc 0.71875, learning_rate 0.00212957\n","2023-06-28T22:19:49.565307: step 664, loss 0.617928, acc 0.78125, learning_rate 0.00212757\n","2023-06-28T22:19:49.661564: step 665, loss 0.734066, acc 0.65625, learning_rate 0.00212558\n","2023-06-28T22:19:49.752981: step 666, loss 0.481611, acc 0.75, learning_rate 0.00212358\n","2023-06-28T22:19:49.850782: step 667, loss 0.415511, acc 0.78125, learning_rate 0.00212159\n","2023-06-28T22:19:49.948310: step 668, loss 0.401398, acc 0.90625, learning_rate 0.0021196\n","2023-06-28T22:19:50.045571: step 669, loss 0.444828, acc 0.78125, learning_rate 0.00211761\n","2023-06-28T22:19:50.146127: step 670, loss 0.771305, acc 0.75, learning_rate 0.00211562\n","2023-06-28T22:19:50.239843: step 671, loss 0.775711, acc 0.65625, learning_rate 0.00211364\n","2023-06-28T22:19:50.336486: step 672, loss 0.425215, acc 0.78125, learning_rate 0.00211166\n","2023-06-28T22:19:50.433518: step 673, loss 0.341597, acc 0.875, learning_rate 0.00210968\n","2023-06-28T22:19:50.547239: step 674, loss 0.923268, acc 0.65625, learning_rate 0.0021077\n","2023-06-28T22:19:50.641029: step 675, loss 0.659131, acc 0.75, learning_rate 0.00210572\n","2023-06-28T22:19:50.738200: step 676, loss 0.649, acc 0.75, learning_rate 0.00210375\n","2023-06-28T22:19:50.853695: step 677, loss 0.671534, acc 0.71875, learning_rate 0.00210177\n","2023-06-28T22:19:50.998254: step 678, loss 0.320391, acc 0.875, learning_rate 0.0020998\n","2023-06-28T22:19:51.163052: step 679, loss 0.640778, acc 0.71875, learning_rate 0.00209783\n","2023-06-28T22:19:51.337237: step 680, loss 0.455333, acc 0.84375, learning_rate 0.00209586\n","2023-06-28T22:19:51.524252: step 681, loss 0.374702, acc 0.84375, learning_rate 0.0020939\n","2023-06-28T22:19:51.702173: step 682, loss 0.430512, acc 0.90625, learning_rate 0.00209194\n","2023-06-28T22:19:51.884823: step 683, loss 0.242929, acc 0.90625, learning_rate 0.00208997\n","2023-06-28T22:19:52.052424: step 684, loss 0.226722, acc 0.9375, learning_rate 0.00208802\n","2023-06-28T22:19:52.241106: step 685, loss 0.684511, acc 0.6875, learning_rate 0.00208606\n","2023-06-28T22:19:52.430604: step 686, loss 0.304379, acc 0.875, learning_rate 0.0020841\n","2023-06-28T22:19:52.614561: step 687, loss 0.73485, acc 0.75, learning_rate 0.00208215\n","2023-06-28T22:19:52.783240: step 688, loss 0.601971, acc 0.75, learning_rate 0.0020802\n","2023-06-28T22:19:52.961694: step 689, loss 0.395628, acc 0.84375, learning_rate 0.00207825\n","2023-06-28T22:19:53.132328: step 690, loss 0.628543, acc 0.8125, learning_rate 0.0020763\n","2023-06-28T22:19:53.294600: step 691, loss 0.444691, acc 0.90625, learning_rate 0.00207435\n","2023-06-28T22:19:53.464157: step 692, loss 0.617664, acc 0.78125, learning_rate 0.00207241\n","2023-06-28T22:19:53.633196: step 693, loss 0.675858, acc 0.78125, learning_rate 0.00207047\n","2023-06-28T22:19:53.799746: step 694, loss 0.390258, acc 0.78125, learning_rate 0.00206853\n","2023-06-28T22:19:53.976632: step 695, loss 0.702694, acc 0.84375, learning_rate 0.00206659\n","2023-06-28T22:19:54.145701: step 696, loss 0.30867, acc 0.90625, learning_rate 0.00206465\n","2023-06-28T22:19:54.315725: step 697, loss 0.431387, acc 0.8125, learning_rate 0.00206272\n","2023-06-28T22:19:54.489433: step 698, loss 0.554182, acc 0.6875, learning_rate 0.00206078\n","2023-06-28T22:19:54.666085: step 699, loss 0.541533, acc 0.71875, learning_rate 0.00205885\n","\n","Evaluation:\n","2023-06-28T22:19:55.884776: step 700, loss 0.495531, acc 0.816195\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-700\n","\n","2023-06-28T22:19:56.174007: step 700, loss 0.330882, acc 0.8125, learning_rate 0.00205693\n","2023-06-28T22:19:56.318747: step 701, loss 0.329863, acc 0.84375, learning_rate 0.002055\n","2023-06-28T22:19:56.484268: step 702, loss 0.641459, acc 0.71875, learning_rate 0.00205307\n","2023-06-28T22:19:56.646175: step 703, loss 0.539833, acc 0.875, learning_rate 0.00205115\n","2023-06-28T22:19:56.812242: step 704, loss 0.634683, acc 0.71875, learning_rate 0.00204923\n","2023-06-28T22:19:56.952671: step 705, loss 0.648528, acc 0.8125, learning_rate 0.00204731\n","2023-06-28T22:19:57.119599: step 706, loss 0.885173, acc 0.875, learning_rate 0.00204539\n","2023-06-28T22:19:57.271692: step 707, loss 0.616746, acc 0.8125, learning_rate 0.00204348\n","2023-06-28T22:19:57.418697: step 708, loss 0.362222, acc 0.84375, learning_rate 0.00204156\n","2023-06-28T22:19:57.579325: step 709, loss 0.388919, acc 0.90625, learning_rate 0.00203965\n","2023-06-28T22:19:57.749144: step 710, loss 0.450126, acc 0.84375, learning_rate 0.00203774\n","2023-06-28T22:19:57.910133: step 711, loss 0.484559, acc 0.84375, learning_rate 0.00203583\n","2023-06-28T22:19:58.074291: step 712, loss 0.369088, acc 0.875, learning_rate 0.00203393\n","2023-06-28T22:19:58.242326: step 713, loss 0.328804, acc 0.875, learning_rate 0.00203202\n","2023-06-28T22:19:58.395325: step 714, loss 0.468248, acc 0.8125, learning_rate 0.00203012\n","2023-06-28T22:19:58.562890: step 715, loss 0.293857, acc 0.875, learning_rate 0.00202822\n","2023-06-28T22:19:58.725994: step 716, loss 0.705404, acc 0.75, learning_rate 0.00202632\n","2023-06-28T22:19:58.883750: step 717, loss 0.521446, acc 0.75, learning_rate 0.00202442\n","2023-06-28T22:19:59.052422: step 718, loss 1.00269, acc 0.6875, learning_rate 0.00202253\n","2023-06-28T22:19:59.205742: step 719, loss 0.542452, acc 0.8125, learning_rate 0.00202064\n","2023-06-28T22:19:59.377529: step 720, loss 0.61108, acc 0.78125, learning_rate 0.00201875\n","2023-06-28T22:19:59.541296: step 721, loss 0.692257, acc 0.75, learning_rate 0.00201686\n","2023-06-28T22:19:59.708802: step 722, loss 0.516361, acc 0.875, learning_rate 0.00201497\n","2023-06-28T22:19:59.892036: step 723, loss 0.445155, acc 0.90625, learning_rate 0.00201308\n","2023-06-28T22:20:00.053806: step 724, loss 0.411565, acc 0.84375, learning_rate 0.0020112\n","2023-06-28T22:20:00.229096: step 725, loss 0.933268, acc 0.71875, learning_rate 0.00200932\n","2023-06-28T22:20:00.390785: step 726, loss 0.245883, acc 0.9375, learning_rate 0.00200744\n","2023-06-28T22:20:00.557044: step 727, loss 0.330977, acc 0.84375, learning_rate 0.00200556\n","2023-06-28T22:20:00.724742: step 728, loss 0.456016, acc 0.78125, learning_rate 0.00200368\n","2023-06-28T22:20:00.886921: step 729, loss 0.349201, acc 0.84375, learning_rate 0.00200181\n","2023-06-28T22:20:01.059511: step 730, loss 0.534842, acc 0.8125, learning_rate 0.00199994\n","2023-06-28T22:20:01.227158: step 731, loss 0.467176, acc 0.8125, learning_rate 0.00199806\n","2023-06-28T22:20:01.392265: step 732, loss 0.742664, acc 0.8125, learning_rate 0.0019962\n","2023-06-28T22:20:01.544190: step 733, loss 0.276954, acc 0.90625, learning_rate 0.00199433\n","2023-06-28T22:20:01.705525: step 734, loss 0.42011, acc 0.84375, learning_rate 0.00199246\n","2023-06-28T22:20:01.818798: step 735, loss 0.663237, acc 0.71875, learning_rate 0.0019906\n","2023-06-28T22:20:01.917586: step 736, loss 0.395229, acc 0.8125, learning_rate 0.00198874\n","2023-06-28T22:20:02.037859: step 737, loss 0.644036, acc 0.6875, learning_rate 0.00198688\n","2023-06-28T22:20:02.139449: step 738, loss 0.428623, acc 0.8125, learning_rate 0.00198502\n","2023-06-28T22:20:02.236802: step 739, loss 0.574048, acc 0.71875, learning_rate 0.00198316\n","2023-06-28T22:20:02.334312: step 740, loss 0.623635, acc 0.78125, learning_rate 0.00198131\n","2023-06-28T22:20:02.435442: step 741, loss 0.911429, acc 0.6875, learning_rate 0.00197946\n","2023-06-28T22:20:02.531948: step 742, loss 0.437164, acc 0.90625, learning_rate 0.00197761\n","2023-06-28T22:20:02.626946: step 743, loss 0.368762, acc 0.78125, learning_rate 0.00197576\n","2023-06-28T22:20:02.728470: step 744, loss 0.602192, acc 0.8125, learning_rate 0.00197391\n","2023-06-28T22:20:02.829457: step 745, loss 0.459072, acc 0.8125, learning_rate 0.00197207\n","2023-06-28T22:20:02.934745: step 746, loss 0.810602, acc 0.6875, learning_rate 0.00197022\n","2023-06-28T22:20:03.035855: step 747, loss 0.625652, acc 0.75, learning_rate 0.00196838\n","2023-06-28T22:20:03.131489: step 748, loss 0.457262, acc 0.875, learning_rate 0.00196654\n","2023-06-28T22:20:03.229019: step 749, loss 0.29245, acc 0.875, learning_rate 0.0019647\n","2023-06-28T22:20:03.322195: step 750, loss 0.58445, acc 0.78125, learning_rate 0.00196287\n","2023-06-28T22:20:03.419775: step 751, loss 0.117743, acc 0.96875, learning_rate 0.00196103\n","2023-06-28T22:20:03.519880: step 752, loss 0.559629, acc 0.84375, learning_rate 0.0019592\n","2023-06-28T22:20:03.614603: step 753, loss 0.347583, acc 0.8125, learning_rate 0.00195737\n","2023-06-28T22:20:03.708026: step 754, loss 0.336895, acc 0.84375, learning_rate 0.00195554\n","2023-06-28T22:20:03.802718: step 755, loss 0.318841, acc 0.90625, learning_rate 0.00195371\n","2023-06-28T22:20:03.908989: step 756, loss 0.48261, acc 0.8125, learning_rate 0.00195189\n","2023-06-28T22:20:04.039560: step 757, loss 0.643039, acc 0.84375, learning_rate 0.00195007\n","2023-06-28T22:20:04.145044: step 758, loss 0.22586, acc 0.90625, learning_rate 0.00194824\n","2023-06-28T22:20:04.239311: step 759, loss 0.850503, acc 0.75, learning_rate 0.00194642\n","2023-06-28T22:20:04.337318: step 760, loss 0.483715, acc 0.84375, learning_rate 0.00194461\n","2023-06-28T22:20:04.435704: step 761, loss 0.577251, acc 0.8125, learning_rate 0.00194279\n","2023-06-28T22:20:04.534239: step 762, loss 0.384635, acc 0.78125, learning_rate 0.00194098\n","2023-06-28T22:20:04.629807: step 763, loss 0.511209, acc 0.78125, learning_rate 0.00193916\n","2023-06-28T22:20:04.730528: step 764, loss 0.334933, acc 0.875, learning_rate 0.00193735\n","2023-06-28T22:20:04.827530: step 765, loss 0.688894, acc 0.8125, learning_rate 0.00193554\n","2023-06-28T22:20:04.923613: step 766, loss 0.367376, acc 0.84375, learning_rate 0.00193373\n","2023-06-28T22:20:05.028738: step 767, loss 0.720839, acc 0.78125, learning_rate 0.00193193\n","2023-06-28T22:20:05.146262: step 768, loss 0.659926, acc 0.71875, learning_rate 0.00193013\n","2023-06-28T22:20:05.244744: step 769, loss 0.617941, acc 0.875, learning_rate 0.00192832\n","2023-06-28T22:20:05.347609: step 770, loss 0.49913, acc 0.78125, learning_rate 0.00192652\n","2023-06-28T22:20:05.443771: step 771, loss 0.25113, acc 0.875, learning_rate 0.00192472\n","2023-06-28T22:20:05.541871: step 772, loss 0.417925, acc 0.8125, learning_rate 0.00192293\n","2023-06-28T22:20:05.636840: step 773, loss 0.891719, acc 0.71875, learning_rate 0.00192113\n","2023-06-28T22:20:05.737941: step 774, loss 0.69997, acc 0.65625, learning_rate 0.00191934\n","2023-06-28T22:20:05.837524: step 775, loss 0.394024, acc 0.8125, learning_rate 0.00191755\n","2023-06-28T22:20:05.935480: step 776, loss 0.560603, acc 0.84375, learning_rate 0.00191576\n","2023-06-28T22:20:06.033328: step 777, loss 0.317417, acc 0.90625, learning_rate 0.00191397\n","2023-06-28T22:20:06.146277: step 778, loss 0.679794, acc 0.75, learning_rate 0.00191218\n","2023-06-28T22:20:06.242618: step 779, loss 0.249634, acc 0.90625, learning_rate 0.0019104\n","2023-06-28T22:20:06.339135: step 780, loss 0.430448, acc 0.8125, learning_rate 0.00190862\n","2023-06-28T22:20:06.438580: step 781, loss 0.674577, acc 0.75, learning_rate 0.00190684\n","2023-06-28T22:20:06.534632: step 782, loss 0.874967, acc 0.65625, learning_rate 0.00190506\n","2023-06-28T22:20:06.631051: step 783, loss 0.692747, acc 0.78125, learning_rate 0.00190328\n","2023-06-28T22:20:06.724685: step 784, loss 0.182665, acc 0.9375, learning_rate 0.0019015\n","2023-06-28T22:20:06.821719: step 785, loss 0.347661, acc 0.875, learning_rate 0.00189973\n","2023-06-28T22:20:06.920833: step 786, loss 0.411059, acc 0.8125, learning_rate 0.00189796\n","2023-06-28T22:20:07.031331: step 787, loss 0.423226, acc 0.84375, learning_rate 0.00189619\n","2023-06-28T22:20:07.138054: step 788, loss 0.487088, acc 0.90625, learning_rate 0.00189442\n","2023-06-28T22:20:07.232599: step 789, loss 0.276009, acc 0.90625, learning_rate 0.00189265\n","2023-06-28T22:20:07.340837: step 790, loss 0.301142, acc 0.90625, learning_rate 0.00189089\n","2023-06-28T22:20:07.439940: step 791, loss 0.808761, acc 0.75, learning_rate 0.00188912\n","2023-06-28T22:20:07.537266: step 792, loss 0.355092, acc 0.84375, learning_rate 0.00188736\n","2023-06-28T22:20:07.632891: step 793, loss 0.579364, acc 0.78125, learning_rate 0.0018856\n","2023-06-28T22:20:07.727141: step 794, loss 0.434476, acc 0.8125, learning_rate 0.00188384\n","2023-06-28T22:20:07.829294: step 795, loss 0.755949, acc 0.71875, learning_rate 0.00188209\n","2023-06-28T22:20:07.930881: step 796, loss 0.547052, acc 0.71875, learning_rate 0.00188033\n","2023-06-28T22:20:08.028489: step 797, loss 0.38835, acc 0.78125, learning_rate 0.00187858\n","2023-06-28T22:20:08.132502: step 798, loss 0.411854, acc 0.84375, learning_rate 0.00187683\n","2023-06-28T22:20:08.244866: step 799, loss 0.543859, acc 0.8125, learning_rate 0.00187508\n","\n","Evaluation:\n","2023-06-28T22:20:08.962576: step 800, loss 0.536767, acc 0.79064\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-800\n","\n","2023-06-28T22:20:09.169155: step 800, loss 0.402025, acc 0.90625, learning_rate 0.00187333\n","2023-06-28T22:20:09.267479: step 801, loss 0.693476, acc 0.78125, learning_rate 0.00187159\n","2023-06-28T22:20:09.362754: step 802, loss 0.37002, acc 0.875, learning_rate 0.00186984\n","2023-06-28T22:20:09.462779: step 803, loss 0.29149, acc 0.875, learning_rate 0.0018681\n","2023-06-28T22:20:09.565400: step 804, loss 0.558324, acc 0.75, learning_rate 0.00186636\n","2023-06-28T22:20:09.664401: step 805, loss 0.594276, acc 0.84375, learning_rate 0.00186462\n","2023-06-28T22:20:09.764829: step 806, loss 0.884119, acc 0.6875, learning_rate 0.00186288\n","2023-06-28T22:20:09.871603: step 807, loss 0.623524, acc 0.75, learning_rate 0.00186114\n","2023-06-28T22:20:09.973139: step 808, loss 0.652766, acc 0.75, learning_rate 0.00185941\n","2023-06-28T22:20:10.077716: step 809, loss 0.973459, acc 0.71875, learning_rate 0.00185768\n","2023-06-28T22:20:10.186964: step 810, loss 0.472007, acc 0.875, learning_rate 0.00185595\n","2023-06-28T22:20:10.288617: step 811, loss 0.47169, acc 0.78125, learning_rate 0.00185422\n","2023-06-28T22:20:10.388004: step 812, loss 0.541097, acc 0.84375, learning_rate 0.00185249\n","2023-06-28T22:20:10.453374: step 813, loss 0.0727128, acc 1, learning_rate 0.00185077\n","2023-06-28T22:20:10.554837: step 814, loss 0.35791, acc 0.90625, learning_rate 0.00184904\n","2023-06-28T22:20:10.656022: step 815, loss 0.47112, acc 0.84375, learning_rate 0.00184732\n","2023-06-28T22:20:10.753900: step 816, loss 0.286531, acc 0.875, learning_rate 0.0018456\n","2023-06-28T22:20:10.856342: step 817, loss 0.225329, acc 0.90625, learning_rate 0.00184388\n","2023-06-28T22:20:10.955316: step 818, loss 0.246813, acc 0.90625, learning_rate 0.00184216\n","2023-06-28T22:20:11.055134: step 819, loss 0.389203, acc 0.8125, learning_rate 0.00184045\n","2023-06-28T22:20:11.181895: step 820, loss 0.400749, acc 0.84375, learning_rate 0.00183873\n","2023-06-28T22:20:11.283932: step 821, loss 0.206409, acc 0.9375, learning_rate 0.00183702\n","2023-06-28T22:20:11.386875: step 822, loss 0.23885, acc 0.90625, learning_rate 0.00183531\n","2023-06-28T22:20:11.490765: step 823, loss 0.468848, acc 0.8125, learning_rate 0.0018336\n","2023-06-28T22:20:11.584630: step 824, loss 0.617496, acc 0.78125, learning_rate 0.0018319\n","2023-06-28T22:20:11.680821: step 825, loss 0.271939, acc 0.90625, learning_rate 0.00183019\n","2023-06-28T22:20:11.801277: step 826, loss 0.484422, acc 0.875, learning_rate 0.00182849\n","2023-06-28T22:20:11.953228: step 827, loss 0.353064, acc 0.8125, learning_rate 0.00182678\n","2023-06-28T22:20:12.133907: step 828, loss 0.284902, acc 0.875, learning_rate 0.00182508\n","2023-06-28T22:20:12.309321: step 829, loss 0.302613, acc 0.875, learning_rate 0.00182339\n","2023-06-28T22:20:12.492373: step 830, loss 0.361122, acc 0.84375, learning_rate 0.00182169\n","2023-06-28T22:20:12.668411: step 831, loss 0.468265, acc 0.875, learning_rate 0.00181999\n","2023-06-28T22:20:12.851254: step 832, loss 0.451594, acc 0.75, learning_rate 0.0018183\n","2023-06-28T22:20:13.031482: step 833, loss 0.19718, acc 0.9375, learning_rate 0.00181661\n","2023-06-28T22:20:13.209348: step 834, loss 0.434934, acc 0.875, learning_rate 0.00181492\n","2023-06-28T22:20:13.408720: step 835, loss 0.433219, acc 0.78125, learning_rate 0.00181323\n","2023-06-28T22:20:13.583805: step 836, loss 0.449576, acc 0.75, learning_rate 0.00181154\n","2023-06-28T22:20:13.756535: step 837, loss 0.735757, acc 0.78125, learning_rate 0.00180986\n","2023-06-28T22:20:13.932206: step 838, loss 0.57499, acc 0.78125, learning_rate 0.00180817\n","2023-06-28T22:20:14.114094: step 839, loss 0.34627, acc 0.84375, learning_rate 0.00180649\n","2023-06-28T22:20:14.289158: step 840, loss 0.364694, acc 0.84375, learning_rate 0.00180481\n","2023-06-28T22:20:14.465064: step 841, loss 0.193055, acc 0.96875, learning_rate 0.00180313\n","2023-06-28T22:20:14.636027: step 842, loss 0.276448, acc 0.84375, learning_rate 0.00180145\n","2023-06-28T22:20:14.814287: step 843, loss 0.40251, acc 0.875, learning_rate 0.00179978\n","2023-06-28T22:20:14.990020: step 844, loss 0.26187, acc 0.9375, learning_rate 0.00179811\n","2023-06-28T22:20:15.174774: step 845, loss 0.370545, acc 0.84375, learning_rate 0.00179643\n","2023-06-28T22:20:15.352342: step 846, loss 0.555135, acc 0.875, learning_rate 0.00179476\n","2023-06-28T22:20:15.510075: step 847, loss 0.332216, acc 0.84375, learning_rate 0.00179309\n","2023-06-28T22:20:15.691258: step 848, loss 0.704189, acc 0.71875, learning_rate 0.00179143\n","2023-06-28T22:20:15.872190: step 849, loss 0.282357, acc 0.84375, learning_rate 0.00178976\n","2023-06-28T22:20:16.032801: step 850, loss 0.235772, acc 0.90625, learning_rate 0.0017881\n","2023-06-28T22:20:16.188739: step 851, loss 0.246334, acc 0.90625, learning_rate 0.00178644\n","2023-06-28T22:20:16.354695: step 852, loss 0.373853, acc 0.8125, learning_rate 0.00178478\n","2023-06-28T22:20:16.528444: step 853, loss 0.263003, acc 0.9375, learning_rate 0.00178312\n","2023-06-28T22:20:16.701632: step 854, loss 0.567179, acc 0.84375, learning_rate 0.00178146\n","2023-06-28T22:20:16.869109: step 855, loss 0.390413, acc 0.8125, learning_rate 0.0017798\n","2023-06-28T22:20:17.036195: step 856, loss 0.666262, acc 0.78125, learning_rate 0.00177815\n","2023-06-28T22:20:17.201518: step 857, loss 0.300426, acc 0.90625, learning_rate 0.0017765\n","2023-06-28T22:20:17.366262: step 858, loss 0.378184, acc 0.875, learning_rate 0.00177485\n","2023-06-28T22:20:17.542138: step 859, loss 0.352368, acc 0.875, learning_rate 0.0017732\n","2023-06-28T22:20:17.690028: step 860, loss 0.593327, acc 0.75, learning_rate 0.00177155\n","2023-06-28T22:20:17.873601: step 861, loss 0.231832, acc 0.9375, learning_rate 0.0017699\n","2023-06-28T22:20:18.037303: step 862, loss 0.22764, acc 0.875, learning_rate 0.00176826\n","2023-06-28T22:20:18.201199: step 863, loss 0.43755, acc 0.84375, learning_rate 0.00176662\n","2023-06-28T22:20:18.364617: step 864, loss 0.511088, acc 0.8125, learning_rate 0.00176498\n","2023-06-28T22:20:18.526125: step 865, loss 0.326424, acc 0.875, learning_rate 0.00176334\n","2023-06-28T22:20:18.707586: step 866, loss 0.363234, acc 0.8125, learning_rate 0.0017617\n","2023-06-28T22:20:18.888117: step 867, loss 0.315121, acc 0.84375, learning_rate 0.00176006\n","2023-06-28T22:20:19.074742: step 868, loss 0.353986, acc 0.9375, learning_rate 0.00175843\n","2023-06-28T22:20:19.239196: step 869, loss 0.496939, acc 0.84375, learning_rate 0.00175679\n","2023-06-28T22:20:19.413206: step 870, loss 0.448779, acc 0.84375, learning_rate 0.00175516\n","2023-06-28T22:20:19.578275: step 871, loss 0.172741, acc 0.9375, learning_rate 0.00175353\n","2023-06-28T22:20:19.743204: step 872, loss 0.292393, acc 0.84375, learning_rate 0.00175191\n","2023-06-28T22:20:19.912190: step 873, loss 0.199722, acc 0.9375, learning_rate 0.00175028\n","2023-06-28T22:20:20.093030: step 874, loss 0.265073, acc 0.875, learning_rate 0.00174865\n","2023-06-28T22:20:20.254289: step 875, loss 0.425089, acc 0.78125, learning_rate 0.00174703\n","2023-06-28T22:20:20.429665: step 876, loss 0.172971, acc 0.90625, learning_rate 0.00174541\n","2023-06-28T22:20:20.607809: step 877, loss 0.179655, acc 0.9375, learning_rate 0.00174379\n","2023-06-28T22:20:20.779230: step 878, loss 0.394671, acc 0.8125, learning_rate 0.00174217\n","2023-06-28T22:20:20.951447: step 879, loss 0.449363, acc 0.75, learning_rate 0.00174055\n","2023-06-28T22:20:21.131792: step 880, loss 0.238601, acc 0.875, learning_rate 0.00173894\n","2023-06-28T22:20:21.294599: step 881, loss 0.467443, acc 0.875, learning_rate 0.00173732\n","2023-06-28T22:20:21.468186: step 882, loss 0.274363, acc 0.84375, learning_rate 0.00173571\n","2023-06-28T22:20:21.622007: step 883, loss 0.794484, acc 0.75, learning_rate 0.0017341\n","2023-06-28T22:20:21.789610: step 884, loss 0.321558, acc 0.9375, learning_rate 0.00173249\n","2023-06-28T22:20:21.956093: step 885, loss 0.421722, acc 0.84375, learning_rate 0.00173088\n","2023-06-28T22:20:22.123094: step 886, loss 0.309293, acc 0.9375, learning_rate 0.00172928\n","2023-06-28T22:20:22.236668: step 887, loss 0.467265, acc 0.75, learning_rate 0.00172767\n","2023-06-28T22:20:22.340241: step 888, loss 0.341249, acc 0.90625, learning_rate 0.00172607\n","2023-06-28T22:20:22.436087: step 889, loss 0.376198, acc 0.84375, learning_rate 0.00172447\n","2023-06-28T22:20:22.532697: step 890, loss 0.33761, acc 0.9375, learning_rate 0.00172287\n","2023-06-28T22:20:22.633579: step 891, loss 0.300911, acc 0.84375, learning_rate 0.00172127\n","2023-06-28T22:20:22.745567: step 892, loss 0.370053, acc 0.90625, learning_rate 0.00171968\n","2023-06-28T22:20:22.846803: step 893, loss 0.540735, acc 0.84375, learning_rate 0.00171808\n","2023-06-28T22:20:22.949592: step 894, loss 0.275281, acc 0.875, learning_rate 0.00171649\n","2023-06-28T22:20:23.061528: step 895, loss 0.509205, acc 0.8125, learning_rate 0.0017149\n","2023-06-28T22:20:23.169326: step 896, loss 0.144135, acc 0.9375, learning_rate 0.00171331\n","2023-06-28T22:20:23.269692: step 897, loss 0.399426, acc 0.84375, learning_rate 0.00171172\n","2023-06-28T22:20:23.371120: step 898, loss 0.46933, acc 0.75, learning_rate 0.00171013\n","2023-06-28T22:20:23.481832: step 899, loss 0.632648, acc 0.75, learning_rate 0.00170855\n","\n","Evaluation:\n","2023-06-28T22:20:24.223766: step 900, loss 0.532578, acc 0.819889\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-900\n","\n","2023-06-28T22:20:24.439891: step 900, loss 0.287823, acc 0.875, learning_rate 0.00170696\n","2023-06-28T22:20:24.541916: step 901, loss 0.196233, acc 0.9375, learning_rate 0.00170538\n","2023-06-28T22:20:24.647050: step 902, loss 0.246179, acc 0.875, learning_rate 0.0017038\n","2023-06-28T22:20:24.756423: step 903, loss 0.367377, acc 0.8125, learning_rate 0.00170222\n","2023-06-28T22:20:24.853915: step 904, loss 0.443211, acc 0.78125, learning_rate 0.00170064\n","2023-06-28T22:20:24.951193: step 905, loss 0.372661, acc 0.875, learning_rate 0.00169907\n","2023-06-28T22:20:25.059691: step 906, loss 0.378783, acc 0.875, learning_rate 0.00169749\n","2023-06-28T22:20:25.158277: step 907, loss 0.483888, acc 0.75, learning_rate 0.00169592\n","2023-06-28T22:20:25.252875: step 908, loss 0.530412, acc 0.84375, learning_rate 0.00169435\n","2023-06-28T22:20:25.358907: step 909, loss 0.476393, acc 0.75, learning_rate 0.00169278\n","2023-06-28T22:20:25.457173: step 910, loss 0.243542, acc 0.9375, learning_rate 0.00169121\n","2023-06-28T22:20:25.558136: step 911, loss 0.192585, acc 0.9375, learning_rate 0.00168964\n","2023-06-28T22:20:25.656945: step 912, loss 0.359981, acc 0.84375, learning_rate 0.00168808\n","2023-06-28T22:20:25.764323: step 913, loss 0.366427, acc 0.875, learning_rate 0.00168651\n","2023-06-28T22:20:25.863047: step 914, loss 0.257096, acc 0.90625, learning_rate 0.00168495\n","2023-06-28T22:20:25.969552: step 915, loss 0.253202, acc 0.875, learning_rate 0.00168339\n","2023-06-28T22:20:26.079129: step 916, loss 0.395156, acc 0.75, learning_rate 0.00168183\n","2023-06-28T22:20:26.181714: step 917, loss 0.410249, acc 0.84375, learning_rate 0.00168027\n","2023-06-28T22:20:26.287243: step 918, loss 0.260492, acc 0.875, learning_rate 0.00167872\n","2023-06-28T22:20:26.382680: step 919, loss 0.292593, acc 0.875, learning_rate 0.00167716\n","2023-06-28T22:20:26.482374: step 920, loss 0.18315, acc 0.9375, learning_rate 0.00167561\n","2023-06-28T22:20:26.579803: step 921, loss 0.303309, acc 0.875, learning_rate 0.00167406\n","2023-06-28T22:20:26.681370: step 922, loss 0.289149, acc 0.90625, learning_rate 0.00167251\n","2023-06-28T22:20:26.784919: step 923, loss 0.478707, acc 0.8125, learning_rate 0.00167096\n","2023-06-28T22:20:26.884712: step 924, loss 0.281309, acc 0.84375, learning_rate 0.00166941\n","2023-06-28T22:20:26.986738: step 925, loss 0.354192, acc 0.875, learning_rate 0.00166787\n","2023-06-28T22:20:27.089817: step 926, loss 0.154328, acc 0.90625, learning_rate 0.00166632\n","2023-06-28T22:20:27.199345: step 927, loss 0.314093, acc 0.84375, learning_rate 0.00166478\n","2023-06-28T22:20:27.298642: step 928, loss 0.29357, acc 0.84375, learning_rate 0.00166324\n","2023-06-28T22:20:27.393089: step 929, loss 0.41264, acc 0.90625, learning_rate 0.0016617\n","2023-06-28T22:20:27.502443: step 930, loss 0.43194, acc 0.78125, learning_rate 0.00166016\n","2023-06-28T22:20:27.656802: step 931, loss 0.481957, acc 0.8125, learning_rate 0.00165863\n","2023-06-28T22:20:27.760071: step 932, loss 0.369204, acc 0.875, learning_rate 0.00165709\n","2023-06-28T22:20:27.868224: step 933, loss 0.255539, acc 0.90625, learning_rate 0.00165556\n","2023-06-28T22:20:27.972666: step 934, loss 0.42725, acc 0.90625, learning_rate 0.00165403\n","2023-06-28T22:20:28.089936: step 935, loss 0.252899, acc 0.84375, learning_rate 0.0016525\n","2023-06-28T22:20:28.195461: step 936, loss 0.265519, acc 0.90625, learning_rate 0.00165097\n","2023-06-28T22:20:28.293884: step 937, loss 0.735227, acc 0.71875, learning_rate 0.00164944\n","2023-06-28T22:20:28.403925: step 938, loss 0.276954, acc 0.9375, learning_rate 0.00164792\n","2023-06-28T22:20:28.509314: step 939, loss 0.444875, acc 0.8125, learning_rate 0.00164639\n","2023-06-28T22:20:28.611039: step 940, loss 0.164505, acc 0.96875, learning_rate 0.00164487\n","2023-06-28T22:20:28.714292: step 941, loss 0.382704, acc 0.84375, learning_rate 0.00164335\n","2023-06-28T22:20:28.825361: step 942, loss 0.490376, acc 0.78125, learning_rate 0.00164183\n","2023-06-28T22:20:28.931431: step 943, loss 0.357617, acc 0.8125, learning_rate 0.00164031\n","2023-06-28T22:20:29.031235: step 944, loss 0.540236, acc 0.875, learning_rate 0.00163879\n","2023-06-28T22:20:29.140429: step 945, loss 0.356531, acc 0.84375, learning_rate 0.00163728\n","2023-06-28T22:20:29.236456: step 946, loss 0.285958, acc 0.875, learning_rate 0.00163576\n","2023-06-28T22:20:29.335230: step 947, loss 0.109866, acc 0.9375, learning_rate 0.00163425\n","2023-06-28T22:20:29.437284: step 948, loss 0.288169, acc 0.875, learning_rate 0.00163274\n","2023-06-28T22:20:29.541891: step 949, loss 0.587509, acc 0.75, learning_rate 0.00163123\n","2023-06-28T22:20:29.647038: step 950, loss 0.26955, acc 0.875, learning_rate 0.00162973\n","2023-06-28T22:20:29.752485: step 951, loss 0.277195, acc 0.9375, learning_rate 0.00162822\n","2023-06-28T22:20:29.863894: step 952, loss 0.272516, acc 0.875, learning_rate 0.00162671\n","2023-06-28T22:20:29.972794: step 953, loss 0.352396, acc 0.84375, learning_rate 0.00162521\n","2023-06-28T22:20:30.085948: step 954, loss 0.430461, acc 0.84375, learning_rate 0.00162371\n","2023-06-28T22:20:30.208901: step 955, loss 0.574159, acc 0.71875, learning_rate 0.00162221\n","2023-06-28T22:20:30.306857: step 956, loss 0.646487, acc 0.78125, learning_rate 0.00162071\n","2023-06-28T22:20:30.408677: step 957, loss 0.406556, acc 0.84375, learning_rate 0.00161921\n","2023-06-28T22:20:30.513884: step 958, loss 0.456231, acc 0.84375, learning_rate 0.00161772\n","2023-06-28T22:20:30.615103: step 959, loss 0.584576, acc 0.71875, learning_rate 0.00161622\n","2023-06-28T22:20:30.718954: step 960, loss 0.633527, acc 0.8125, learning_rate 0.00161473\n","2023-06-28T22:20:30.818676: step 961, loss 0.65013, acc 0.78125, learning_rate 0.00161324\n","2023-06-28T22:20:30.940022: step 962, loss 0.248501, acc 0.90625, learning_rate 0.00161175\n","2023-06-28T22:20:31.042701: step 963, loss 0.58378, acc 0.78125, learning_rate 0.00161026\n","2023-06-28T22:20:31.144655: step 964, loss 0.364654, acc 0.84375, learning_rate 0.00160877\n","2023-06-28T22:20:31.246926: step 965, loss 0.381037, acc 0.875, learning_rate 0.00160729\n","2023-06-28T22:20:31.343939: step 966, loss 0.304817, acc 0.875, learning_rate 0.0016058\n","2023-06-28T22:20:31.452718: step 967, loss 0.182455, acc 0.875, learning_rate 0.00160432\n","2023-06-28T22:20:31.551454: step 968, loss 0.375004, acc 0.8125, learning_rate 0.00160284\n","2023-06-28T22:20:31.651040: step 969, loss 0.220134, acc 0.96875, learning_rate 0.00160136\n","2023-06-28T22:20:31.750008: step 970, loss 0.213089, acc 0.9375, learning_rate 0.00159988\n","2023-06-28T22:20:31.846905: step 971, loss 0.555572, acc 0.78125, learning_rate 0.0015984\n","2023-06-28T22:20:31.957134: step 972, loss 0.352174, acc 0.8125, learning_rate 0.00159693\n","2023-06-28T22:20:32.063517: step 973, loss 0.223521, acc 0.90625, learning_rate 0.00159545\n","2023-06-28T22:20:32.179034: step 974, loss 0.266045, acc 0.875, learning_rate 0.00159398\n","2023-06-28T22:20:32.344411: step 975, loss 0.416896, acc 0.78125, learning_rate 0.00159251\n","2023-06-28T22:20:32.517404: step 976, loss 0.50609, acc 0.8125, learning_rate 0.00159104\n","2023-06-28T22:20:32.683486: step 977, loss 0.428676, acc 0.90625, learning_rate 0.00158957\n","2023-06-28T22:20:32.863346: step 978, loss 0.426905, acc 0.8125, learning_rate 0.00158811\n","2023-06-28T22:20:33.049528: step 979, loss 0.347861, acc 0.90625, learning_rate 0.00158664\n","2023-06-28T22:20:33.219474: step 980, loss 0.243497, acc 0.90625, learning_rate 0.00158518\n","2023-06-28T22:20:33.387572: step 981, loss 0.488138, acc 0.71875, learning_rate 0.00158371\n","2023-06-28T22:20:33.567077: step 982, loss 0.285143, acc 0.875, learning_rate 0.00158225\n","2023-06-28T22:20:33.744612: step 983, loss 0.431332, acc 0.84375, learning_rate 0.00158079\n","2023-06-28T22:20:33.897902: step 984, loss 0.336984, acc 0.78125, learning_rate 0.00157934\n","2023-06-28T22:20:34.092812: step 985, loss 0.493525, acc 0.8125, learning_rate 0.00157788\n","2023-06-28T22:20:34.278280: step 986, loss 0.321545, acc 0.90625, learning_rate 0.00157642\n","2023-06-28T22:20:34.441592: step 987, loss 0.185303, acc 0.9375, learning_rate 0.00157497\n","2023-06-28T22:20:34.608566: step 988, loss 0.511012, acc 0.9375, learning_rate 0.00157352\n","2023-06-28T22:20:34.786738: step 989, loss 0.49916, acc 0.84375, learning_rate 0.00157207\n","2023-06-28T22:20:34.950235: step 990, loss 0.461959, acc 0.8125, learning_rate 0.00157062\n","2023-06-28T22:20:35.106163: step 991, loss 0.338886, acc 0.875, learning_rate 0.00156917\n","2023-06-28T22:20:35.292867: step 992, loss 0.39031, acc 0.8125, learning_rate 0.00156772\n","2023-06-28T22:20:35.463839: step 993, loss 0.43335, acc 0.875, learning_rate 0.00156628\n","2023-06-28T22:20:35.625906: step 994, loss 0.43511, acc 0.84375, learning_rate 0.00156483\n","2023-06-28T22:20:35.794902: step 995, loss 0.22626, acc 0.875, learning_rate 0.00156339\n","2023-06-28T22:20:35.955547: step 996, loss 0.195286, acc 0.90625, learning_rate 0.00156195\n","2023-06-28T22:20:36.121519: step 997, loss 0.179606, acc 0.90625, learning_rate 0.00156051\n","2023-06-28T22:20:36.294937: step 998, loss 0.253135, acc 0.90625, learning_rate 0.00155907\n","2023-06-28T22:20:36.473276: step 999, loss 0.2716, acc 0.8125, learning_rate 0.00155764\n","\n","Evaluation:\n","2023-06-28T22:20:37.762990: step 1000, loss 0.561755, acc 0.808498\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1000\n","\n","2023-06-28T22:20:38.055347: step 1000, loss 0.343597, acc 0.875, learning_rate 0.0015562\n","2023-06-28T22:20:38.221407: step 1001, loss 0.370006, acc 0.8125, learning_rate 0.00155477\n","2023-06-28T22:20:38.383410: step 1002, loss 0.275768, acc 0.875, learning_rate 0.00155333\n","2023-06-28T22:20:38.544058: step 1003, loss 0.417994, acc 0.84375, learning_rate 0.0015519\n","2023-06-28T22:20:38.717621: step 1004, loss 0.20616, acc 0.9375, learning_rate 0.00155047\n","2023-06-28T22:20:38.878379: step 1005, loss 0.363865, acc 0.84375, learning_rate 0.00154905\n","2023-06-28T22:20:39.049282: step 1006, loss 0.350699, acc 0.8125, learning_rate 0.00154762\n","2023-06-28T22:20:39.220688: step 1007, loss 0.34527, acc 0.84375, learning_rate 0.00154619\n","2023-06-28T22:20:39.378793: step 1008, loss 0.249472, acc 0.8125, learning_rate 0.00154477\n","2023-06-28T22:20:39.541089: step 1009, loss 0.407081, acc 0.78125, learning_rate 0.00154335\n","2023-06-28T22:20:39.698359: step 1010, loss 0.681133, acc 0.6875, learning_rate 0.00154193\n","2023-06-28T22:20:39.875248: step 1011, loss 0.166339, acc 0.9375, learning_rate 0.00154051\n","2023-06-28T22:20:40.040527: step 1012, loss 0.307743, acc 0.875, learning_rate 0.00153909\n","2023-06-28T22:20:40.208955: step 1013, loss 0.374367, acc 0.84375, learning_rate 0.00153767\n","2023-06-28T22:20:40.357140: step 1014, loss 0.571548, acc 0.78125, learning_rate 0.00153625\n","2023-06-28T22:20:40.499420: step 1015, loss 0.274683, acc 0.84375, learning_rate 0.00153484\n","2023-06-28T22:20:40.673573: step 1016, loss 0.21329, acc 0.9375, learning_rate 0.00153343\n","2023-06-28T22:20:40.841839: step 1017, loss 0.41743, acc 0.90625, learning_rate 0.00153202\n","2023-06-28T22:20:41.005477: step 1018, loss 0.496658, acc 0.78125, learning_rate 0.00153061\n","2023-06-28T22:20:41.163998: step 1019, loss 0.386062, acc 0.84375, learning_rate 0.0015292\n","2023-06-28T22:20:41.320775: step 1020, loss 0.512111, acc 0.84375, learning_rate 0.00152779\n","2023-06-28T22:20:41.488519: step 1021, loss 0.799841, acc 0.78125, learning_rate 0.00152638\n","2023-06-28T22:20:41.664729: step 1022, loss 0.178217, acc 0.875, learning_rate 0.00152498\n","2023-06-28T22:20:41.826934: step 1023, loss 0.321323, acc 0.90625, learning_rate 0.00152358\n","2023-06-28T22:20:41.999352: step 1024, loss 0.296717, acc 0.875, learning_rate 0.00152217\n","2023-06-28T22:20:42.181502: step 1025, loss 0.351089, acc 0.84375, learning_rate 0.00152077\n","2023-06-28T22:20:42.351403: step 1026, loss 0.371386, acc 0.8125, learning_rate 0.00151938\n","2023-06-28T22:20:42.493834: step 1027, loss 0.261803, acc 0.90625, learning_rate 0.00151798\n","2023-06-28T22:20:42.643824: step 1028, loss 0.437794, acc 0.78125, learning_rate 0.00151658\n","2023-06-28T22:20:42.818186: step 1029, loss 0.497612, acc 0.8125, learning_rate 0.00151519\n","2023-06-28T22:20:42.922763: step 1030, loss 0.421006, acc 0.875, learning_rate 0.00151379\n","2023-06-28T22:20:43.020140: step 1031, loss 0.23449, acc 0.90625, learning_rate 0.0015124\n","2023-06-28T22:20:43.122309: step 1032, loss 0.356505, acc 0.875, learning_rate 0.00151101\n","2023-06-28T22:20:43.225090: step 1033, loss 0.302866, acc 0.90625, learning_rate 0.00150962\n","2023-06-28T22:20:43.329873: step 1034, loss 0.648352, acc 0.75, learning_rate 0.00150823\n","2023-06-28T22:20:43.424505: step 1035, loss 0.415107, acc 0.8125, learning_rate 0.00150685\n","2023-06-28T22:20:43.524214: step 1036, loss 0.74525, acc 0.71875, learning_rate 0.00150546\n","2023-06-28T22:20:43.619664: step 1037, loss 0.166834, acc 0.90625, learning_rate 0.00150408\n","2023-06-28T22:20:43.719943: step 1038, loss 0.567857, acc 0.75, learning_rate 0.00150269\n","2023-06-28T22:20:43.822289: step 1039, loss 0.401494, acc 0.84375, learning_rate 0.00150131\n","2023-06-28T22:20:43.921312: step 1040, loss 0.123114, acc 0.96875, learning_rate 0.00149993\n","2023-06-28T22:20:44.026219: step 1041, loss 0.378552, acc 0.875, learning_rate 0.00149856\n","2023-06-28T22:20:44.144559: step 1042, loss 0.226303, acc 0.90625, learning_rate 0.00149718\n","2023-06-28T22:20:44.247196: step 1043, loss 0.630923, acc 0.75, learning_rate 0.0014958\n","2023-06-28T22:20:44.354787: step 1044, loss 0.270818, acc 0.9375, learning_rate 0.00149443\n","2023-06-28T22:20:44.452228: step 1045, loss 0.406673, acc 0.78125, learning_rate 0.00149305\n","2023-06-28T22:20:44.554888: step 1046, loss 0.288429, acc 0.90625, learning_rate 0.00149168\n","2023-06-28T22:20:44.651409: step 1047, loss 0.229635, acc 0.9375, learning_rate 0.00149031\n","2023-06-28T22:20:44.759820: step 1048, loss 0.42724, acc 0.78125, learning_rate 0.00148894\n","2023-06-28T22:20:44.857140: step 1049, loss 0.555879, acc 0.78125, learning_rate 0.00148758\n","2023-06-28T22:20:44.960839: step 1050, loss 0.337227, acc 0.90625, learning_rate 0.00148621\n","2023-06-28T22:20:45.068769: step 1051, loss 0.640188, acc 0.8125, learning_rate 0.00148485\n","2023-06-28T22:20:45.170117: step 1052, loss 0.558353, acc 0.8125, learning_rate 0.00148348\n","2023-06-28T22:20:45.266897: step 1053, loss 0.867263, acc 0.75, learning_rate 0.00148212\n","2023-06-28T22:20:45.374674: step 1054, loss 0.270141, acc 0.90625, learning_rate 0.00148076\n","2023-06-28T22:20:45.468548: step 1055, loss 0.459063, acc 0.8125, learning_rate 0.0014794\n","2023-06-28T22:20:45.578457: step 1056, loss 0.766424, acc 0.8125, learning_rate 0.00147804\n","2023-06-28T22:20:45.682488: step 1057, loss 0.176157, acc 0.9375, learning_rate 0.00147668\n","2023-06-28T22:20:45.787975: step 1058, loss 0.793183, acc 0.78125, learning_rate 0.00147533\n","2023-06-28T22:20:45.886729: step 1059, loss 0.211581, acc 0.875, learning_rate 0.00147397\n","2023-06-28T22:20:45.986676: step 1060, loss 0.457617, acc 0.84375, learning_rate 0.00147262\n","2023-06-28T22:20:46.088754: step 1061, loss 0.257512, acc 0.90625, learning_rate 0.00147127\n","2023-06-28T22:20:46.199852: step 1062, loss 0.310295, acc 0.90625, learning_rate 0.00146992\n","2023-06-28T22:20:46.299037: step 1063, loss 0.321906, acc 0.84375, learning_rate 0.00146857\n","2023-06-28T22:20:46.405166: step 1064, loss 0.516731, acc 0.84375, learning_rate 0.00146722\n","2023-06-28T22:20:46.520505: step 1065, loss 0.47859, acc 0.8125, learning_rate 0.00146588\n","2023-06-28T22:20:46.621832: step 1066, loss 0.59109, acc 0.875, learning_rate 0.00146453\n","2023-06-28T22:20:46.715754: step 1067, loss 0.276231, acc 0.90625, learning_rate 0.00146319\n","2023-06-28T22:20:46.814371: step 1068, loss 0.266925, acc 0.90625, learning_rate 0.00146185\n","2023-06-28T22:20:46.919591: step 1069, loss 0.190183, acc 0.9375, learning_rate 0.0014605\n","2023-06-28T22:20:47.016069: step 1070, loss 0.614392, acc 0.90625, learning_rate 0.00145916\n","2023-06-28T22:20:47.106983: step 1071, loss 0.439983, acc 0.84375, learning_rate 0.00145783\n","2023-06-28T22:20:47.220806: step 1072, loss 0.293901, acc 0.875, learning_rate 0.00145649\n","2023-06-28T22:20:47.330211: step 1073, loss 0.127043, acc 0.9375, learning_rate 0.00145515\n","2023-06-28T22:20:47.438940: step 1074, loss 0.322479, acc 0.875, learning_rate 0.00145382\n","2023-06-28T22:20:47.538399: step 1075, loss 0.654534, acc 0.78125, learning_rate 0.00145249\n","2023-06-28T22:20:47.628402: step 1076, loss 0.382788, acc 0.78125, learning_rate 0.00145115\n","2023-06-28T22:20:47.726419: step 1077, loss 0.375704, acc 0.875, learning_rate 0.00144982\n","2023-06-28T22:20:47.828248: step 1078, loss 0.312828, acc 0.84375, learning_rate 0.0014485\n","2023-06-28T22:20:47.931974: step 1079, loss 0.655755, acc 0.78125, learning_rate 0.00144717\n","2023-06-28T22:20:48.031488: step 1080, loss 0.219904, acc 0.96875, learning_rate 0.00144584\n","2023-06-28T22:20:48.138371: step 1081, loss 0.430674, acc 0.8125, learning_rate 0.00144452\n","2023-06-28T22:20:48.236433: step 1082, loss 0.382111, acc 0.78125, learning_rate 0.00144319\n","2023-06-28T22:20:48.335727: step 1083, loss 0.539151, acc 0.84375, learning_rate 0.00144187\n","2023-06-28T22:20:48.450539: step 1084, loss 0.395064, acc 0.84375, learning_rate 0.00144055\n","2023-06-28T22:20:48.557443: step 1085, loss 0.341596, acc 0.8125, learning_rate 0.00143923\n","2023-06-28T22:20:48.661800: step 1086, loss 0.689336, acc 0.6875, learning_rate 0.00143791\n","2023-06-28T22:20:48.760957: step 1087, loss 0.220656, acc 0.90625, learning_rate 0.00143659\n","2023-06-28T22:20:48.865135: step 1088, loss 0.242023, acc 0.90625, learning_rate 0.00143528\n","2023-06-28T22:20:48.971488: step 1089, loss 0.29003, acc 0.8125, learning_rate 0.00143396\n","2023-06-28T22:20:49.091569: step 1090, loss 0.465388, acc 0.8125, learning_rate 0.00143265\n","2023-06-28T22:20:49.196891: step 1091, loss 0.189966, acc 0.9375, learning_rate 0.00143134\n","2023-06-28T22:20:49.295267: step 1092, loss 0.351953, acc 0.78125, learning_rate 0.00143002\n","2023-06-28T22:20:49.392897: step 1093, loss 0.200966, acc 0.90625, learning_rate 0.00142871\n","2023-06-28T22:20:49.508589: step 1094, loss 0.213718, acc 0.90625, learning_rate 0.00142741\n","2023-06-28T22:20:49.610588: step 1095, loss 0.56872, acc 0.78125, learning_rate 0.0014261\n","2023-06-28T22:20:49.720702: step 1096, loss 0.520424, acc 0.78125, learning_rate 0.00142479\n","2023-06-28T22:20:49.819883: step 1097, loss 0.314008, acc 0.875, learning_rate 0.00142349\n","2023-06-28T22:20:49.922194: step 1098, loss 0.144567, acc 0.96875, learning_rate 0.00142219\n","2023-06-28T22:20:50.023465: step 1099, loss 0.695954, acc 0.78125, learning_rate 0.00142088\n","\n","Evaluation:\n","2023-06-28T22:20:50.736608: step 1100, loss 0.514598, acc 0.81681\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1100\n","\n","2023-06-28T22:20:50.953416: step 1100, loss 0.475237, acc 0.75, learning_rate 0.00141958\n","2023-06-28T22:20:51.055675: step 1101, loss 0.511199, acc 0.8125, learning_rate 0.00141828\n","2023-06-28T22:20:51.160660: step 1102, loss 0.330966, acc 0.8125, learning_rate 0.00141699\n","2023-06-28T22:20:51.261149: step 1103, loss 1.04628, acc 0.71875, learning_rate 0.00141569\n","2023-06-28T22:20:51.355582: step 1104, loss 0.157934, acc 0.9375, learning_rate 0.00141439\n","2023-06-28T22:20:51.456088: step 1105, loss 0.553819, acc 0.75, learning_rate 0.0014131\n","2023-06-28T22:20:51.571048: step 1106, loss 0.230813, acc 0.90625, learning_rate 0.00141181\n","2023-06-28T22:20:51.674156: step 1107, loss 0.356303, acc 0.9375, learning_rate 0.00141051\n","2023-06-28T22:20:51.785856: step 1108, loss 0.386354, acc 0.875, learning_rate 0.00140922\n","2023-06-28T22:20:51.878498: step 1109, loss 0.67467, acc 0.8125, learning_rate 0.00140794\n","2023-06-28T22:20:51.976095: step 1110, loss 0.592279, acc 0.8125, learning_rate 0.00140665\n","2023-06-28T22:20:52.074906: step 1111, loss 0.316686, acc 0.84375, learning_rate 0.00140536\n","2023-06-28T22:20:52.174063: step 1112, loss 0.503533, acc 0.8125, learning_rate 0.00140408\n","2023-06-28T22:20:52.283357: step 1113, loss 0.39979, acc 0.875, learning_rate 0.00140279\n","2023-06-28T22:20:52.380638: step 1114, loss 0.252133, acc 0.84375, learning_rate 0.00140151\n","2023-06-28T22:20:52.475166: step 1115, loss 0.719562, acc 0.71875, learning_rate 0.00140023\n","2023-06-28T22:20:52.587456: step 1116, loss 0.431908, acc 0.875, learning_rate 0.00139895\n","2023-06-28T22:20:52.688930: step 1117, loss 0.476963, acc 0.875, learning_rate 0.00139767\n","2023-06-28T22:20:52.786358: step 1118, loss 0.422808, acc 0.8125, learning_rate 0.00139639\n","2023-06-28T22:20:52.925492: step 1119, loss 0.411562, acc 0.84375, learning_rate 0.00139511\n","2023-06-28T22:20:53.085331: step 1120, loss 0.355591, acc 0.84375, learning_rate 0.00139384\n","2023-06-28T22:20:53.274198: step 1121, loss 0.144771, acc 0.96875, learning_rate 0.00139256\n","2023-06-28T22:20:53.447896: step 1122, loss 0.28722, acc 0.90625, learning_rate 0.00139129\n","2023-06-28T22:20:53.635961: step 1123, loss 0.429176, acc 0.84375, learning_rate 0.00139002\n","2023-06-28T22:20:53.817728: step 1124, loss 0.191545, acc 0.875, learning_rate 0.00138875\n","2023-06-28T22:20:54.003520: step 1125, loss 0.431886, acc 0.90625, learning_rate 0.00138748\n","2023-06-28T22:20:54.205786: step 1126, loss 0.441606, acc 0.9375, learning_rate 0.00138621\n","2023-06-28T22:20:54.395593: step 1127, loss 0.615664, acc 0.78125, learning_rate 0.00138495\n","2023-06-28T22:20:54.573860: step 1128, loss 0.656682, acc 0.78125, learning_rate 0.00138368\n","2023-06-28T22:20:54.745143: step 1129, loss 0.631667, acc 0.6875, learning_rate 0.00138242\n","2023-06-28T22:20:54.933276: step 1130, loss 0.396051, acc 0.84375, learning_rate 0.00138115\n","2023-06-28T22:20:55.118914: step 1131, loss 0.236998, acc 0.90625, learning_rate 0.00137989\n","2023-06-28T22:20:55.309598: step 1132, loss 0.456668, acc 0.8125, learning_rate 0.00137863\n","2023-06-28T22:20:55.483322: step 1133, loss 0.416477, acc 0.8125, learning_rate 0.00137737\n","2023-06-28T22:20:55.647686: step 1134, loss 0.52073, acc 0.6875, learning_rate 0.00137612\n","2023-06-28T22:20:55.813533: step 1135, loss 0.304506, acc 0.90625, learning_rate 0.00137486\n","2023-06-28T22:20:56.001299: step 1136, loss 0.472185, acc 0.75, learning_rate 0.0013736\n","2023-06-28T22:20:56.179831: step 1137, loss 0.354139, acc 0.875, learning_rate 0.00137235\n","2023-06-28T22:20:56.355848: step 1138, loss 0.594207, acc 0.84375, learning_rate 0.0013711\n","2023-06-28T22:20:56.526492: step 1139, loss 0.42614, acc 0.84375, learning_rate 0.00136985\n","2023-06-28T22:20:56.672781: step 1140, loss 0.580513, acc 0.78125, learning_rate 0.0013686\n","2023-06-28T22:20:56.831635: step 1141, loss 0.238188, acc 0.9375, learning_rate 0.00136735\n","2023-06-28T22:20:57.024139: step 1142, loss 0.302505, acc 0.9375, learning_rate 0.0013661\n","2023-06-28T22:20:57.178496: step 1143, loss 0.376227, acc 0.84375, learning_rate 0.00136485\n","2023-06-28T22:20:57.337489: step 1144, loss 0.383383, acc 0.875, learning_rate 0.00136361\n","2023-06-28T22:20:57.496587: step 1145, loss 0.644392, acc 0.8125, learning_rate 0.00136236\n","2023-06-28T22:20:57.661069: step 1146, loss 0.266543, acc 0.90625, learning_rate 0.00136112\n","2023-06-28T22:20:57.821308: step 1147, loss 0.243052, acc 0.90625, learning_rate 0.00135988\n","2023-06-28T22:20:57.962266: step 1148, loss 0.242799, acc 0.90625, learning_rate 0.00135864\n","2023-06-28T22:20:58.152756: step 1149, loss 0.336256, acc 0.84375, learning_rate 0.0013574\n","2023-06-28T22:20:58.316582: step 1150, loss 0.34554, acc 0.78125, learning_rate 0.00135616\n","2023-06-28T22:20:58.496094: step 1151, loss 0.419196, acc 0.8125, learning_rate 0.00135492\n","2023-06-28T22:20:58.626039: step 1152, loss 0.996407, acc 0.6875, learning_rate 0.00135369\n","2023-06-28T22:20:58.784253: step 1153, loss 0.198806, acc 0.9375, learning_rate 0.00135245\n","2023-06-28T22:20:58.946956: step 1154, loss 0.252301, acc 0.9375, learning_rate 0.00135122\n","2023-06-28T22:20:59.148446: step 1155, loss 0.294037, acc 0.84375, learning_rate 0.00134999\n","2023-06-28T22:20:59.319558: step 1156, loss 0.461703, acc 0.90625, learning_rate 0.00134876\n","2023-06-28T22:20:59.474549: step 1157, loss 0.332614, acc 0.90625, learning_rate 0.00134753\n","2023-06-28T22:20:59.641410: step 1158, loss 0.336405, acc 0.90625, learning_rate 0.0013463\n","2023-06-28T22:20:59.815233: step 1159, loss 0.51581, acc 0.84375, learning_rate 0.00134507\n","2023-06-28T22:20:59.974507: step 1160, loss 0.416567, acc 0.84375, learning_rate 0.00134384\n","2023-06-28T22:21:00.158156: step 1161, loss 0.251041, acc 0.875, learning_rate 0.00134262\n","2023-06-28T22:21:00.325190: step 1162, loss 0.611799, acc 0.84375, learning_rate 0.0013414\n","2023-06-28T22:21:00.483100: step 1163, loss 0.375001, acc 0.84375, learning_rate 0.00134017\n","2023-06-28T22:21:00.648641: step 1164, loss 0.402895, acc 0.84375, learning_rate 0.00133895\n","2023-06-28T22:21:00.806918: step 1165, loss 0.585155, acc 0.8125, learning_rate 0.00133773\n","2023-06-28T22:21:00.960004: step 1166, loss 0.214087, acc 0.90625, learning_rate 0.00133651\n","2023-06-28T22:21:01.134228: step 1167, loss 0.329421, acc 0.875, learning_rate 0.0013353\n","2023-06-28T22:21:01.297924: step 1168, loss 0.603944, acc 0.8125, learning_rate 0.00133408\n","2023-06-28T22:21:01.454077: step 1169, loss 0.186958, acc 0.9375, learning_rate 0.00133287\n","2023-06-28T22:21:01.621106: step 1170, loss 0.389186, acc 0.8125, learning_rate 0.00133165\n","2023-06-28T22:21:01.775596: step 1171, loss 0.40741, acc 0.71875, learning_rate 0.00133044\n","2023-06-28T22:21:01.941225: step 1172, loss 0.340294, acc 0.84375, learning_rate 0.00132923\n","2023-06-28T22:21:02.098095: step 1173, loss 0.404372, acc 0.84375, learning_rate 0.00132802\n","2023-06-28T22:21:02.276191: step 1174, loss 0.561935, acc 0.78125, learning_rate 0.00132681\n","2023-06-28T22:21:02.429423: step 1175, loss 0.892238, acc 0.8125, learning_rate 0.0013256\n","2023-06-28T22:21:02.601581: step 1176, loss 0.365554, acc 0.875, learning_rate 0.00132439\n","2023-06-28T22:21:02.768833: step 1177, loss 0.37614, acc 0.875, learning_rate 0.00132319\n","2023-06-28T22:21:02.936134: step 1178, loss 0.282005, acc 0.875, learning_rate 0.00132198\n","2023-06-28T22:21:03.107132: step 1179, loss 0.506713, acc 0.78125, learning_rate 0.00132078\n","2023-06-28T22:21:03.274960: step 1180, loss 0.38539, acc 0.84375, learning_rate 0.00131958\n","2023-06-28T22:21:03.414672: step 1181, loss 0.678404, acc 0.75, learning_rate 0.00131838\n","2023-06-28T22:21:03.562675: step 1182, loss 0.57526, acc 0.78125, learning_rate 0.00131718\n","2023-06-28T22:21:03.656055: step 1183, loss 0.650069, acc 0.875, learning_rate 0.00131598\n","2023-06-28T22:21:03.755705: step 1184, loss 0.497168, acc 0.84375, learning_rate 0.00131478\n","2023-06-28T22:21:03.859549: step 1185, loss 0.311157, acc 0.8125, learning_rate 0.00131358\n","2023-06-28T22:21:03.959854: step 1186, loss 0.183646, acc 0.9375, learning_rate 0.00131239\n","2023-06-28T22:21:04.077925: step 1187, loss 0.507698, acc 0.78125, learning_rate 0.0013112\n","2023-06-28T22:21:04.187568: step 1188, loss 0.387381, acc 0.78125, learning_rate 0.00131\n","2023-06-28T22:21:04.288665: step 1189, loss 0.356994, acc 0.84375, learning_rate 0.00130881\n","2023-06-28T22:21:04.393485: step 1190, loss 0.302087, acc 0.84375, learning_rate 0.00130762\n","2023-06-28T22:21:04.491426: step 1191, loss 0.613604, acc 0.78125, learning_rate 0.00130643\n","2023-06-28T22:21:04.584860: step 1192, loss 0.602911, acc 0.84375, learning_rate 0.00130524\n","2023-06-28T22:21:04.676961: step 1193, loss 0.390854, acc 0.90625, learning_rate 0.00130406\n","2023-06-28T22:21:04.777323: step 1194, loss 0.458304, acc 0.84375, learning_rate 0.00130287\n","2023-06-28T22:21:04.871875: step 1195, loss 0.37015, acc 0.84375, learning_rate 0.00130169\n","2023-06-28T22:21:04.963885: step 1196, loss 0.253166, acc 0.90625, learning_rate 0.0013005\n","2023-06-28T22:21:05.071042: step 1197, loss 0.377554, acc 0.78125, learning_rate 0.00129932\n","2023-06-28T22:21:05.169447: step 1198, loss 0.903861, acc 0.75, learning_rate 0.00129814\n","2023-06-28T22:21:05.274216: step 1199, loss 0.301995, acc 0.9375, learning_rate 0.00129696\n","\n","Evaluation:\n","2023-06-28T22:21:05.950136: step 1200, loss 0.512951, acc 0.825123\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1200\n","\n","2023-06-28T22:21:06.147277: step 1200, loss 0.120059, acc 0.96875, learning_rate 0.00129578\n","2023-06-28T22:21:06.242807: step 1201, loss 0.181828, acc 0.90625, learning_rate 0.00129461\n","2023-06-28T22:21:06.342234: step 1202, loss 0.401994, acc 0.84375, learning_rate 0.00129343\n","2023-06-28T22:21:06.448470: step 1203, loss 0.500169, acc 0.78125, learning_rate 0.00129225\n","2023-06-28T22:21:06.547512: step 1204, loss 0.777588, acc 0.75, learning_rate 0.00129108\n","2023-06-28T22:21:06.659991: step 1205, loss 0.181006, acc 0.96875, learning_rate 0.00128991\n","2023-06-28T22:21:06.760044: step 1206, loss 0.445599, acc 0.8125, learning_rate 0.00128874\n","2023-06-28T22:21:06.854443: step 1207, loss 0.346999, acc 0.875, learning_rate 0.00128757\n","2023-06-28T22:21:06.951293: step 1208, loss 0.530238, acc 0.8125, learning_rate 0.0012864\n","2023-06-28T22:21:07.047578: step 1209, loss 0.815694, acc 0.75, learning_rate 0.00128523\n","2023-06-28T22:21:07.162162: step 1210, loss 0.619317, acc 0.84375, learning_rate 0.00128406\n","2023-06-28T22:21:07.257689: step 1211, loss 0.533324, acc 0.8125, learning_rate 0.0012829\n","2023-06-28T22:21:07.352173: step 1212, loss 0.0895293, acc 0.96875, learning_rate 0.00128173\n","2023-06-28T22:21:07.449723: step 1213, loss 0.427416, acc 0.8125, learning_rate 0.00128057\n","2023-06-28T22:21:07.547528: step 1214, loss 0.513898, acc 0.8125, learning_rate 0.0012794\n","2023-06-28T22:21:07.641635: step 1215, loss 0.375022, acc 0.78125, learning_rate 0.00127824\n","2023-06-28T22:21:07.737980: step 1216, loss 0.615626, acc 0.71875, learning_rate 0.00127708\n","2023-06-28T22:21:07.831660: step 1217, loss 0.454102, acc 0.75, learning_rate 0.00127592\n","2023-06-28T22:21:07.931582: step 1218, loss 0.368307, acc 0.75, learning_rate 0.00127477\n","2023-06-28T22:21:08.028002: step 1219, loss 0.436675, acc 0.84375, learning_rate 0.00127361\n","2023-06-28T22:21:08.091911: step 1220, loss 0.56252, acc 1, learning_rate 0.00127245\n","2023-06-28T22:21:08.201766: step 1221, loss 0.204083, acc 0.90625, learning_rate 0.0012713\n","2023-06-28T22:21:08.300930: step 1222, loss 0.189249, acc 0.9375, learning_rate 0.00127015\n","2023-06-28T22:21:08.392728: step 1223, loss 0.612882, acc 0.84375, learning_rate 0.00126899\n","2023-06-28T22:21:08.497476: step 1224, loss 0.3115, acc 0.90625, learning_rate 0.00126784\n","2023-06-28T22:21:08.600764: step 1225, loss 0.3065, acc 0.875, learning_rate 0.00126669\n","2023-06-28T22:21:08.693634: step 1226, loss 0.168, acc 0.9375, learning_rate 0.00126554\n","2023-06-28T22:21:08.798492: step 1227, loss 0.320589, acc 0.875, learning_rate 0.0012644\n","2023-06-28T22:21:08.894287: step 1228, loss 0.472356, acc 0.8125, learning_rate 0.00126325\n","2023-06-28T22:21:08.995118: step 1229, loss 0.2197, acc 0.875, learning_rate 0.0012621\n","2023-06-28T22:21:09.097393: step 1230, loss 0.583119, acc 0.78125, learning_rate 0.00126096\n","2023-06-28T22:21:09.204378: step 1231, loss 0.14045, acc 0.90625, learning_rate 0.00125982\n","2023-06-28T22:21:09.303915: step 1232, loss 0.229257, acc 0.90625, learning_rate 0.00125867\n","2023-06-28T22:21:09.395464: step 1233, loss 0.233614, acc 0.875, learning_rate 0.00125753\n","2023-06-28T22:21:09.491943: step 1234, loss 0.21114, acc 0.90625, learning_rate 0.00125639\n","2023-06-28T22:21:09.589594: step 1235, loss 0.240879, acc 0.875, learning_rate 0.00125526\n","2023-06-28T22:21:09.702242: step 1236, loss 0.210262, acc 0.9375, learning_rate 0.00125412\n","2023-06-28T22:21:09.796154: step 1237, loss 0.230189, acc 0.875, learning_rate 0.00125298\n","2023-06-28T22:21:09.891724: step 1238, loss 0.202007, acc 0.90625, learning_rate 0.00125185\n","2023-06-28T22:21:09.997467: step 1239, loss 0.216853, acc 0.9375, learning_rate 0.00125071\n","2023-06-28T22:21:10.097202: step 1240, loss 0.399722, acc 0.84375, learning_rate 0.00124958\n","2023-06-28T22:21:10.219752: step 1241, loss 0.210386, acc 0.90625, learning_rate 0.00124845\n","2023-06-28T22:21:10.321783: step 1242, loss 0.386177, acc 0.875, learning_rate 0.00124732\n","2023-06-28T22:21:10.422264: step 1243, loss 0.31605, acc 0.875, learning_rate 0.00124619\n","2023-06-28T22:21:10.523679: step 1244, loss 0.161464, acc 0.96875, learning_rate 0.00124506\n","2023-06-28T22:21:10.626689: step 1245, loss 0.172624, acc 0.96875, learning_rate 0.00124393\n","2023-06-28T22:21:10.726552: step 1246, loss 0.12823, acc 0.96875, learning_rate 0.0012428\n","2023-06-28T22:21:10.830768: step 1247, loss 0.307293, acc 0.78125, learning_rate 0.00124168\n","2023-06-28T22:21:10.932895: step 1248, loss 0.238758, acc 0.9375, learning_rate 0.00124055\n","2023-06-28T22:21:11.033816: step 1249, loss 0.228943, acc 0.90625, learning_rate 0.00123943\n","2023-06-28T22:21:11.138086: step 1250, loss 0.194131, acc 0.96875, learning_rate 0.00123831\n","2023-06-28T22:21:11.250241: step 1251, loss 0.399416, acc 0.75, learning_rate 0.00123719\n","2023-06-28T22:21:11.355893: step 1252, loss 0.25877, acc 0.90625, learning_rate 0.00123607\n","2023-06-28T22:21:11.463213: step 1253, loss 0.338997, acc 0.84375, learning_rate 0.00123495\n","2023-06-28T22:21:11.563376: step 1254, loss 0.213906, acc 0.90625, learning_rate 0.00123383\n","2023-06-28T22:21:11.662527: step 1255, loss 0.381067, acc 0.875, learning_rate 0.00123272\n","2023-06-28T22:21:11.763695: step 1256, loss 0.385514, acc 0.8125, learning_rate 0.0012316\n","2023-06-28T22:21:11.873396: step 1257, loss 0.147412, acc 0.9375, learning_rate 0.00123049\n","2023-06-28T22:21:11.971741: step 1258, loss 0.423594, acc 0.875, learning_rate 0.00122937\n","2023-06-28T22:21:12.079339: step 1259, loss 0.33143, acc 0.90625, learning_rate 0.00122826\n","2023-06-28T22:21:12.186466: step 1260, loss 0.403795, acc 0.875, learning_rate 0.00122715\n","2023-06-28T22:21:12.297804: step 1261, loss 0.32772, acc 0.90625, learning_rate 0.00122604\n","2023-06-28T22:21:12.451070: step 1262, loss 0.202309, acc 0.90625, learning_rate 0.00122493\n","2023-06-28T22:21:12.628394: step 1263, loss 0.557837, acc 0.6875, learning_rate 0.00122382\n","2023-06-28T22:21:12.805490: step 1264, loss 0.192996, acc 0.90625, learning_rate 0.00122272\n","2023-06-28T22:21:12.980039: step 1265, loss 0.258272, acc 0.90625, learning_rate 0.00122161\n","2023-06-28T22:21:13.153323: step 1266, loss 0.265468, acc 0.875, learning_rate 0.00122051\n","2023-06-28T22:21:13.339021: step 1267, loss 0.453496, acc 0.8125, learning_rate 0.0012194\n","2023-06-28T22:21:13.524421: step 1268, loss 0.232552, acc 0.90625, learning_rate 0.0012183\n","2023-06-28T22:21:13.704987: step 1269, loss 0.186038, acc 0.90625, learning_rate 0.0012172\n","2023-06-28T22:21:13.882004: step 1270, loss 0.332862, acc 0.875, learning_rate 0.0012161\n","2023-06-28T22:21:14.153561: step 1271, loss 0.236056, acc 0.875, learning_rate 0.001215\n","2023-06-28T22:21:14.356406: step 1272, loss 0.273043, acc 0.90625, learning_rate 0.0012139\n","2023-06-28T22:21:14.534938: step 1273, loss 0.197048, acc 0.90625, learning_rate 0.00121281\n","2023-06-28T22:21:14.726223: step 1274, loss 0.278234, acc 0.875, learning_rate 0.00121171\n","2023-06-28T22:21:14.966667: step 1275, loss 0.508859, acc 0.75, learning_rate 0.00121062\n","2023-06-28T22:21:15.204519: step 1276, loss 0.532489, acc 0.875, learning_rate 0.00120952\n","2023-06-28T22:21:15.425286: step 1277, loss 0.354338, acc 0.8125, learning_rate 0.00120843\n","2023-06-28T22:21:15.642519: step 1278, loss 0.212209, acc 0.9375, learning_rate 0.00120734\n","2023-06-28T22:21:15.873535: step 1279, loss 0.296331, acc 0.8125, learning_rate 0.00120625\n","2023-06-28T22:21:16.080588: step 1280, loss 0.431935, acc 0.8125, learning_rate 0.00120516\n","2023-06-28T22:21:16.265228: step 1281, loss 0.465423, acc 0.78125, learning_rate 0.00120407\n","2023-06-28T22:21:16.463287: step 1282, loss 0.163698, acc 0.9375, learning_rate 0.00120298\n","2023-06-28T22:21:16.632029: step 1283, loss 0.209884, acc 0.90625, learning_rate 0.0012019\n","2023-06-28T22:21:16.842092: step 1284, loss 0.794323, acc 0.8125, learning_rate 0.00120081\n","2023-06-28T22:21:17.005784: step 1285, loss 0.455157, acc 0.8125, learning_rate 0.00119973\n","2023-06-28T22:21:17.178674: step 1286, loss 0.384187, acc 0.875, learning_rate 0.00119865\n","2023-06-28T22:21:17.379337: step 1287, loss 0.187605, acc 0.90625, learning_rate 0.00119756\n","2023-06-28T22:21:17.597422: step 1288, loss 0.470658, acc 0.8125, learning_rate 0.00119648\n","2023-06-28T22:21:17.795652: step 1289, loss 0.28021, acc 0.9375, learning_rate 0.0011954\n","2023-06-28T22:21:17.976543: step 1290, loss 0.267644, acc 0.90625, learning_rate 0.00119433\n","2023-06-28T22:21:18.170280: step 1291, loss 0.424331, acc 0.84375, learning_rate 0.00119325\n","2023-06-28T22:21:18.371950: step 1292, loss 0.266648, acc 0.875, learning_rate 0.00119217\n","2023-06-28T22:21:18.565630: step 1293, loss 0.24573, acc 0.84375, learning_rate 0.0011911\n","2023-06-28T22:21:18.736589: step 1294, loss 0.145726, acc 0.9375, learning_rate 0.00119002\n","2023-06-28T22:21:18.919354: step 1295, loss 0.120058, acc 0.9375, learning_rate 0.00118895\n","2023-06-28T22:21:19.128599: step 1296, loss 0.122459, acc 0.9375, learning_rate 0.00118788\n","2023-06-28T22:21:19.341473: step 1297, loss 0.458413, acc 0.71875, learning_rate 0.00118681\n","2023-06-28T22:21:19.521723: step 1298, loss 0.145615, acc 0.90625, learning_rate 0.00118574\n","2023-06-28T22:21:19.722907: step 1299, loss 0.407902, acc 0.84375, learning_rate 0.00118467\n","\n","Evaluation:\n","2023-06-28T22:21:21.076934: step 1300, loss 0.589549, acc 0.813424\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1300\n","\n","2023-06-28T22:21:21.445010: step 1300, loss 0.286989, acc 0.90625, learning_rate 0.0011836\n","2023-06-28T22:21:21.624923: step 1301, loss 0.321443, acc 0.8125, learning_rate 0.00118253\n","2023-06-28T22:21:21.795870: step 1302, loss 0.200324, acc 0.9375, learning_rate 0.00118147\n","2023-06-28T22:21:21.980462: step 1303, loss 0.265276, acc 0.84375, learning_rate 0.0011804\n","2023-06-28T22:21:22.157972: step 1304, loss 0.150395, acc 0.96875, learning_rate 0.00117934\n","2023-06-28T22:21:22.329962: step 1305, loss 0.303043, acc 0.90625, learning_rate 0.00117827\n","2023-06-28T22:21:22.501141: step 1306, loss 0.317067, acc 0.875, learning_rate 0.00117721\n","2023-06-28T22:21:22.721931: step 1307, loss 0.389981, acc 0.875, learning_rate 0.00117615\n","2023-06-28T22:21:22.899547: step 1308, loss 0.813464, acc 0.78125, learning_rate 0.00117509\n","2023-06-28T22:21:23.113639: step 1309, loss 0.324548, acc 0.875, learning_rate 0.00117403\n","2023-06-28T22:21:23.296094: step 1310, loss 0.419374, acc 0.875, learning_rate 0.00117298\n","2023-06-28T22:21:23.475928: step 1311, loss 0.250627, acc 0.875, learning_rate 0.00117192\n","2023-06-28T22:21:23.664450: step 1312, loss 0.523967, acc 0.8125, learning_rate 0.00117086\n","2023-06-28T22:21:23.850047: step 1313, loss 0.65362, acc 0.78125, learning_rate 0.00116981\n","2023-06-28T22:21:24.034535: step 1314, loss 0.12576, acc 0.96875, learning_rate 0.00116876\n","2023-06-28T22:21:24.242745: step 1315, loss 0.376813, acc 0.875, learning_rate 0.0011677\n","2023-06-28T22:21:24.444216: step 1316, loss 0.579719, acc 0.78125, learning_rate 0.00116665\n","2023-06-28T22:21:24.625217: step 1317, loss 0.240087, acc 0.875, learning_rate 0.0011656\n","2023-06-28T22:21:24.801713: step 1318, loss 0.218626, acc 0.9375, learning_rate 0.00116455\n","2023-06-28T22:21:24.987242: step 1319, loss 0.311398, acc 0.90625, learning_rate 0.0011635\n","2023-06-28T22:21:25.166142: step 1320, loss 0.45298, acc 0.875, learning_rate 0.00116246\n","2023-06-28T22:21:25.335476: step 1321, loss 0.266014, acc 0.875, learning_rate 0.00116141\n","2023-06-28T22:21:25.524545: step 1322, loss 0.454319, acc 0.84375, learning_rate 0.00116037\n","2023-06-28T22:21:25.676486: step 1323, loss 0.204002, acc 0.875, learning_rate 0.00115932\n","2023-06-28T22:21:25.831769: step 1324, loss 0.288363, acc 0.90625, learning_rate 0.00115828\n","2023-06-28T22:21:26.009591: step 1325, loss 0.136255, acc 0.9375, learning_rate 0.00115724\n","2023-06-28T22:21:26.187935: step 1326, loss 0.405751, acc 0.90625, learning_rate 0.0011562\n","2023-06-28T22:21:26.352606: step 1327, loss 0.246727, acc 0.9375, learning_rate 0.00115516\n","2023-06-28T22:21:26.521121: step 1328, loss 0.325408, acc 0.875, learning_rate 0.00115412\n","2023-06-28T22:21:26.697046: step 1329, loss 0.196006, acc 0.96875, learning_rate 0.00115308\n","2023-06-28T22:21:26.874555: step 1330, loss 0.247386, acc 0.9375, learning_rate 0.00115204\n","2023-06-28T22:21:27.062911: step 1331, loss 0.375199, acc 0.875, learning_rate 0.00115101\n","2023-06-28T22:21:27.245034: step 1332, loss 0.333349, acc 0.875, learning_rate 0.00114997\n","2023-06-28T22:21:27.412187: step 1333, loss 0.179328, acc 0.9375, learning_rate 0.00114894\n","2023-06-28T22:21:27.574980: step 1334, loss 0.474544, acc 0.84375, learning_rate 0.0011479\n","2023-06-28T22:21:27.739568: step 1335, loss 0.692703, acc 0.8125, learning_rate 0.00114687\n","2023-06-28T22:21:27.908297: step 1336, loss 0.418347, acc 0.84375, learning_rate 0.00114584\n","2023-06-28T22:21:28.079363: step 1337, loss 0.289253, acc 0.90625, learning_rate 0.00114481\n","2023-06-28T22:21:28.267835: step 1338, loss 0.208473, acc 0.875, learning_rate 0.00114378\n","2023-06-28T22:21:28.446778: step 1339, loss 0.133962, acc 0.9375, learning_rate 0.00114276\n","2023-06-28T22:21:28.625990: step 1340, loss 0.190007, acc 0.9375, learning_rate 0.00114173\n","2023-06-28T22:21:28.786235: step 1341, loss 0.344138, acc 0.84375, learning_rate 0.0011407\n","2023-06-28T22:21:28.961971: step 1342, loss 0.234505, acc 0.9375, learning_rate 0.00113968\n","2023-06-28T22:21:29.178957: step 1343, loss 0.280499, acc 0.90625, learning_rate 0.00113865\n","2023-06-28T22:21:29.339379: step 1344, loss 0.0835534, acc 0.96875, learning_rate 0.00113763\n","2023-06-28T22:21:29.510813: step 1345, loss 0.0968618, acc 0.96875, learning_rate 0.00113661\n","2023-06-28T22:21:29.690326: step 1346, loss 0.238442, acc 0.9375, learning_rate 0.00113559\n","2023-06-28T22:21:29.854836: step 1347, loss 0.483065, acc 0.875, learning_rate 0.00113457\n","2023-06-28T22:21:29.998072: step 1348, loss 0.283586, acc 0.875, learning_rate 0.00113355\n","2023-06-28T22:21:30.163418: step 1349, loss 0.352592, acc 0.90625, learning_rate 0.00113253\n","2023-06-28T22:21:30.322044: step 1350, loss 0.190813, acc 0.9375, learning_rate 0.00113152\n","2023-06-28T22:21:30.493733: step 1351, loss 0.284012, acc 0.875, learning_rate 0.0011305\n","2023-06-28T22:21:30.662397: step 1352, loss 0.323728, acc 0.875, learning_rate 0.00112949\n","2023-06-28T22:21:30.830384: step 1353, loss 0.158596, acc 0.96875, learning_rate 0.00112847\n","2023-06-28T22:21:30.986472: step 1354, loss 0.229259, acc 0.9375, learning_rate 0.00112746\n","2023-06-28T22:21:31.092054: step 1355, loss 0.398242, acc 0.84375, learning_rate 0.00112645\n","2023-06-28T22:21:31.210445: step 1356, loss 0.329579, acc 0.90625, learning_rate 0.00112544\n","2023-06-28T22:21:31.316586: step 1357, loss 0.386901, acc 0.875, learning_rate 0.00112443\n","2023-06-28T22:21:31.416958: step 1358, loss 0.374069, acc 0.8125, learning_rate 0.00112342\n","2023-06-28T22:21:31.519881: step 1359, loss 0.383811, acc 0.84375, learning_rate 0.00112241\n","2023-06-28T22:21:31.639513: step 1360, loss 0.375191, acc 0.875, learning_rate 0.0011214\n","2023-06-28T22:21:31.734720: step 1361, loss 0.407796, acc 0.78125, learning_rate 0.0011204\n","2023-06-28T22:21:31.833736: step 1362, loss 0.398399, acc 0.875, learning_rate 0.00111939\n","2023-06-28T22:21:31.935942: step 1363, loss 0.341687, acc 0.875, learning_rate 0.00111839\n","2023-06-28T22:21:32.038211: step 1364, loss 0.23293, acc 0.90625, learning_rate 0.00111739\n","2023-06-28T22:21:32.149456: step 1365, loss 0.486386, acc 0.84375, learning_rate 0.00111639\n","2023-06-28T22:21:32.261358: step 1366, loss 0.321434, acc 0.9375, learning_rate 0.00111538\n","2023-06-28T22:21:32.354731: step 1367, loss 0.707388, acc 0.71875, learning_rate 0.00111438\n","2023-06-28T22:21:32.447159: step 1368, loss 0.377093, acc 0.8125, learning_rate 0.00111339\n","2023-06-28T22:21:32.550317: step 1369, loss 0.24461, acc 0.9375, learning_rate 0.00111239\n","2023-06-28T22:21:32.647310: step 1370, loss 0.431039, acc 0.875, learning_rate 0.00111139\n","2023-06-28T22:21:32.747457: step 1371, loss 0.274534, acc 0.875, learning_rate 0.0011104\n","2023-06-28T22:21:32.845293: step 1372, loss 0.26763, acc 0.875, learning_rate 0.0011094\n","2023-06-28T22:21:32.945571: step 1373, loss 0.319378, acc 0.84375, learning_rate 0.00110841\n","2023-06-28T22:21:33.043124: step 1374, loss 0.329046, acc 0.875, learning_rate 0.00110741\n","2023-06-28T22:21:33.144278: step 1375, loss 0.362596, acc 0.84375, learning_rate 0.00110642\n","2023-06-28T22:21:33.256517: step 1376, loss 0.644182, acc 0.78125, learning_rate 0.00110543\n","2023-06-28T22:21:33.361272: step 1377, loss 0.846733, acc 0.78125, learning_rate 0.00110444\n","2023-06-28T22:21:33.468270: step 1378, loss 0.284148, acc 0.875, learning_rate 0.00110345\n","2023-06-28T22:21:33.570022: step 1379, loss 0.203577, acc 0.875, learning_rate 0.00110246\n","2023-06-28T22:21:33.670475: step 1380, loss 0.107105, acc 0.9375, learning_rate 0.00110148\n","2023-06-28T22:21:33.773213: step 1381, loss 0.325513, acc 0.9375, learning_rate 0.00110049\n","2023-06-28T22:21:33.885459: step 1382, loss 0.238429, acc 0.90625, learning_rate 0.00109951\n","2023-06-28T22:21:33.980130: step 1383, loss 0.255701, acc 0.96875, learning_rate 0.00109852\n","2023-06-28T22:21:34.082402: step 1384, loss 0.264677, acc 0.90625, learning_rate 0.00109754\n","2023-06-28T22:21:34.183801: step 1385, loss 0.441054, acc 0.8125, learning_rate 0.00109656\n","2023-06-28T22:21:34.290205: step 1386, loss 0.489558, acc 0.84375, learning_rate 0.00109557\n","2023-06-28T22:21:34.384194: step 1387, loss 0.250284, acc 0.90625, learning_rate 0.00109459\n","2023-06-28T22:21:34.482154: step 1388, loss 0.303922, acc 0.90625, learning_rate 0.00109361\n","2023-06-28T22:21:34.587906: step 1389, loss 0.157517, acc 0.90625, learning_rate 0.00109264\n","2023-06-28T22:21:34.681841: step 1390, loss 0.322564, acc 0.78125, learning_rate 0.00109166\n","2023-06-28T22:21:34.784481: step 1391, loss 0.209018, acc 0.90625, learning_rate 0.00109068\n","2023-06-28T22:21:34.887690: step 1392, loss 0.265728, acc 0.875, learning_rate 0.00108971\n","2023-06-28T22:21:34.989107: step 1393, loss 0.298213, acc 0.875, learning_rate 0.00108873\n","2023-06-28T22:21:35.089255: step 1394, loss 0.216271, acc 0.90625, learning_rate 0.00108776\n","2023-06-28T22:21:35.198431: step 1395, loss 0.444836, acc 0.84375, learning_rate 0.00108679\n","2023-06-28T22:21:35.299204: step 1396, loss 0.202707, acc 0.90625, learning_rate 0.00108581\n","2023-06-28T22:21:35.395011: step 1397, loss 0.469008, acc 0.8125, learning_rate 0.00108484\n","2023-06-28T22:21:35.498766: step 1398, loss 0.37639, acc 0.875, learning_rate 0.00108387\n","2023-06-28T22:21:35.598786: step 1399, loss 0.40802, acc 0.84375, learning_rate 0.00108291\n","\n","Evaluation:\n","2023-06-28T22:21:36.305468: step 1400, loss 0.566631, acc 0.823584\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1400\n","\n","2023-06-28T22:21:36.519022: step 1400, loss 0.386736, acc 0.8125, learning_rate 0.00108194\n","2023-06-28T22:21:36.616052: step 1401, loss 0.143639, acc 0.9375, learning_rate 0.00108097\n","2023-06-28T22:21:36.706393: step 1402, loss 0.368279, acc 0.90625, learning_rate 0.00108\n","2023-06-28T22:21:36.802036: step 1403, loss 0.225038, acc 0.875, learning_rate 0.00107904\n","2023-06-28T22:21:36.895816: step 1404, loss 0.118492, acc 1, learning_rate 0.00107808\n","2023-06-28T22:21:36.994166: step 1405, loss 0.501821, acc 0.9375, learning_rate 0.00107711\n","2023-06-28T22:21:37.092761: step 1406, loss 0.508842, acc 0.75, learning_rate 0.00107615\n","2023-06-28T22:21:37.206957: step 1407, loss 0.455925, acc 0.8125, learning_rate 0.00107519\n","2023-06-28T22:21:37.306266: step 1408, loss 0.157217, acc 0.96875, learning_rate 0.00107423\n","2023-06-28T22:21:37.398158: step 1409, loss 0.198675, acc 0.96875, learning_rate 0.00107327\n","2023-06-28T22:21:37.497538: step 1410, loss 0.515851, acc 0.9375, learning_rate 0.00107231\n","2023-06-28T22:21:37.590282: step 1411, loss 0.400926, acc 0.90625, learning_rate 0.00107135\n","2023-06-28T22:21:37.689356: step 1412, loss 0.540399, acc 0.8125, learning_rate 0.0010704\n","2023-06-28T22:21:37.781476: step 1413, loss 0.237849, acc 0.875, learning_rate 0.00106944\n","2023-06-28T22:21:37.875756: step 1414, loss 0.548425, acc 0.84375, learning_rate 0.00106849\n","2023-06-28T22:21:37.973821: step 1415, loss 1.07726, acc 0.71875, learning_rate 0.00106753\n","2023-06-28T22:21:38.073604: step 1416, loss 0.168513, acc 0.96875, learning_rate 0.00106658\n","2023-06-28T22:21:38.175627: step 1417, loss 0.485512, acc 0.875, learning_rate 0.00106563\n","2023-06-28T22:21:38.272169: step 1418, loss 0.151006, acc 0.9375, learning_rate 0.00106468\n","2023-06-28T22:21:38.373331: step 1419, loss 0.235343, acc 0.875, learning_rate 0.00106373\n","2023-06-28T22:21:38.474032: step 1420, loss 0.279527, acc 0.90625, learning_rate 0.00106278\n","2023-06-28T22:21:38.615307: step 1421, loss 0.646051, acc 0.71875, learning_rate 0.00106183\n","2023-06-28T22:21:38.713989: step 1422, loss 0.428241, acc 0.84375, learning_rate 0.00106088\n","2023-06-28T22:21:38.810658: step 1423, loss 0.13917, acc 0.9375, learning_rate 0.00105994\n","2023-06-28T22:21:38.906823: step 1424, loss 0.350454, acc 0.84375, learning_rate 0.00105899\n","2023-06-28T22:21:39.007078: step 1425, loss 0.209862, acc 0.9375, learning_rate 0.00105805\n","2023-06-28T22:21:39.103123: step 1426, loss 0.317424, acc 0.84375, learning_rate 0.00105711\n","2023-06-28T22:21:39.201027: step 1427, loss 0.301147, acc 0.8125, learning_rate 0.00105616\n","2023-06-28T22:21:39.301662: step 1428, loss 0.616566, acc 0.84375, learning_rate 0.00105522\n","2023-06-28T22:21:39.401019: step 1429, loss 0.219976, acc 0.875, learning_rate 0.00105428\n","2023-06-28T22:21:39.499716: step 1430, loss 0.0659426, acc 1, learning_rate 0.00105334\n","2023-06-28T22:21:39.597792: step 1431, loss 0.518586, acc 0.875, learning_rate 0.0010524\n","2023-06-28T22:21:39.693117: step 1432, loss 0.223978, acc 0.875, learning_rate 0.00105147\n","2023-06-28T22:21:39.795881: step 1433, loss 0.50816, acc 0.9375, learning_rate 0.00105053\n","2023-06-28T22:21:39.896062: step 1434, loss 0.774054, acc 0.8125, learning_rate 0.00104959\n","2023-06-28T22:21:39.993147: step 1435, loss 0.29336, acc 0.875, learning_rate 0.00104866\n","2023-06-28T22:21:40.095525: step 1436, loss 0.0975681, acc 0.96875, learning_rate 0.00104772\n","2023-06-28T22:21:40.201948: step 1437, loss 0.31933, acc 0.90625, learning_rate 0.00104679\n","2023-06-28T22:21:40.306835: step 1438, loss 0.350184, acc 0.90625, learning_rate 0.00104586\n","2023-06-28T22:21:40.412231: step 1439, loss 0.59276, acc 0.875, learning_rate 0.00104493\n","2023-06-28T22:21:40.508548: step 1440, loss 0.410023, acc 0.84375, learning_rate 0.001044\n","2023-06-28T22:21:40.603016: step 1441, loss 0.345664, acc 0.90625, learning_rate 0.00104307\n","2023-06-28T22:21:40.703040: step 1442, loss 0.22922, acc 0.90625, learning_rate 0.00104214\n","2023-06-28T22:21:40.808901: step 1443, loss 0.331356, acc 0.875, learning_rate 0.00104121\n","2023-06-28T22:21:40.907530: step 1444, loss 0.397055, acc 0.84375, learning_rate 0.00104028\n","2023-06-28T22:21:41.031323: step 1445, loss 0.201133, acc 0.875, learning_rate 0.00103936\n","2023-06-28T22:21:41.200163: step 1446, loss 0.177738, acc 0.9375, learning_rate 0.00103843\n","2023-06-28T22:21:41.363263: step 1447, loss 0.562632, acc 0.90625, learning_rate 0.00103751\n","2023-06-28T22:21:41.540450: step 1448, loss 0.30321, acc 0.84375, learning_rate 0.00103659\n","2023-06-28T22:21:41.714618: step 1449, loss 0.330546, acc 0.8125, learning_rate 0.00103566\n","2023-06-28T22:21:41.889299: step 1450, loss 0.324431, acc 0.875, learning_rate 0.00103474\n","2023-06-28T22:21:42.057465: step 1451, loss 0.326125, acc 0.875, learning_rate 0.00103382\n","2023-06-28T22:21:42.226652: step 1452, loss 0.513409, acc 0.875, learning_rate 0.0010329\n","2023-06-28T22:21:42.394741: step 1453, loss 0.280121, acc 0.84375, learning_rate 0.00103198\n","2023-06-28T22:21:42.566676: step 1454, loss 0.15185, acc 0.96875, learning_rate 0.00103107\n","2023-06-28T22:21:42.715007: step 1455, loss 0.351814, acc 0.90625, learning_rate 0.00103015\n","2023-06-28T22:21:42.891286: step 1456, loss 0.201438, acc 0.9375, learning_rate 0.00102923\n","2023-06-28T22:21:43.060957: step 1457, loss 0.454751, acc 0.9375, learning_rate 0.00102832\n","2023-06-28T22:21:43.234189: step 1458, loss 0.308522, acc 0.84375, learning_rate 0.0010274\n","2023-06-28T22:21:43.408669: step 1459, loss 0.322748, acc 0.875, learning_rate 0.00102649\n","2023-06-28T22:21:43.577648: step 1460, loss 0.235527, acc 0.90625, learning_rate 0.00102558\n","2023-06-28T22:21:43.745304: step 1461, loss 0.523842, acc 0.875, learning_rate 0.00102467\n","2023-06-28T22:21:43.898926: step 1462, loss 0.25001, acc 0.90625, learning_rate 0.00102376\n","2023-06-28T22:21:44.089871: step 1463, loss 0.183406, acc 0.9375, learning_rate 0.00102285\n","2023-06-28T22:21:44.275437: step 1464, loss 0.193393, acc 0.875, learning_rate 0.00102194\n","2023-06-28T22:21:44.454349: step 1465, loss 0.125802, acc 0.9375, learning_rate 0.00102103\n","2023-06-28T22:21:44.637471: step 1466, loss 0.149766, acc 0.9375, learning_rate 0.00102012\n","2023-06-28T22:21:44.814559: step 1467, loss 0.313323, acc 0.90625, learning_rate 0.00101922\n","2023-06-28T22:21:44.977453: step 1468, loss 0.360934, acc 0.8125, learning_rate 0.00101831\n","2023-06-28T22:21:45.147909: step 1469, loss 0.247407, acc 0.90625, learning_rate 0.00101741\n","2023-06-28T22:21:45.308015: step 1470, loss 0.294386, acc 0.875, learning_rate 0.0010165\n","2023-06-28T22:21:45.462076: step 1471, loss 0.144411, acc 0.9375, learning_rate 0.0010156\n","2023-06-28T22:21:45.637189: step 1472, loss 0.123443, acc 0.9375, learning_rate 0.0010147\n","2023-06-28T22:21:45.805164: step 1473, loss 0.616819, acc 0.875, learning_rate 0.0010138\n","2023-06-28T22:21:45.967223: step 1474, loss 0.244778, acc 0.875, learning_rate 0.0010129\n","2023-06-28T22:21:46.131089: step 1475, loss 0.234177, acc 0.90625, learning_rate 0.001012\n","2023-06-28T22:21:46.299613: step 1476, loss 0.174435, acc 0.875, learning_rate 0.0010111\n","2023-06-28T22:21:46.465647: step 1477, loss 0.296574, acc 0.90625, learning_rate 0.00101021\n","2023-06-28T22:21:46.630166: step 1478, loss 0.163247, acc 0.9375, learning_rate 0.00100931\n","2023-06-28T22:21:46.789618: step 1479, loss 0.195731, acc 0.90625, learning_rate 0.00100841\n","2023-06-28T22:21:46.958968: step 1480, loss 0.447721, acc 0.96875, learning_rate 0.00100752\n","2023-06-28T22:21:47.137325: step 1481, loss 0.278953, acc 0.90625, learning_rate 0.00100663\n","2023-06-28T22:21:47.285076: step 1482, loss 0.139929, acc 0.96875, learning_rate 0.00100573\n","2023-06-28T22:21:47.443237: step 1483, loss 0.522494, acc 0.90625, learning_rate 0.00100484\n","2023-06-28T22:21:47.599193: step 1484, loss 0.519798, acc 0.84375, learning_rate 0.00100395\n","2023-06-28T22:21:47.763260: step 1485, loss 0.318263, acc 0.8125, learning_rate 0.00100306\n","2023-06-28T22:21:47.932001: step 1486, loss 0.448215, acc 0.875, learning_rate 0.00100217\n","2023-06-28T22:21:48.086127: step 1487, loss 0.192801, acc 0.90625, learning_rate 0.00100128\n","2023-06-28T22:21:48.244811: step 1488, loss 0.253738, acc 0.875, learning_rate 0.0010004\n","2023-06-28T22:21:48.390130: step 1489, loss 0.256541, acc 0.8125, learning_rate 0.00099951\n","2023-06-28T22:21:48.579216: step 1490, loss 0.431742, acc 0.84375, learning_rate 0.000998624\n","2023-06-28T22:21:48.739722: step 1491, loss 0.35193, acc 0.84375, learning_rate 0.000997739\n","2023-06-28T22:21:48.896123: step 1492, loss 0.30592, acc 0.96875, learning_rate 0.000996855\n","2023-06-28T22:21:49.043907: step 1493, loss 0.124174, acc 0.96875, learning_rate 0.000995972\n","2023-06-28T22:21:49.221090: step 1494, loss 0.258134, acc 0.875, learning_rate 0.00099509\n","2023-06-28T22:21:49.379070: step 1495, loss 0.205295, acc 0.9375, learning_rate 0.000994208\n","2023-06-28T22:21:49.540606: step 1496, loss 0.760964, acc 0.75, learning_rate 0.000993328\n","2023-06-28T22:21:49.679504: step 1497, loss 0.56239, acc 0.84375, learning_rate 0.000992448\n","2023-06-28T22:21:49.833101: step 1498, loss 0.44295, acc 0.78125, learning_rate 0.000991569\n","2023-06-28T22:21:49.989926: step 1499, loss 0.233368, acc 0.9375, learning_rate 0.000990692\n","\n","Evaluation:\n","2023-06-28T22:21:51.225740: step 1500, loss 0.61286, acc 0.822352\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1500\n","\n","2023-06-28T22:21:51.521792: step 1500, loss 0.726741, acc 0.71875, learning_rate 0.000989815\n","2023-06-28T22:21:51.689175: step 1501, loss 0.295087, acc 0.84375, learning_rate 0.000988938\n","2023-06-28T22:21:51.793192: step 1502, loss 0.52341, acc 0.75, learning_rate 0.000988063\n","2023-06-28T22:21:51.900782: step 1503, loss 0.452714, acc 0.84375, learning_rate 0.000987189\n","2023-06-28T22:21:52.002640: step 1504, loss 0.259814, acc 0.90625, learning_rate 0.000986315\n","2023-06-28T22:21:52.110996: step 1505, loss 0.253354, acc 0.84375, learning_rate 0.000985442\n","2023-06-28T22:21:52.205250: step 1506, loss 0.275701, acc 0.875, learning_rate 0.00098457\n","2023-06-28T22:21:52.305460: step 1507, loss 0.164791, acc 0.90625, learning_rate 0.000983699\n","2023-06-28T22:21:52.398752: step 1508, loss 0.148742, acc 0.96875, learning_rate 0.000982829\n","2023-06-28T22:21:52.497506: step 1509, loss 0.229905, acc 0.9375, learning_rate 0.00098196\n","2023-06-28T22:21:52.623033: step 1510, loss 0.334669, acc 0.8125, learning_rate 0.000981092\n","2023-06-28T22:21:52.724623: step 1511, loss 0.362143, acc 0.8125, learning_rate 0.000980224\n","2023-06-28T22:21:52.825740: step 1512, loss 0.390961, acc 0.90625, learning_rate 0.000979357\n","2023-06-28T22:21:52.931287: step 1513, loss 0.543915, acc 0.84375, learning_rate 0.000978491\n","2023-06-28T22:21:53.027013: step 1514, loss 0.373681, acc 0.8125, learning_rate 0.000977626\n","2023-06-28T22:21:53.128286: step 1515, loss 0.213415, acc 0.9375, learning_rate 0.000976762\n","2023-06-28T22:21:53.228377: step 1516, loss 0.214646, acc 0.90625, learning_rate 0.000975899\n","2023-06-28T22:21:53.327828: step 1517, loss 0.250659, acc 0.9375, learning_rate 0.000975036\n","2023-06-28T22:21:53.421391: step 1518, loss 0.113491, acc 1, learning_rate 0.000974175\n","2023-06-28T22:21:53.517694: step 1519, loss 0.455114, acc 0.8125, learning_rate 0.000973314\n","2023-06-28T22:21:53.614280: step 1520, loss 0.491073, acc 0.875, learning_rate 0.000972454\n","2023-06-28T22:21:53.713434: step 1521, loss 0.235148, acc 0.875, learning_rate 0.000971595\n","2023-06-28T22:21:53.807653: step 1522, loss 0.625084, acc 0.71875, learning_rate 0.000970737\n","2023-06-28T22:21:53.910589: step 1523, loss 0.420203, acc 0.8125, learning_rate 0.00096988\n","2023-06-28T22:21:54.008188: step 1524, loss 0.217051, acc 0.90625, learning_rate 0.000969023\n","2023-06-28T22:21:54.110947: step 1525, loss 0.514475, acc 0.875, learning_rate 0.000968167\n","2023-06-28T22:21:54.214693: step 1526, loss 0.28127, acc 0.875, learning_rate 0.000967312\n","2023-06-28T22:21:54.313470: step 1527, loss 0.24722, acc 0.9375, learning_rate 0.000966458\n","2023-06-28T22:21:54.407534: step 1528, loss 0.908264, acc 0.8125, learning_rate 0.000965605\n","2023-06-28T22:21:54.513836: step 1529, loss 0.265963, acc 0.84375, learning_rate 0.000964753\n","2023-06-28T22:21:54.609550: step 1530, loss 0.433864, acc 0.875, learning_rate 0.000963901\n","2023-06-28T22:21:54.707652: step 1531, loss 0.449573, acc 0.8125, learning_rate 0.000963051\n","2023-06-28T22:21:54.813140: step 1532, loss 0.142706, acc 0.90625, learning_rate 0.000962201\n","2023-06-28T22:21:54.924229: step 1533, loss 0.205092, acc 0.9375, learning_rate 0.000961352\n","2023-06-28T22:21:55.019945: step 1534, loss 0.315759, acc 0.84375, learning_rate 0.000960504\n","2023-06-28T22:21:55.123907: step 1535, loss 0.222515, acc 0.90625, learning_rate 0.000959657\n","2023-06-28T22:21:55.226033: step 1536, loss 0.392176, acc 0.875, learning_rate 0.00095881\n","2023-06-28T22:21:55.327146: step 1537, loss 0.230395, acc 0.875, learning_rate 0.000957964\n","2023-06-28T22:21:55.419280: step 1538, loss 0.633886, acc 0.84375, learning_rate 0.00095712\n","2023-06-28T22:21:55.514816: step 1539, loss 0.268661, acc 0.84375, learning_rate 0.000956276\n","2023-06-28T22:21:55.620462: step 1540, loss 0.716894, acc 0.875, learning_rate 0.000955433\n","2023-06-28T22:21:55.720751: step 1541, loss 0.343988, acc 0.84375, learning_rate 0.00095459\n","2023-06-28T22:21:55.823559: step 1542, loss 0.113819, acc 0.96875, learning_rate 0.000953749\n","2023-06-28T22:21:55.935927: step 1543, loss 0.280462, acc 0.875, learning_rate 0.000952908\n","2023-06-28T22:21:56.037304: step 1544, loss 0.201079, acc 0.9375, learning_rate 0.000952068\n","2023-06-28T22:21:56.137024: step 1545, loss 0.0909556, acc 0.96875, learning_rate 0.000951229\n","2023-06-28T22:21:56.242636: step 1546, loss 0.746221, acc 0.8125, learning_rate 0.000950391\n","2023-06-28T22:21:56.338869: step 1547, loss 0.194974, acc 0.9375, learning_rate 0.000949554\n","2023-06-28T22:21:56.440332: step 1548, loss 0.260629, acc 0.8125, learning_rate 0.000948717\n","2023-06-28T22:21:56.545003: step 1549, loss 0.472942, acc 0.8125, learning_rate 0.000947882\n","2023-06-28T22:21:56.641785: step 1550, loss 0.364925, acc 0.875, learning_rate 0.000947047\n","2023-06-28T22:21:56.739967: step 1551, loss 0.237712, acc 0.96875, learning_rate 0.000946213\n","2023-06-28T22:21:56.837117: step 1552, loss 0.550629, acc 0.875, learning_rate 0.000945379\n","2023-06-28T22:21:56.927938: step 1553, loss 0.210564, acc 0.875, learning_rate 0.000944547\n","2023-06-28T22:21:57.039636: step 1554, loss 0.303701, acc 0.875, learning_rate 0.000943715\n","2023-06-28T22:21:57.144767: step 1555, loss 0.262898, acc 0.875, learning_rate 0.000942885\n","2023-06-28T22:21:57.239635: step 1556, loss 0.504998, acc 0.78125, learning_rate 0.000942055\n","2023-06-28T22:21:57.339885: step 1557, loss 0.302857, acc 0.90625, learning_rate 0.000941225\n","2023-06-28T22:21:57.433086: step 1558, loss 0.652671, acc 0.75, learning_rate 0.000940397\n","2023-06-28T22:21:57.533523: step 1559, loss 0.877598, acc 0.75, learning_rate 0.00093957\n","2023-06-28T22:21:57.631928: step 1560, loss 0.328263, acc 0.875, learning_rate 0.000938743\n","2023-06-28T22:21:57.733301: step 1561, loss 0.445059, acc 0.84375, learning_rate 0.000937917\n","2023-06-28T22:21:57.832481: step 1562, loss 0.45688, acc 0.8125, learning_rate 0.000937092\n","2023-06-28T22:21:57.937123: step 1563, loss 0.159952, acc 0.96875, learning_rate 0.000936268\n","2023-06-28T22:21:58.051133: step 1564, loss 0.358565, acc 0.875, learning_rate 0.000935444\n","2023-06-28T22:21:58.164529: step 1565, loss 0.616812, acc 0.8125, learning_rate 0.000934622\n","2023-06-28T22:21:58.265020: step 1566, loss 0.428063, acc 0.875, learning_rate 0.0009338\n","2023-06-28T22:21:58.369303: step 1567, loss 0.583184, acc 0.8125, learning_rate 0.000932979\n","2023-06-28T22:21:58.466233: step 1568, loss 0.427403, acc 0.84375, learning_rate 0.000932159\n","2023-06-28T22:21:58.567406: step 1569, loss 0.178371, acc 0.90625, learning_rate 0.000931339\n","2023-06-28T22:21:58.674633: step 1570, loss 0.659454, acc 0.84375, learning_rate 0.000930521\n","2023-06-28T22:21:58.770316: step 1571, loss 0.476809, acc 0.875, learning_rate 0.000929703\n","2023-06-28T22:21:58.882937: step 1572, loss 0.647847, acc 0.8125, learning_rate 0.000928886\n","2023-06-28T22:21:58.993789: step 1573, loss 0.652393, acc 0.84375, learning_rate 0.00092807\n","2023-06-28T22:21:59.092685: step 1574, loss 0.301191, acc 0.90625, learning_rate 0.000927254\n","2023-06-28T22:21:59.235010: step 1575, loss 0.541469, acc 0.75, learning_rate 0.00092644\n","2023-06-28T22:21:59.331364: step 1576, loss 0.211817, acc 0.9375, learning_rate 0.000925626\n","2023-06-28T22:21:59.429535: step 1577, loss 0.329311, acc 0.90625, learning_rate 0.000924813\n","2023-06-28T22:21:59.528897: step 1578, loss 0.319936, acc 0.875, learning_rate 0.000924001\n","2023-06-28T22:21:59.631132: step 1579, loss 0.310385, acc 0.875, learning_rate 0.00092319\n","2023-06-28T22:21:59.726649: step 1580, loss 0.203681, acc 0.90625, learning_rate 0.000922379\n","2023-06-28T22:21:59.825854: step 1581, loss 0.340134, acc 0.875, learning_rate 0.000921569\n","2023-06-28T22:21:59.920551: step 1582, loss 0.221863, acc 0.96875, learning_rate 0.00092076\n","2023-06-28T22:22:00.020556: step 1583, loss 0.579796, acc 0.875, learning_rate 0.000919952\n","2023-06-28T22:22:00.127019: step 1584, loss 0.29251, acc 0.90625, learning_rate 0.000919145\n","2023-06-28T22:22:00.229696: step 1585, loss 0.151441, acc 0.96875, learning_rate 0.000918338\n","2023-06-28T22:22:00.336029: step 1586, loss 0.366487, acc 0.875, learning_rate 0.000917532\n","2023-06-28T22:22:00.435874: step 1587, loss 0.181798, acc 0.875, learning_rate 0.000916727\n","2023-06-28T22:22:00.538336: step 1588, loss 0.294584, acc 0.875, learning_rate 0.000915923\n","2023-06-28T22:22:00.632947: step 1589, loss 0.324317, acc 0.875, learning_rate 0.00091512\n","2023-06-28T22:22:00.732875: step 1590, loss 0.152377, acc 0.96875, learning_rate 0.000914317\n","2023-06-28T22:22:00.828593: step 1591, loss 0.172778, acc 0.90625, learning_rate 0.000913515\n","2023-06-28T22:22:00.922229: step 1592, loss 0.445406, acc 0.9375, learning_rate 0.000912714\n","2023-06-28T22:22:01.024001: step 1593, loss 0.409182, acc 0.90625, learning_rate 0.000911914\n","2023-06-28T22:22:01.127242: step 1594, loss 0.210751, acc 0.90625, learning_rate 0.000911115\n","2023-06-28T22:22:01.221379: step 1595, loss 0.51056, acc 0.8125, learning_rate 0.000910316\n","2023-06-28T22:22:01.322639: step 1596, loss 0.431081, acc 0.90625, learning_rate 0.000909518\n","2023-06-28T22:22:01.418288: step 1597, loss 0.249984, acc 0.90625, learning_rate 0.000908721\n","2023-06-28T22:22:01.523603: step 1598, loss 0.204208, acc 0.875, learning_rate 0.000907925\n","2023-06-28T22:22:01.625864: step 1599, loss 0.212642, acc 0.90625, learning_rate 0.000907129\n","\n","Evaluation:\n","2023-06-28T22:22:02.731397: step 1600, loss 0.600156, acc 0.824199\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1600\n","\n","2023-06-28T22:22:03.046215: step 1600, loss 0.0709167, acc 0.96875, learning_rate 0.000906334\n","2023-06-28T22:22:03.228345: step 1601, loss 0.276945, acc 0.84375, learning_rate 0.00090554\n","2023-06-28T22:22:03.392370: step 1602, loss 0.777134, acc 0.84375, learning_rate 0.000904747\n","2023-06-28T22:22:03.569520: step 1603, loss 0.221592, acc 0.875, learning_rate 0.000903955\n","2023-06-28T22:22:03.739166: step 1604, loss 0.320906, acc 0.9375, learning_rate 0.000903163\n","2023-06-28T22:22:03.909258: step 1605, loss 0.278283, acc 0.9375, learning_rate 0.000902372\n","2023-06-28T22:22:04.087303: step 1606, loss 0.255043, acc 0.90625, learning_rate 0.000901582\n","2023-06-28T22:22:04.257935: step 1607, loss 0.270308, acc 0.875, learning_rate 0.000900793\n","2023-06-28T22:22:04.430612: step 1608, loss 0.487764, acc 0.90625, learning_rate 0.000900004\n","2023-06-28T22:22:04.615777: step 1609, loss 0.396751, acc 0.875, learning_rate 0.000899217\n","2023-06-28T22:22:04.772965: step 1610, loss 0.348204, acc 0.90625, learning_rate 0.00089843\n","2023-06-28T22:22:04.936466: step 1611, loss 0.261995, acc 0.9375, learning_rate 0.000897644\n","2023-06-28T22:22:05.108137: step 1612, loss 0.193099, acc 0.9375, learning_rate 0.000896858\n","2023-06-28T22:22:05.285145: step 1613, loss 0.315985, acc 0.8125, learning_rate 0.000896074\n","2023-06-28T22:22:05.454360: step 1614, loss 0.280248, acc 0.90625, learning_rate 0.00089529\n","2023-06-28T22:22:05.627206: step 1615, loss 0.733011, acc 0.84375, learning_rate 0.000894507\n","2023-06-28T22:22:05.799634: step 1616, loss 0.265335, acc 0.90625, learning_rate 0.000893724\n","2023-06-28T22:22:05.968256: step 1617, loss 0.101015, acc 0.96875, learning_rate 0.000892943\n","2023-06-28T22:22:06.142449: step 1618, loss 0.198693, acc 0.90625, learning_rate 0.000892162\n","2023-06-28T22:22:06.298901: step 1619, loss 0.462225, acc 0.875, learning_rate 0.000891382\n","2023-06-28T22:22:06.475542: step 1620, loss 0.444267, acc 0.84375, learning_rate 0.000890603\n","2023-06-28T22:22:06.658351: step 1621, loss 0.562651, acc 0.875, learning_rate 0.000889824\n","2023-06-28T22:22:06.823105: step 1622, loss 0.0743422, acc 0.96875, learning_rate 0.000889047\n","2023-06-28T22:22:06.994644: step 1623, loss 0.303059, acc 0.90625, learning_rate 0.00088827\n","2023-06-28T22:22:07.142708: step 1624, loss 0.842894, acc 0.78125, learning_rate 0.000887493\n","2023-06-28T22:22:07.315968: step 1625, loss 0.368199, acc 0.78125, learning_rate 0.000886718\n","2023-06-28T22:22:07.477368: step 1626, loss 0.38487, acc 0.875, learning_rate 0.000885943\n","2023-06-28T22:22:07.598770: step 1627, loss 0.000991371, acc 1, learning_rate 0.000885169\n","2023-06-28T22:22:07.772118: step 1628, loss 0.164348, acc 0.90625, learning_rate 0.000884396\n","2023-06-28T22:22:07.940798: step 1629, loss 0.311255, acc 0.875, learning_rate 0.000883624\n","2023-06-28T22:22:08.097482: step 1630, loss 0.254366, acc 0.9375, learning_rate 0.000882852\n","2023-06-28T22:22:08.255089: step 1631, loss 0.252921, acc 0.875, learning_rate 0.000882081\n","2023-06-28T22:22:08.414972: step 1632, loss 0.128715, acc 0.96875, learning_rate 0.000881311\n","2023-06-28T22:22:08.588299: step 1633, loss 0.139631, acc 0.96875, learning_rate 0.000880542\n","2023-06-28T22:22:08.768702: step 1634, loss 0.17489, acc 0.875, learning_rate 0.000879774\n","2023-06-28T22:22:08.932487: step 1635, loss 0.0618563, acc 1, learning_rate 0.000879006\n","2023-06-28T22:22:09.084267: step 1636, loss 0.364725, acc 0.8125, learning_rate 0.000878239\n","2023-06-28T22:22:09.285740: step 1637, loss 0.144102, acc 0.90625, learning_rate 0.000877472\n","2023-06-28T22:22:09.443432: step 1638, loss 0.223379, acc 0.90625, learning_rate 0.000876707\n","2023-06-28T22:22:09.601437: step 1639, loss 0.0678648, acc 0.96875, learning_rate 0.000875942\n","2023-06-28T22:22:09.768667: step 1640, loss 0.230525, acc 0.90625, learning_rate 0.000875178\n","2023-06-28T22:22:09.927611: step 1641, loss 0.162592, acc 0.90625, learning_rate 0.000874415\n","2023-06-28T22:22:10.100037: step 1642, loss 0.275573, acc 0.875, learning_rate 0.000873652\n","2023-06-28T22:22:10.279197: step 1643, loss 0.146324, acc 0.96875, learning_rate 0.00087289\n","2023-06-28T22:22:10.433299: step 1644, loss 0.41675, acc 0.84375, learning_rate 0.000872129\n","2023-06-28T22:22:10.603005: step 1645, loss 0.241629, acc 0.875, learning_rate 0.000871369\n","2023-06-28T22:22:10.755428: step 1646, loss 0.236361, acc 0.90625, learning_rate 0.00087061\n","2023-06-28T22:22:10.931038: step 1647, loss 0.180151, acc 0.96875, learning_rate 0.000869851\n","2023-06-28T22:22:11.099765: step 1648, loss 0.213961, acc 0.9375, learning_rate 0.000869093\n","2023-06-28T22:22:11.270316: step 1649, loss 0.277886, acc 0.9375, learning_rate 0.000868335\n","2023-06-28T22:22:11.459388: step 1650, loss 0.151586, acc 0.96875, learning_rate 0.000867579\n","2023-06-28T22:22:11.627144: step 1651, loss 0.0794505, acc 0.96875, learning_rate 0.000866823\n","2023-06-28T22:22:11.796660: step 1652, loss 0.204361, acc 0.90625, learning_rate 0.000866068\n","2023-06-28T22:22:11.942275: step 1653, loss 0.266484, acc 0.84375, learning_rate 0.000865314\n","2023-06-28T22:22:12.095516: step 1654, loss 0.089062, acc 0.9375, learning_rate 0.00086456\n","2023-06-28T22:22:12.283742: step 1655, loss 0.298517, acc 0.90625, learning_rate 0.000863807\n","2023-06-28T22:22:12.459807: step 1656, loss 0.100406, acc 0.9375, learning_rate 0.000863055\n","2023-06-28T22:22:12.635350: step 1657, loss 0.142912, acc 0.9375, learning_rate 0.000862304\n","2023-06-28T22:22:12.738344: step 1658, loss 0.28615, acc 0.84375, learning_rate 0.000861553\n","2023-06-28T22:22:12.834615: step 1659, loss 0.261457, acc 0.84375, learning_rate 0.000860803\n","2023-06-28T22:22:12.936538: step 1660, loss 0.158928, acc 0.9375, learning_rate 0.000860054\n","2023-06-28T22:22:13.037435: step 1661, loss 0.128006, acc 0.96875, learning_rate 0.000859306\n","2023-06-28T22:22:13.140359: step 1662, loss 0.306034, acc 0.9375, learning_rate 0.000858558\n","2023-06-28T22:22:13.244922: step 1663, loss 0.534701, acc 0.84375, learning_rate 0.000857811\n","2023-06-28T22:22:13.340810: step 1664, loss 0.36979, acc 0.9375, learning_rate 0.000857065\n","2023-06-28T22:22:13.435573: step 1665, loss 0.160939, acc 0.9375, learning_rate 0.00085632\n","2023-06-28T22:22:13.550563: step 1666, loss 0.23521, acc 0.9375, learning_rate 0.000855575\n","2023-06-28T22:22:13.649985: step 1667, loss 0.215678, acc 0.90625, learning_rate 0.000854831\n","2023-06-28T22:22:13.754697: step 1668, loss 0.637472, acc 0.90625, learning_rate 0.000854088\n","2023-06-28T22:22:13.863578: step 1669, loss 0.27317, acc 0.90625, learning_rate 0.000853345\n","2023-06-28T22:22:13.963623: step 1670, loss 0.280586, acc 0.875, learning_rate 0.000852603\n","2023-06-28T22:22:14.065908: step 1671, loss 0.435108, acc 0.84375, learning_rate 0.000851862\n","2023-06-28T22:22:14.175964: step 1672, loss 0.244377, acc 0.9375, learning_rate 0.000851122\n","2023-06-28T22:22:14.293954: step 1673, loss 0.0960135, acc 0.96875, learning_rate 0.000850382\n","2023-06-28T22:22:14.393422: step 1674, loss 0.228747, acc 0.875, learning_rate 0.000849643\n","2023-06-28T22:22:14.504644: step 1675, loss 0.382078, acc 0.78125, learning_rate 0.000848905\n","2023-06-28T22:22:14.610409: step 1676, loss 0.588677, acc 0.78125, learning_rate 0.000848168\n","2023-06-28T22:22:14.708442: step 1677, loss 0.28992, acc 0.84375, learning_rate 0.000847431\n","2023-06-28T22:22:14.816927: step 1678, loss 0.0734844, acc 0.96875, learning_rate 0.000846695\n","2023-06-28T22:22:14.914659: step 1679, loss 0.197357, acc 0.90625, learning_rate 0.00084596\n","2023-06-28T22:22:15.014663: step 1680, loss 0.289001, acc 0.875, learning_rate 0.000845225\n","2023-06-28T22:22:15.121005: step 1681, loss 0.165436, acc 0.9375, learning_rate 0.000844492\n","2023-06-28T22:22:15.221159: step 1682, loss 0.379539, acc 0.9375, learning_rate 0.000843759\n","2023-06-28T22:22:15.316457: step 1683, loss 0.122813, acc 0.96875, learning_rate 0.000843026\n","2023-06-28T22:22:15.412601: step 1684, loss 0.351342, acc 0.90625, learning_rate 0.000842295\n","2023-06-28T22:22:15.513024: step 1685, loss 0.179457, acc 0.9375, learning_rate 0.000841564\n","2023-06-28T22:22:15.624025: step 1686, loss 0.0933504, acc 1, learning_rate 0.000840834\n","2023-06-28T22:22:15.726283: step 1687, loss 0.275902, acc 0.9375, learning_rate 0.000840104\n","2023-06-28T22:22:15.821070: step 1688, loss 0.235933, acc 0.875, learning_rate 0.000839375\n","2023-06-28T22:22:15.914686: step 1689, loss 0.351855, acc 0.84375, learning_rate 0.000838647\n","2023-06-28T22:22:16.017996: step 1690, loss 0.286764, acc 0.9375, learning_rate 0.00083792\n","2023-06-28T22:22:16.123985: step 1691, loss 0.385211, acc 0.875, learning_rate 0.000837193\n","2023-06-28T22:22:16.226133: step 1692, loss 0.213303, acc 0.9375, learning_rate 0.000836467\n","2023-06-28T22:22:16.338280: step 1693, loss 0.265455, acc 0.9375, learning_rate 0.000835742\n","2023-06-28T22:22:16.438954: step 1694, loss 0.262876, acc 0.875, learning_rate 0.000835018\n","2023-06-28T22:22:16.540993: step 1695, loss 0.244375, acc 0.875, learning_rate 0.000834294\n","2023-06-28T22:22:16.651803: step 1696, loss 0.367318, acc 0.90625, learning_rate 0.000833571\n","2023-06-28T22:22:16.746446: step 1697, loss 0.264329, acc 0.90625, learning_rate 0.000832849\n","2023-06-28T22:22:16.841895: step 1698, loss 0.119364, acc 0.96875, learning_rate 0.000832127\n","2023-06-28T22:22:16.941326: step 1699, loss 0.220142, acc 0.90625, learning_rate 0.000831406\n","\n","Evaluation:\n","2023-06-28T22:22:17.670766: step 1700, loss 0.585248, acc 0.825123\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1700\n","\n","2023-06-28T22:22:17.888368: step 1700, loss 0.172947, acc 0.90625, learning_rate 0.000830686\n","2023-06-28T22:22:17.992302: step 1701, loss 0.173579, acc 0.9375, learning_rate 0.000829967\n","2023-06-28T22:22:18.094189: step 1702, loss 0.176283, acc 0.9375, learning_rate 0.000829248\n","2023-06-28T22:22:18.195364: step 1703, loss 0.396693, acc 0.84375, learning_rate 0.00082853\n","2023-06-28T22:22:18.300456: step 1704, loss 0.198051, acc 0.90625, learning_rate 0.000827812\n","2023-06-28T22:22:18.401719: step 1705, loss 0.132235, acc 0.9375, learning_rate 0.000827096\n","2023-06-28T22:22:18.500109: step 1706, loss 0.190583, acc 0.90625, learning_rate 0.00082638\n","2023-06-28T22:22:18.599852: step 1707, loss 0.153449, acc 0.9375, learning_rate 0.000825665\n","2023-06-28T22:22:18.704369: step 1708, loss 0.163059, acc 0.9375, learning_rate 0.00082495\n","2023-06-28T22:22:18.802248: step 1709, loss 0.292357, acc 0.90625, learning_rate 0.000824236\n","2023-06-28T22:22:18.911063: step 1710, loss 0.217649, acc 0.90625, learning_rate 0.000823523\n","2023-06-28T22:22:19.012913: step 1711, loss 0.0900159, acc 1, learning_rate 0.000822811\n","2023-06-28T22:22:19.113835: step 1712, loss 0.242827, acc 0.90625, learning_rate 0.000822099\n","2023-06-28T22:22:19.213777: step 1713, loss 0.135654, acc 0.96875, learning_rate 0.000821388\n","2023-06-28T22:22:19.317033: step 1714, loss 0.11616, acc 0.96875, learning_rate 0.000820678\n","2023-06-28T22:22:19.423854: step 1715, loss 0.327896, acc 0.84375, learning_rate 0.000819968\n","2023-06-28T22:22:19.531529: step 1716, loss 0.47111, acc 0.84375, learning_rate 0.000819259\n","2023-06-28T22:22:19.628079: step 1717, loss 0.47745, acc 0.90625, learning_rate 0.000818551\n","2023-06-28T22:22:19.739673: step 1718, loss 0.224345, acc 0.90625, learning_rate 0.000817843\n","2023-06-28T22:22:19.841399: step 1719, loss 0.267948, acc 0.9375, learning_rate 0.000817136\n","2023-06-28T22:22:19.947785: step 1720, loss 0.213702, acc 0.875, learning_rate 0.00081643\n","2023-06-28T22:22:20.043325: step 1721, loss 0.222152, acc 0.90625, learning_rate 0.000815725\n","2023-06-28T22:22:20.152793: step 1722, loss 0.487478, acc 0.78125, learning_rate 0.00081502\n","2023-06-28T22:22:20.253910: step 1723, loss 0.0525386, acc 0.96875, learning_rate 0.000814316\n","2023-06-28T22:22:20.363471: step 1724, loss 0.155647, acc 0.96875, learning_rate 0.000813613\n","2023-06-28T22:22:20.463799: step 1725, loss 0.16708, acc 0.9375, learning_rate 0.00081291\n","2023-06-28T22:22:20.570619: step 1726, loss 0.0690496, acc 0.96875, learning_rate 0.000812208\n","2023-06-28T22:22:20.678420: step 1727, loss 0.254384, acc 0.84375, learning_rate 0.000811507\n","2023-06-28T22:22:20.798066: step 1728, loss 0.238097, acc 0.90625, learning_rate 0.000810806\n","2023-06-28T22:22:20.891517: step 1729, loss 0.337118, acc 0.875, learning_rate 0.000810106\n","2023-06-28T22:22:20.992492: step 1730, loss 0.234578, acc 0.875, learning_rate 0.000809407\n","2023-06-28T22:22:21.086718: step 1731, loss 0.104624, acc 0.96875, learning_rate 0.000808709\n","2023-06-28T22:22:21.184696: step 1732, loss 0.277425, acc 0.84375, learning_rate 0.000808011\n","2023-06-28T22:22:21.283949: step 1733, loss 0.227268, acc 0.90625, learning_rate 0.000807314\n","2023-06-28T22:22:21.374474: step 1734, loss 0.131028, acc 0.96875, learning_rate 0.000806617\n","2023-06-28T22:22:21.474202: step 1735, loss 0.251873, acc 0.90625, learning_rate 0.000805921\n","2023-06-28T22:22:21.583632: step 1736, loss 0.13623, acc 1, learning_rate 0.000805226\n","2023-06-28T22:22:21.687352: step 1737, loss 0.368741, acc 0.90625, learning_rate 0.000804532\n","2023-06-28T22:22:21.802841: step 1738, loss 0.224104, acc 0.9375, learning_rate 0.000803838\n","2023-06-28T22:22:21.898943: step 1739, loss 0.0935258, acc 1, learning_rate 0.000803145\n","2023-06-28T22:22:22.000621: step 1740, loss 0.431896, acc 0.84375, learning_rate 0.000802453\n","2023-06-28T22:22:22.114621: step 1741, loss 0.156644, acc 0.9375, learning_rate 0.000801761\n","2023-06-28T22:22:22.214831: step 1742, loss 0.0685612, acc 0.96875, learning_rate 0.00080107\n","2023-06-28T22:22:22.312099: step 1743, loss 0.164123, acc 0.84375, learning_rate 0.00080038\n","2023-06-28T22:22:22.401164: step 1744, loss 0.132198, acc 0.96875, learning_rate 0.00079969\n","2023-06-28T22:22:22.506748: step 1745, loss 0.185698, acc 0.90625, learning_rate 0.000799001\n","2023-06-28T22:22:22.606121: step 1746, loss 0.137808, acc 0.9375, learning_rate 0.000798313\n","2023-06-28T22:22:22.734001: step 1747, loss 0.110712, acc 0.96875, learning_rate 0.000797625\n","2023-06-28T22:22:22.887573: step 1748, loss 0.323636, acc 0.875, learning_rate 0.000796938\n","2023-06-28T22:22:23.043262: step 1749, loss 0.0891675, acc 1, learning_rate 0.000796252\n","2023-06-28T22:22:23.221081: step 1750, loss 0.198983, acc 0.96875, learning_rate 0.000795566\n","2023-06-28T22:22:23.399433: step 1751, loss 0.557514, acc 0.875, learning_rate 0.000794882\n","2023-06-28T22:22:23.572639: step 1752, loss 0.174637, acc 0.9375, learning_rate 0.000794197\n","2023-06-28T22:22:23.738665: step 1753, loss 0.264331, acc 0.875, learning_rate 0.000793514\n","2023-06-28T22:22:23.916870: step 1754, loss 0.231869, acc 0.90625, learning_rate 0.000792831\n","2023-06-28T22:22:24.102972: step 1755, loss 0.133733, acc 0.9375, learning_rate 0.000792149\n","2023-06-28T22:22:24.303397: step 1756, loss 0.120288, acc 0.96875, learning_rate 0.000791467\n","2023-06-28T22:22:24.477146: step 1757, loss 0.161491, acc 0.9375, learning_rate 0.000790786\n","2023-06-28T22:22:24.662076: step 1758, loss 0.284054, acc 0.875, learning_rate 0.000790106\n","2023-06-28T22:22:24.843292: step 1759, loss 0.243522, acc 0.90625, learning_rate 0.000789427\n","2023-06-28T22:22:25.030400: step 1760, loss 0.336826, acc 0.84375, learning_rate 0.000788748\n","2023-06-28T22:22:25.201174: step 1761, loss 0.301815, acc 0.9375, learning_rate 0.00078807\n","2023-06-28T22:22:25.365971: step 1762, loss 0.284004, acc 0.90625, learning_rate 0.000787392\n","2023-06-28T22:22:25.540940: step 1763, loss 0.196567, acc 0.9375, learning_rate 0.000786715\n","2023-06-28T22:22:25.723188: step 1764, loss 0.131729, acc 0.9375, learning_rate 0.000786039\n","2023-06-28T22:22:25.888959: step 1765, loss 0.363157, acc 0.875, learning_rate 0.000785364\n","2023-06-28T22:22:26.070576: step 1766, loss 0.147989, acc 0.96875, learning_rate 0.000784689\n","2023-06-28T22:22:26.239336: step 1767, loss 0.270317, acc 0.84375, learning_rate 0.000784015\n","2023-06-28T22:22:26.399167: step 1768, loss 0.373316, acc 0.875, learning_rate 0.000783341\n","2023-06-28T22:22:26.566824: step 1769, loss 0.358989, acc 0.84375, learning_rate 0.000782668\n","2023-06-28T22:22:26.731982: step 1770, loss 0.130699, acc 0.96875, learning_rate 0.000781996\n","2023-06-28T22:22:26.894656: step 1771, loss 0.280032, acc 0.875, learning_rate 0.000781324\n","2023-06-28T22:22:27.055784: step 1772, loss 0.0560112, acc 1, learning_rate 0.000780654\n","2023-06-28T22:22:27.219122: step 1773, loss 0.204273, acc 0.96875, learning_rate 0.000779983\n","2023-06-28T22:22:27.393105: step 1774, loss 0.451611, acc 0.875, learning_rate 0.000779314\n","2023-06-28T22:22:27.551129: step 1775, loss 0.462971, acc 0.875, learning_rate 0.000778645\n","2023-06-28T22:22:27.717929: step 1776, loss 0.310269, acc 0.9375, learning_rate 0.000777977\n","2023-06-28T22:22:27.867488: step 1777, loss 0.186653, acc 0.875, learning_rate 0.000777309\n","2023-06-28T22:22:28.027396: step 1778, loss 0.155438, acc 0.90625, learning_rate 0.000776642\n","2023-06-28T22:22:28.204232: step 1779, loss 0.185293, acc 0.9375, learning_rate 0.000775976\n","2023-06-28T22:22:28.368630: step 1780, loss 0.184853, acc 0.90625, learning_rate 0.00077531\n","2023-06-28T22:22:28.537829: step 1781, loss 0.435575, acc 0.875, learning_rate 0.000774645\n","2023-06-28T22:22:28.698243: step 1782, loss 0.244728, acc 0.90625, learning_rate 0.000773981\n","2023-06-28T22:22:28.862872: step 1783, loss 0.120667, acc 0.9375, learning_rate 0.000773317\n","2023-06-28T22:22:29.038723: step 1784, loss 0.24301, acc 0.90625, learning_rate 0.000772654\n","2023-06-28T22:22:29.211784: step 1785, loss 0.118531, acc 0.9375, learning_rate 0.000771992\n","2023-06-28T22:22:29.379766: step 1786, loss 0.490023, acc 0.8125, learning_rate 0.00077133\n","2023-06-28T22:22:29.553623: step 1787, loss 0.179402, acc 0.9375, learning_rate 0.000770669\n","2023-06-28T22:22:29.722488: step 1788, loss 0.260017, acc 0.90625, learning_rate 0.000770009\n","2023-06-28T22:22:29.884516: step 1789, loss 0.168085, acc 0.90625, learning_rate 0.000769349\n","2023-06-28T22:22:30.065628: step 1790, loss 0.203406, acc 0.9375, learning_rate 0.00076869\n","2023-06-28T22:22:30.239640: step 1791, loss 0.123738, acc 0.9375, learning_rate 0.000768032\n","2023-06-28T22:22:30.412864: step 1792, loss 0.249941, acc 0.90625, learning_rate 0.000767374\n","2023-06-28T22:22:30.571785: step 1793, loss 0.115265, acc 0.96875, learning_rate 0.000766717\n","2023-06-28T22:22:30.743002: step 1794, loss 0.454707, acc 0.875, learning_rate 0.00076606\n","2023-06-28T22:22:30.911033: step 1795, loss 0.504367, acc 0.84375, learning_rate 0.000765404\n","2023-06-28T22:22:31.065454: step 1796, loss 0.239199, acc 0.875, learning_rate 0.000764749\n","2023-06-28T22:22:31.247007: step 1797, loss 0.271396, acc 0.90625, learning_rate 0.000764095\n","2023-06-28T22:22:31.409658: step 1798, loss 0.310869, acc 0.875, learning_rate 0.000763441\n","2023-06-28T22:22:31.565356: step 1799, loss 0.224965, acc 0.90625, learning_rate 0.000762788\n","\n","Evaluation:\n","2023-06-28T22:22:32.758047: step 1800, loss 0.661379, acc 0.818965\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1800\n","\n","2023-06-28T22:22:33.038051: step 1800, loss 0.380052, acc 0.84375, learning_rate 0.000762135\n","2023-06-28T22:22:33.204467: step 1801, loss 0.104279, acc 0.96875, learning_rate 0.000761483\n","2023-06-28T22:22:33.369550: step 1802, loss 0.336459, acc 0.90625, learning_rate 0.000760832\n","2023-06-28T22:22:33.531649: step 1803, loss 0.659623, acc 0.8125, learning_rate 0.000760181\n","2023-06-28T22:22:33.649870: step 1804, loss 0.164962, acc 0.90625, learning_rate 0.000759531\n","2023-06-28T22:22:33.764985: step 1805, loss 0.155115, acc 0.96875, learning_rate 0.000758881\n","2023-06-28T22:22:33.881132: step 1806, loss 0.0910905, acc 1, learning_rate 0.000758233\n","2023-06-28T22:22:33.987569: step 1807, loss 0.222361, acc 0.875, learning_rate 0.000757585\n","2023-06-28T22:22:34.090372: step 1808, loss 0.0921248, acc 0.9375, learning_rate 0.000756937\n","2023-06-28T22:22:34.196605: step 1809, loss 0.540869, acc 0.875, learning_rate 0.00075629\n","2023-06-28T22:22:34.317071: step 1810, loss 0.1266, acc 0.9375, learning_rate 0.000755644\n","2023-06-28T22:22:34.421735: step 1811, loss 0.495911, acc 0.84375, learning_rate 0.000754998\n","2023-06-28T22:22:34.519496: step 1812, loss 0.235694, acc 0.96875, learning_rate 0.000754353\n","2023-06-28T22:22:34.623794: step 1813, loss 0.377143, acc 0.8125, learning_rate 0.000753709\n","2023-06-28T22:22:34.720301: step 1814, loss 0.426047, acc 0.9375, learning_rate 0.000753065\n","2023-06-28T22:22:34.823524: step 1815, loss 0.078753, acc 1, learning_rate 0.000752422\n","2023-06-28T22:22:34.920191: step 1816, loss 0.458787, acc 0.90625, learning_rate 0.00075178\n","2023-06-28T22:22:35.017860: step 1817, loss 0.109625, acc 0.96875, learning_rate 0.000751138\n","2023-06-28T22:22:35.124425: step 1818, loss 0.173239, acc 0.90625, learning_rate 0.000750497\n","2023-06-28T22:22:35.228588: step 1819, loss 0.347336, acc 0.90625, learning_rate 0.000749856\n","2023-06-28T22:22:35.327610: step 1820, loss 0.150013, acc 0.90625, learning_rate 0.000749217\n","2023-06-28T22:22:35.441166: step 1821, loss 0.231677, acc 0.875, learning_rate 0.000748577\n","2023-06-28T22:22:35.539747: step 1822, loss 0.115196, acc 0.9375, learning_rate 0.000747939\n","2023-06-28T22:22:35.644504: step 1823, loss 0.658751, acc 0.90625, learning_rate 0.000747301\n","2023-06-28T22:22:35.743051: step 1824, loss 0.15671, acc 1, learning_rate 0.000746663\n","2023-06-28T22:22:35.845712: step 1825, loss 0.320519, acc 0.8125, learning_rate 0.000746027\n","2023-06-28T22:22:35.947156: step 1826, loss 0.102333, acc 0.96875, learning_rate 0.00074539\n","2023-06-28T22:22:36.045588: step 1827, loss 0.356969, acc 0.96875, learning_rate 0.000744755\n","2023-06-28T22:22:36.154245: step 1828, loss 0.0366847, acc 1, learning_rate 0.00074412\n","2023-06-28T22:22:36.268864: step 1829, loss 0.289761, acc 0.9375, learning_rate 0.000743486\n","2023-06-28T22:22:36.376779: step 1830, loss 0.251036, acc 0.875, learning_rate 0.000742852\n","2023-06-28T22:22:36.494068: step 1831, loss 0.369854, acc 0.875, learning_rate 0.000742219\n","2023-06-28T22:22:36.591534: step 1832, loss 0.385373, acc 0.90625, learning_rate 0.000741587\n","2023-06-28T22:22:36.694262: step 1833, loss 0.30674, acc 0.875, learning_rate 0.000740955\n","2023-06-28T22:22:36.791234: step 1834, loss 0.217128, acc 0.875, learning_rate 0.000740324\n","2023-06-28T22:22:36.890034: step 1835, loss 0.387865, acc 0.90625, learning_rate 0.000739694\n","2023-06-28T22:22:37.024988: step 1836, loss 0.240466, acc 0.90625, learning_rate 0.000739064\n","2023-06-28T22:22:37.127076: step 1837, loss 0.217417, acc 0.90625, learning_rate 0.000738434\n","2023-06-28T22:22:37.226691: step 1838, loss 0.165714, acc 0.96875, learning_rate 0.000737806\n","2023-06-28T22:22:37.331991: step 1839, loss 0.13035, acc 0.96875, learning_rate 0.000737178\n","2023-06-28T22:22:37.431767: step 1840, loss 0.286245, acc 0.9375, learning_rate 0.00073655\n","2023-06-28T22:22:37.536038: step 1841, loss 0.527318, acc 0.84375, learning_rate 0.000735924\n","2023-06-28T22:22:37.638764: step 1842, loss 0.562475, acc 0.875, learning_rate 0.000735297\n","2023-06-28T22:22:37.735564: step 1843, loss 0.389591, acc 0.8125, learning_rate 0.000734672\n","2023-06-28T22:22:37.844308: step 1844, loss 0.140619, acc 0.9375, learning_rate 0.000734047\n","2023-06-28T22:22:37.935519: step 1845, loss 0.162779, acc 0.9375, learning_rate 0.000733423\n","2023-06-28T22:22:38.038605: step 1846, loss 0.253944, acc 0.875, learning_rate 0.000732799\n","2023-06-28T22:22:38.144197: step 1847, loss 0.190637, acc 0.9375, learning_rate 0.000732176\n","2023-06-28T22:22:38.245367: step 1848, loss 0.109949, acc 0.96875, learning_rate 0.000731553\n","2023-06-28T22:22:38.346854: step 1849, loss 0.0969013, acc 0.96875, learning_rate 0.000730931\n","2023-06-28T22:22:38.441401: step 1850, loss 0.218675, acc 0.9375, learning_rate 0.00073031\n","2023-06-28T22:22:38.550945: step 1851, loss 0.239867, acc 0.9375, learning_rate 0.00072969\n","2023-06-28T22:22:38.643694: step 1852, loss 0.020371, acc 1, learning_rate 0.000729069\n","2023-06-28T22:22:38.738714: step 1853, loss 0.21473, acc 0.9375, learning_rate 0.00072845\n","2023-06-28T22:22:38.840779: step 1854, loss 0.212037, acc 0.9375, learning_rate 0.000727831\n","2023-06-28T22:22:38.950428: step 1855, loss 0.228286, acc 0.875, learning_rate 0.000727213\n","2023-06-28T22:22:39.047893: step 1856, loss 0.219694, acc 0.9375, learning_rate 0.000726595\n","2023-06-28T22:22:39.140325: step 1857, loss 0.555029, acc 0.78125, learning_rate 0.000725978\n","2023-06-28T22:22:39.265293: step 1858, loss 0.267014, acc 0.875, learning_rate 0.000725362\n","2023-06-28T22:22:39.365567: step 1859, loss 0.372855, acc 0.875, learning_rate 0.000724746\n","2023-06-28T22:22:39.471107: step 1860, loss 0.140457, acc 1, learning_rate 0.000724131\n","2023-06-28T22:22:39.574882: step 1861, loss 0.14146, acc 0.9375, learning_rate 0.000723517\n","2023-06-28T22:22:39.675796: step 1862, loss 0.603486, acc 0.8125, learning_rate 0.000722903\n","2023-06-28T22:22:39.772464: step 1863, loss 0.489307, acc 0.84375, learning_rate 0.000722289\n","2023-06-28T22:22:39.873345: step 1864, loss 0.109325, acc 0.9375, learning_rate 0.000721677\n","2023-06-28T22:22:39.977123: step 1865, loss 0.0763825, acc 1, learning_rate 0.000721064\n","2023-06-28T22:22:40.081659: step 1866, loss 0.483653, acc 0.84375, learning_rate 0.000720453\n","2023-06-28T22:22:40.193262: step 1867, loss 0.102049, acc 0.96875, learning_rate 0.000719842\n","2023-06-28T22:22:40.293941: step 1868, loss 0.432313, acc 0.90625, learning_rate 0.000719232\n","2023-06-28T22:22:40.393363: step 1869, loss 0.389187, acc 0.8125, learning_rate 0.000718622\n","2023-06-28T22:22:40.488206: step 1870, loss 0.128602, acc 0.9375, learning_rate 0.000718013\n","2023-06-28T22:22:40.594923: step 1871, loss 0.181779, acc 0.9375, learning_rate 0.000717404\n","2023-06-28T22:22:40.699699: step 1872, loss 0.0989579, acc 0.9375, learning_rate 0.000716796\n","2023-06-28T22:22:40.796565: step 1873, loss 0.367412, acc 0.84375, learning_rate 0.000716189\n","2023-06-28T22:22:40.896775: step 1874, loss 0.127235, acc 0.9375, learning_rate 0.000715582\n","2023-06-28T22:22:40.995328: step 1875, loss 0.162319, acc 0.96875, learning_rate 0.000714976\n","2023-06-28T22:22:41.099342: step 1876, loss 0.257391, acc 0.90625, learning_rate 0.000714371\n","2023-06-28T22:22:41.204558: step 1877, loss 0.284487, acc 0.875, learning_rate 0.000713766\n","2023-06-28T22:22:41.302641: step 1878, loss 0.0565444, acc 1, learning_rate 0.000713161\n","2023-06-28T22:22:41.408353: step 1879, loss 0.205591, acc 0.84375, learning_rate 0.000712557\n","2023-06-28T22:22:41.513723: step 1880, loss 0.0333885, acc 1, learning_rate 0.000711954\n","2023-06-28T22:22:41.630895: step 1881, loss 0.184917, acc 0.875, learning_rate 0.000711352\n","2023-06-28T22:22:41.725668: step 1882, loss 0.330898, acc 0.875, learning_rate 0.00071075\n","2023-06-28T22:22:41.831666: step 1883, loss 0.404022, acc 0.875, learning_rate 0.000710148\n","2023-06-28T22:22:41.936149: step 1884, loss 0.300267, acc 0.84375, learning_rate 0.000709548\n","2023-06-28T22:22:42.041364: step 1885, loss 0.189123, acc 0.90625, learning_rate 0.000708947\n","2023-06-28T22:22:42.144592: step 1886, loss 0.279406, acc 0.875, learning_rate 0.000708348\n","2023-06-28T22:22:42.247514: step 1887, loss 0.0624394, acc 0.96875, learning_rate 0.000707749\n","2023-06-28T22:22:42.347442: step 1888, loss 0.327051, acc 0.875, learning_rate 0.00070715\n","2023-06-28T22:22:42.443197: step 1889, loss 0.426215, acc 0.84375, learning_rate 0.000706553\n","2023-06-28T22:22:42.547269: step 1890, loss 0.0806257, acc 0.96875, learning_rate 0.000705955\n","2023-06-28T22:22:42.651996: step 1891, loss 0.248092, acc 0.875, learning_rate 0.000705359\n","2023-06-28T22:22:42.749401: step 1892, loss 0.175765, acc 0.9375, learning_rate 0.000704763\n","2023-06-28T22:22:42.856933: step 1893, loss 0.370378, acc 0.90625, learning_rate 0.000704167\n","2023-06-28T22:22:42.953666: step 1894, loss 0.17479, acc 0.90625, learning_rate 0.000703572\n","2023-06-28T22:22:43.049374: step 1895, loss 0.280244, acc 0.875, learning_rate 0.000702978\n","2023-06-28T22:22:43.166976: step 1896, loss 0.287713, acc 0.875, learning_rate 0.000702384\n","2023-06-28T22:22:43.258439: step 1897, loss 0.684345, acc 0.84375, learning_rate 0.000701791\n","2023-06-28T22:22:43.362056: step 1898, loss 0.137736, acc 0.96875, learning_rate 0.000701198\n","2023-06-28T22:22:43.454820: step 1899, loss 0.104648, acc 0.96875, learning_rate 0.000700606\n","\n","Evaluation:\n","2023-06-28T22:22:44.470753: step 1900, loss 0.652111, acc 0.836823\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-1900\n","\n","2023-06-28T22:22:44.783509: step 1900, loss 0.394841, acc 0.84375, learning_rate 0.000700015\n","2023-06-28T22:22:44.945930: step 1901, loss 0.473571, acc 0.84375, learning_rate 0.000699424\n","2023-06-28T22:22:45.134993: step 1902, loss 0.184325, acc 0.9375, learning_rate 0.000698834\n","2023-06-28T22:22:45.310995: step 1903, loss 0.122036, acc 0.96875, learning_rate 0.000698244\n","2023-06-28T22:22:45.491710: step 1904, loss 0.183979, acc 0.90625, learning_rate 0.000697655\n","2023-06-28T22:22:45.660253: step 1905, loss 0.0968749, acc 0.9375, learning_rate 0.000697067\n","2023-06-28T22:22:45.829978: step 1906, loss 0.467155, acc 0.8125, learning_rate 0.000696479\n","2023-06-28T22:22:46.003716: step 1907, loss 0.126309, acc 0.9375, learning_rate 0.000695892\n","2023-06-28T22:22:46.182546: step 1908, loss 0.404394, acc 0.90625, learning_rate 0.000695305\n","2023-06-28T22:22:46.354664: step 1909, loss 0.507883, acc 0.90625, learning_rate 0.000694719\n","2023-06-28T22:22:46.529240: step 1910, loss 0.170897, acc 0.9375, learning_rate 0.000694133\n","2023-06-28T22:22:46.687721: step 1911, loss 0.210231, acc 0.9375, learning_rate 0.000693548\n","2023-06-28T22:22:46.866799: step 1912, loss 0.30222, acc 0.90625, learning_rate 0.000692964\n","2023-06-28T22:22:47.029456: step 1913, loss 0.248252, acc 0.90625, learning_rate 0.00069238\n","2023-06-28T22:22:47.212804: step 1914, loss 0.145101, acc 0.90625, learning_rate 0.000691796\n","2023-06-28T22:22:47.390147: step 1915, loss 0.293352, acc 0.875, learning_rate 0.000691214\n","2023-06-28T22:22:47.564548: step 1916, loss 0.195912, acc 0.96875, learning_rate 0.000690632\n","2023-06-28T22:22:47.742135: step 1917, loss 0.388536, acc 0.84375, learning_rate 0.00069005\n","2023-06-28T22:22:47.912477: step 1918, loss 0.538262, acc 0.875, learning_rate 0.000689469\n","2023-06-28T22:22:48.076248: step 1919, loss 0.189098, acc 0.90625, learning_rate 0.000688889\n","2023-06-28T22:22:48.249495: step 1920, loss 0.259011, acc 0.875, learning_rate 0.000688309\n","2023-06-28T22:22:48.430478: step 1921, loss 0.460278, acc 0.90625, learning_rate 0.000687729\n","2023-06-28T22:22:48.598146: step 1922, loss 0.206884, acc 0.90625, learning_rate 0.000687151\n","2023-06-28T22:22:48.752703: step 1923, loss 0.137029, acc 0.9375, learning_rate 0.000686573\n","2023-06-28T22:22:48.924448: step 1924, loss 0.240524, acc 0.875, learning_rate 0.000685995\n","2023-06-28T22:22:49.092341: step 1925, loss 0.194685, acc 0.90625, learning_rate 0.000685418\n","2023-06-28T22:22:49.259425: step 1926, loss 0.0344289, acc 1, learning_rate 0.000684842\n","2023-06-28T22:22:49.422523: step 1927, loss 0.0404079, acc 1, learning_rate 0.000684266\n","2023-06-28T22:22:49.584846: step 1928, loss 0.0867667, acc 1, learning_rate 0.00068369\n","2023-06-28T22:22:49.768604: step 1929, loss 0.42268, acc 0.84375, learning_rate 0.000683116\n","2023-06-28T22:22:49.942255: step 1930, loss 0.155952, acc 0.9375, learning_rate 0.000682541\n","2023-06-28T22:22:50.088249: step 1931, loss 0.221146, acc 0.9375, learning_rate 0.000681968\n","2023-06-28T22:22:50.246799: step 1932, loss 0.156263, acc 0.9375, learning_rate 0.000681395\n","2023-06-28T22:22:50.403748: step 1933, loss 0.0548708, acc 1, learning_rate 0.000680822\n","2023-06-28T22:22:50.557701: step 1934, loss 0.173202, acc 0.90625, learning_rate 0.00068025\n","2023-06-28T22:22:50.716825: step 1935, loss 0.18263, acc 0.90625, learning_rate 0.000679679\n","2023-06-28T22:22:50.893332: step 1936, loss 0.234288, acc 0.90625, learning_rate 0.000679108\n","2023-06-28T22:22:51.067499: step 1937, loss 0.104011, acc 0.96875, learning_rate 0.000678538\n","2023-06-28T22:22:51.233289: step 1938, loss 0.143796, acc 0.9375, learning_rate 0.000677968\n","2023-06-28T22:22:51.390843: step 1939, loss 0.550928, acc 0.90625, learning_rate 0.000677399\n","2023-06-28T22:22:51.553905: step 1940, loss 0.232451, acc 0.9375, learning_rate 0.000676831\n","2023-06-28T22:22:51.722946: step 1941, loss 0.142052, acc 0.96875, learning_rate 0.000676263\n","2023-06-28T22:22:51.913046: step 1942, loss 0.228172, acc 0.90625, learning_rate 0.000675695\n","2023-06-28T22:22:52.087139: step 1943, loss 0.203101, acc 0.96875, learning_rate 0.000675128\n","2023-06-28T22:22:52.252796: step 1944, loss 0.155475, acc 0.90625, learning_rate 0.000674562\n","2023-06-28T22:22:52.417634: step 1945, loss 0.165951, acc 0.96875, learning_rate 0.000673996\n","2023-06-28T22:22:52.593889: step 1946, loss 0.0909772, acc 0.96875, learning_rate 0.000673431\n","2023-06-28T22:22:52.761590: step 1947, loss 0.257604, acc 0.90625, learning_rate 0.000672867\n","2023-06-28T22:22:52.938111: step 1948, loss 0.240581, acc 0.9375, learning_rate 0.000672303\n","2023-06-28T22:22:53.117237: step 1949, loss 0.222284, acc 0.90625, learning_rate 0.000671739\n","2023-06-28T22:22:53.284962: step 1950, loss 0.412167, acc 0.875, learning_rate 0.000671176\n","2023-06-28T22:22:53.455482: step 1951, loss 0.104419, acc 0.9375, learning_rate 0.000670614\n","2023-06-28T22:22:53.622520: step 1952, loss 0.254769, acc 0.9375, learning_rate 0.000670052\n","2023-06-28T22:22:53.795592: step 1953, loss 0.231839, acc 0.90625, learning_rate 0.00066949\n","2023-06-28T22:22:53.976145: step 1954, loss 0.253775, acc 0.875, learning_rate 0.00066893\n","2023-06-28T22:22:54.118852: step 1955, loss 0.173671, acc 0.9375, learning_rate 0.000668369\n","2023-06-28T22:22:54.222149: step 1956, loss 0.212893, acc 0.90625, learning_rate 0.00066781\n","2023-06-28T22:22:54.317410: step 1957, loss 0.238959, acc 0.90625, learning_rate 0.000667251\n","2023-06-28T22:22:54.417264: step 1958, loss 0.265485, acc 0.90625, learning_rate 0.000666692\n","2023-06-28T22:22:54.518324: step 1959, loss 0.143908, acc 0.90625, learning_rate 0.000666134\n","2023-06-28T22:22:54.613803: step 1960, loss 0.108588, acc 0.96875, learning_rate 0.000665577\n","2023-06-28T22:22:54.713574: step 1961, loss 0.0766588, acc 0.96875, learning_rate 0.00066502\n","2023-06-28T22:22:54.813487: step 1962, loss 0.333121, acc 0.8125, learning_rate 0.000664463\n","2023-06-28T22:22:54.923065: step 1963, loss 0.278038, acc 0.90625, learning_rate 0.000663908\n","2023-06-28T22:22:55.021634: step 1964, loss 0.18391, acc 0.90625, learning_rate 0.000663352\n","2023-06-28T22:22:55.123931: step 1965, loss 0.369775, acc 0.84375, learning_rate 0.000662798\n","2023-06-28T22:22:55.222981: step 1966, loss 0.195514, acc 0.9375, learning_rate 0.000662244\n","2023-06-28T22:22:55.321120: step 1967, loss 0.345824, acc 0.84375, learning_rate 0.00066169\n","2023-06-28T22:22:55.416535: step 1968, loss 0.221373, acc 0.96875, learning_rate 0.000661137\n","2023-06-28T22:22:55.517361: step 1969, loss 0.249125, acc 0.96875, learning_rate 0.000660584\n","2023-06-28T22:22:55.610969: step 1970, loss 0.084745, acc 0.96875, learning_rate 0.000660032\n","2023-06-28T22:22:55.721669: step 1971, loss 0.186365, acc 0.90625, learning_rate 0.000659481\n","2023-06-28T22:22:55.820675: step 1972, loss 0.475498, acc 0.78125, learning_rate 0.00065893\n","2023-06-28T22:22:55.918889: step 1973, loss 0.0937379, acc 0.9375, learning_rate 0.00065838\n","2023-06-28T22:22:56.018573: step 1974, loss 0.23814, acc 0.96875, learning_rate 0.00065783\n","2023-06-28T22:22:56.137743: step 1975, loss 0.248316, acc 0.84375, learning_rate 0.000657281\n","2023-06-28T22:22:56.236048: step 1976, loss 0.0905564, acc 1, learning_rate 0.000656732\n","2023-06-28T22:22:56.344582: step 1977, loss 0.20328, acc 0.90625, learning_rate 0.000656184\n","2023-06-28T22:22:56.458618: step 1978, loss 0.264764, acc 0.875, learning_rate 0.000655636\n","2023-06-28T22:22:56.563469: step 1979, loss 0.705693, acc 0.78125, learning_rate 0.000655089\n","2023-06-28T22:22:56.662962: step 1980, loss 0.191873, acc 0.96875, learning_rate 0.000654542\n","2023-06-28T22:22:56.756394: step 1981, loss 0.159218, acc 0.96875, learning_rate 0.000653996\n","2023-06-28T22:22:56.855740: step 1982, loss 0.0903701, acc 1, learning_rate 0.000653451\n","2023-06-28T22:22:56.948584: step 1983, loss 0.232925, acc 0.9375, learning_rate 0.000652906\n","2023-06-28T22:22:57.050934: step 1984, loss 0.129494, acc 0.96875, learning_rate 0.000652361\n","2023-06-28T22:22:57.156510: step 1985, loss 0.680932, acc 0.84375, learning_rate 0.000651817\n","2023-06-28T22:22:57.257516: step 1986, loss 0.64344, acc 0.84375, learning_rate 0.000651274\n","2023-06-28T22:22:57.372794: step 1987, loss 0.502411, acc 0.78125, learning_rate 0.000650731\n","2023-06-28T22:22:57.470508: step 1988, loss 0.260016, acc 0.875, learning_rate 0.000650189\n","2023-06-28T22:22:57.571871: step 1989, loss 0.279161, acc 0.8125, learning_rate 0.000649647\n","2023-06-28T22:22:57.670506: step 1990, loss 0.261829, acc 0.90625, learning_rate 0.000649106\n","2023-06-28T22:22:57.779746: step 1991, loss 0.0926213, acc 0.96875, learning_rate 0.000648565\n","2023-06-28T22:22:57.881902: step 1992, loss 0.346676, acc 0.84375, learning_rate 0.000648025\n","2023-06-28T22:22:57.984010: step 1993, loss 0.265509, acc 0.96875, learning_rate 0.000647486\n","2023-06-28T22:22:58.085323: step 1994, loss 0.194142, acc 0.875, learning_rate 0.000646947\n","2023-06-28T22:22:58.202178: step 1995, loss 0.239124, acc 0.9375, learning_rate 0.000646408\n","2023-06-28T22:22:58.305877: step 1996, loss 0.0920111, acc 1, learning_rate 0.00064587\n","2023-06-28T22:22:58.399984: step 1997, loss 0.333705, acc 0.875, learning_rate 0.000645332\n","2023-06-28T22:22:58.508078: step 1998, loss 0.69701, acc 0.84375, learning_rate 0.000644795\n","2023-06-28T22:22:58.609311: step 1999, loss 0.293114, acc 0.90625, learning_rate 0.000644259\n","\n","Evaluation:\n","2023-06-28T22:22:59.340215: step 2000, loss 0.677976, acc 0.82851\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2000\n","\n","2023-06-28T22:22:59.549768: step 2000, loss 0.413769, acc 0.84375, learning_rate 0.000643723\n","2023-06-28T22:22:59.658688: step 2001, loss 0.266022, acc 0.90625, learning_rate 0.000643188\n","2023-06-28T22:22:59.757751: step 2002, loss 0.177261, acc 0.9375, learning_rate 0.000642653\n","2023-06-28T22:22:59.871322: step 2003, loss 0.706064, acc 0.84375, learning_rate 0.000642119\n","2023-06-28T22:22:59.967199: step 2004, loss 0.203307, acc 0.90625, learning_rate 0.000641585\n","2023-06-28T22:23:00.072020: step 2005, loss 0.393942, acc 0.84375, learning_rate 0.000641051\n","2023-06-28T22:23:00.178695: step 2006, loss 0.464803, acc 0.875, learning_rate 0.000640519\n","2023-06-28T22:23:00.291642: step 2007, loss 0.327786, acc 0.875, learning_rate 0.000639986\n","2023-06-28T22:23:00.390080: step 2008, loss 0.245944, acc 0.8125, learning_rate 0.000639455\n","2023-06-28T22:23:00.487677: step 2009, loss 0.148041, acc 0.9375, learning_rate 0.000638924\n","2023-06-28T22:23:00.586103: step 2010, loss 0.264929, acc 0.84375, learning_rate 0.000638393\n","2023-06-28T22:23:00.687073: step 2011, loss 0.597699, acc 0.875, learning_rate 0.000637863\n","2023-06-28T22:23:00.786027: step 2012, loss 0.172544, acc 0.9375, learning_rate 0.000637333\n","2023-06-28T22:23:00.888548: step 2013, loss 0.455951, acc 0.8125, learning_rate 0.000636804\n","2023-06-28T22:23:00.996107: step 2014, loss 0.468014, acc 0.90625, learning_rate 0.000636276\n","2023-06-28T22:23:01.087830: step 2015, loss 0.518506, acc 0.90625, learning_rate 0.000635747\n","2023-06-28T22:23:01.192105: step 2016, loss 0.201706, acc 0.90625, learning_rate 0.00063522\n","2023-06-28T22:23:01.292614: step 2017, loss 0.359808, acc 0.8125, learning_rate 0.000634693\n","2023-06-28T22:23:01.385536: step 2018, loss 0.22762, acc 0.875, learning_rate 0.000634166\n","2023-06-28T22:23:01.486656: step 2019, loss 0.678504, acc 0.8125, learning_rate 0.00063364\n","2023-06-28T22:23:01.580336: step 2020, loss 0.24108, acc 0.90625, learning_rate 0.000633115\n","2023-06-28T22:23:01.672546: step 2021, loss 0.375099, acc 0.875, learning_rate 0.00063259\n","2023-06-28T22:23:01.771423: step 2022, loss 0.224308, acc 0.9375, learning_rate 0.000632066\n","2023-06-28T22:23:01.873725: step 2023, loss 0.304327, acc 0.9375, learning_rate 0.000631542\n","2023-06-28T22:23:01.968338: step 2024, loss 0.264047, acc 0.90625, learning_rate 0.000631018\n","2023-06-28T22:23:02.074157: step 2025, loss 0.459663, acc 0.84375, learning_rate 0.000630496\n","2023-06-28T22:23:02.175822: step 2026, loss 0.437027, acc 0.84375, learning_rate 0.000629973\n","2023-06-28T22:23:02.289068: step 2027, loss 0.20438, acc 0.9375, learning_rate 0.000629451\n","2023-06-28T22:23:02.392485: step 2028, loss 0.288966, acc 0.90625, learning_rate 0.00062893\n","2023-06-28T22:23:02.489739: step 2029, loss 0.216099, acc 0.875, learning_rate 0.000628409\n","2023-06-28T22:23:02.598033: step 2030, loss 0.252298, acc 0.90625, learning_rate 0.000627889\n","2023-06-28T22:23:02.695893: step 2031, loss 0.33736, acc 0.84375, learning_rate 0.000627369\n","2023-06-28T22:23:02.800400: step 2032, loss 0.229097, acc 0.875, learning_rate 0.00062685\n","2023-06-28T22:23:02.897662: step 2033, loss 0.23172, acc 0.9375, learning_rate 0.000626331\n","2023-06-28T22:23:02.964435: step 2034, loss 0.000348668, acc 1, learning_rate 0.000625813\n","2023-06-28T22:23:03.077559: step 2035, loss 0.136014, acc 0.9375, learning_rate 0.000625295\n","2023-06-28T22:23:03.217153: step 2036, loss 0.145324, acc 0.96875, learning_rate 0.000624778\n","2023-06-28T22:23:03.324164: step 2037, loss 0.252047, acc 0.84375, learning_rate 0.000624261\n","2023-06-28T22:23:03.427447: step 2038, loss 0.130104, acc 0.9375, learning_rate 0.000623745\n","2023-06-28T22:23:03.538241: step 2039, loss 0.127476, acc 0.96875, learning_rate 0.000623229\n","2023-06-28T22:23:03.642621: step 2040, loss 0.0567512, acc 1, learning_rate 0.000622714\n","2023-06-28T22:23:03.743865: step 2041, loss 0.195238, acc 0.96875, learning_rate 0.000622199\n","2023-06-28T22:23:03.847430: step 2042, loss 0.15736, acc 0.84375, learning_rate 0.000621685\n","2023-06-28T22:23:03.945799: step 2043, loss 0.141763, acc 0.96875, learning_rate 0.000621171\n","2023-06-28T22:23:04.046139: step 2044, loss 0.111796, acc 0.90625, learning_rate 0.000620658\n","2023-06-28T22:23:04.179668: step 2045, loss 0.200857, acc 0.9375, learning_rate 0.000620145\n","2023-06-28T22:23:04.351119: step 2046, loss 0.175784, acc 0.90625, learning_rate 0.000619633\n","2023-06-28T22:23:04.519134: step 2047, loss 0.185172, acc 0.9375, learning_rate 0.000619122\n","2023-06-28T22:23:04.672980: step 2048, loss 0.185805, acc 0.9375, learning_rate 0.000618611\n","2023-06-28T22:23:04.856196: step 2049, loss 0.570347, acc 0.9375, learning_rate 0.0006181\n","2023-06-28T22:23:05.042437: step 2050, loss 0.416538, acc 0.875, learning_rate 0.00061759\n","2023-06-28T22:23:05.234073: step 2051, loss 0.113896, acc 0.9375, learning_rate 0.00061708\n","2023-06-28T22:23:05.421219: step 2052, loss 0.0769305, acc 0.96875, learning_rate 0.000616571\n","2023-06-28T22:23:05.596097: step 2053, loss 0.146353, acc 0.9375, learning_rate 0.000616062\n","2023-06-28T22:23:05.762630: step 2054, loss 0.0624177, acc 0.96875, learning_rate 0.000615554\n","2023-06-28T22:23:05.933178: step 2055, loss 0.517405, acc 0.9375, learning_rate 0.000615046\n","2023-06-28T22:23:06.107848: step 2056, loss 0.139256, acc 0.9375, learning_rate 0.000614539\n","2023-06-28T22:23:06.289795: step 2057, loss 0.314724, acc 0.90625, learning_rate 0.000614033\n","2023-06-28T22:23:06.477787: step 2058, loss 0.0734912, acc 1, learning_rate 0.000613527\n","2023-06-28T22:23:06.653244: step 2059, loss 0.209317, acc 0.96875, learning_rate 0.000613021\n","2023-06-28T22:23:06.834561: step 2060, loss 0.232516, acc 0.9375, learning_rate 0.000612516\n","2023-06-28T22:23:07.002316: step 2061, loss 0.366439, acc 0.875, learning_rate 0.000612011\n","2023-06-28T22:23:07.174970: step 2062, loss 0.202208, acc 0.90625, learning_rate 0.000611507\n","2023-06-28T22:23:07.357296: step 2063, loss 0.125876, acc 0.9375, learning_rate 0.000611003\n","2023-06-28T22:23:07.516005: step 2064, loss 0.069919, acc 0.96875, learning_rate 0.0006105\n","2023-06-28T22:23:07.678469: step 2065, loss 0.310536, acc 0.84375, learning_rate 0.000609997\n","2023-06-28T22:23:07.843266: step 2066, loss 0.226034, acc 0.875, learning_rate 0.000609495\n","2023-06-28T22:23:08.009858: step 2067, loss 0.370282, acc 0.8125, learning_rate 0.000608994\n","2023-06-28T22:23:08.173935: step 2068, loss 0.121794, acc 0.96875, learning_rate 0.000608492\n","2023-06-28T22:23:08.341018: step 2069, loss 0.123757, acc 0.96875, learning_rate 0.000607992\n","2023-06-28T22:23:08.504056: step 2070, loss 0.125034, acc 0.96875, learning_rate 0.000607491\n","2023-06-28T22:23:08.680743: step 2071, loss 0.187123, acc 0.9375, learning_rate 0.000606992\n","2023-06-28T22:23:08.847231: step 2072, loss 0.127007, acc 0.9375, learning_rate 0.000606493\n","2023-06-28T22:23:09.015828: step 2073, loss 0.183337, acc 0.90625, learning_rate 0.000605994\n","2023-06-28T22:23:09.176654: step 2074, loss 0.22344, acc 0.9375, learning_rate 0.000605496\n","2023-06-28T22:23:09.361815: step 2075, loss 0.119667, acc 0.96875, learning_rate 0.000604998\n","2023-06-28T22:23:09.537028: step 2076, loss 0.142991, acc 0.96875, learning_rate 0.000604501\n","2023-06-28T22:23:09.706223: step 2077, loss 0.0366784, acc 0.96875, learning_rate 0.000604004\n","2023-06-28T22:23:09.871914: step 2078, loss 0.0643534, acc 1, learning_rate 0.000603508\n","2023-06-28T22:23:10.044732: step 2079, loss 0.17364, acc 0.9375, learning_rate 0.000603012\n","2023-06-28T22:23:10.207753: step 2080, loss 0.185063, acc 0.9375, learning_rate 0.000602516\n","2023-06-28T22:23:10.362948: step 2081, loss 0.164093, acc 0.9375, learning_rate 0.000602022\n","2023-06-28T22:23:10.533860: step 2082, loss 0.226922, acc 0.90625, learning_rate 0.000601527\n","2023-06-28T22:23:10.698261: step 2083, loss 0.20452, acc 0.96875, learning_rate 0.000601034\n","2023-06-28T22:23:10.846818: step 2084, loss 0.132305, acc 0.96875, learning_rate 0.00060054\n","2023-06-28T22:23:11.023968: step 2085, loss 0.169175, acc 0.90625, learning_rate 0.000600047\n","2023-06-28T22:23:11.196741: step 2086, loss 0.0907914, acc 0.96875, learning_rate 0.000599555\n","2023-06-28T22:23:11.363869: step 2087, loss 0.606818, acc 0.84375, learning_rate 0.000599063\n","2023-06-28T22:23:11.519671: step 2088, loss 0.447441, acc 0.875, learning_rate 0.000598572\n","2023-06-28T22:23:11.679636: step 2089, loss 0.217985, acc 0.9375, learning_rate 0.000598081\n","2023-06-28T22:23:11.837070: step 2090, loss 0.164133, acc 0.90625, learning_rate 0.00059759\n","2023-06-28T22:23:11.994270: step 2091, loss 0.294114, acc 0.96875, learning_rate 0.0005971\n","2023-06-28T22:23:12.161590: step 2092, loss 0.202957, acc 0.96875, learning_rate 0.000596611\n","2023-06-28T22:23:12.336235: step 2093, loss 0.227866, acc 0.9375, learning_rate 0.000596122\n","2023-06-28T22:23:12.495569: step 2094, loss 0.184362, acc 0.90625, learning_rate 0.000595633\n","2023-06-28T22:23:12.676266: step 2095, loss 0.104804, acc 0.9375, learning_rate 0.000595145\n","2023-06-28T22:23:12.839809: step 2096, loss 0.758095, acc 0.71875, learning_rate 0.000594658\n","2023-06-28T22:23:12.985200: step 2097, loss 0.122655, acc 0.90625, learning_rate 0.000594171\n","2023-06-28T22:23:13.155183: step 2098, loss 0.126808, acc 0.9375, learning_rate 0.000593684\n","2023-06-28T22:23:13.317228: step 2099, loss 0.145625, acc 0.9375, learning_rate 0.000593198\n","\n","Evaluation:\n","2023-06-28T22:23:14.629404: step 2100, loss 0.664457, acc 0.83436\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2100\n","\n","2023-06-28T22:23:14.925974: step 2100, loss 0.25374, acc 0.875, learning_rate 0.000592712\n","2023-06-28T22:23:15.068246: step 2101, loss 0.288247, acc 0.84375, learning_rate 0.000592227\n","2023-06-28T22:23:15.170346: step 2102, loss 0.503279, acc 0.84375, learning_rate 0.000591743\n","2023-06-28T22:23:15.278099: step 2103, loss 0.28974, acc 0.90625, learning_rate 0.000591258\n","2023-06-28T22:23:15.374946: step 2104, loss 0.125165, acc 0.90625, learning_rate 0.000590775\n","2023-06-28T22:23:15.472290: step 2105, loss 0.275599, acc 0.9375, learning_rate 0.000590291\n","2023-06-28T22:23:15.580055: step 2106, loss 0.0819414, acc 0.96875, learning_rate 0.000589809\n","2023-06-28T22:23:15.681066: step 2107, loss 0.123224, acc 0.9375, learning_rate 0.000589326\n","2023-06-28T22:23:15.790556: step 2108, loss 0.126979, acc 0.96875, learning_rate 0.000588844\n","2023-06-28T22:23:15.920495: step 2109, loss 0.201755, acc 0.90625, learning_rate 0.000588363\n","2023-06-28T22:23:16.025477: step 2110, loss 0.156368, acc 0.9375, learning_rate 0.000587882\n","2023-06-28T22:23:16.134512: step 2111, loss 0.185598, acc 0.9375, learning_rate 0.000587402\n","2023-06-28T22:23:16.231056: step 2112, loss 0.119917, acc 0.96875, learning_rate 0.000586922\n","2023-06-28T22:23:16.337662: step 2113, loss 0.182598, acc 0.9375, learning_rate 0.000586442\n","2023-06-28T22:23:16.431221: step 2114, loss 0.0609318, acc 0.96875, learning_rate 0.000585963\n","2023-06-28T22:23:16.532109: step 2115, loss 0.926554, acc 0.90625, learning_rate 0.000585485\n","2023-06-28T22:23:16.624932: step 2116, loss 0.0557071, acc 1, learning_rate 0.000585007\n","2023-06-28T22:23:16.724443: step 2117, loss 0.0216402, acc 1, learning_rate 0.000584529\n","2023-06-28T22:23:16.838508: step 2118, loss 0.145395, acc 0.9375, learning_rate 0.000584052\n","2023-06-28T22:23:16.947192: step 2119, loss 0.338544, acc 0.9375, learning_rate 0.000583576\n","2023-06-28T22:23:17.050535: step 2120, loss 0.270551, acc 0.875, learning_rate 0.000583099\n","2023-06-28T22:23:17.150319: step 2121, loss 0.316346, acc 0.9375, learning_rate 0.000582624\n","2023-06-28T22:23:17.250696: step 2122, loss 0.274377, acc 0.96875, learning_rate 0.000582149\n","2023-06-28T22:23:17.357729: step 2123, loss 0.121899, acc 0.9375, learning_rate 0.000581674\n","2023-06-28T22:23:17.451876: step 2124, loss 0.0452817, acc 1, learning_rate 0.0005812\n","2023-06-28T22:23:17.548824: step 2125, loss 0.14915, acc 0.9375, learning_rate 0.000580726\n","2023-06-28T22:23:17.649780: step 2126, loss 0.177436, acc 0.9375, learning_rate 0.000580252\n","2023-06-28T22:23:17.746658: step 2127, loss 0.300523, acc 0.875, learning_rate 0.000579779\n","2023-06-28T22:23:17.852608: step 2128, loss 0.220919, acc 0.875, learning_rate 0.000579307\n","2023-06-28T22:23:17.952863: step 2129, loss 0.209148, acc 0.96875, learning_rate 0.000578835\n","2023-06-28T22:23:18.048924: step 2130, loss 0.23271, acc 0.96875, learning_rate 0.000578364\n","2023-06-28T22:23:18.167881: step 2131, loss 0.283299, acc 0.90625, learning_rate 0.000577893\n","2023-06-28T22:23:18.260503: step 2132, loss 0.0480131, acc 0.96875, learning_rate 0.000577422\n","2023-06-28T22:23:18.358670: step 2133, loss 0.320825, acc 0.84375, learning_rate 0.000576952\n","2023-06-28T22:23:18.453890: step 2134, loss 0.107713, acc 0.96875, learning_rate 0.000576482\n","2023-06-28T22:23:18.549127: step 2135, loss 0.250485, acc 0.90625, learning_rate 0.000576013\n","2023-06-28T22:23:18.650252: step 2136, loss 0.0455634, acc 1, learning_rate 0.000575544\n","2023-06-28T22:23:18.746958: step 2137, loss 0.106487, acc 0.9375, learning_rate 0.000575076\n","2023-06-28T22:23:18.856869: step 2138, loss 0.250129, acc 0.9375, learning_rate 0.000574608\n","2023-06-28T22:23:18.966886: step 2139, loss 0.279983, acc 0.90625, learning_rate 0.000574141\n","2023-06-28T22:23:19.082650: step 2140, loss 0.122673, acc 0.96875, learning_rate 0.000573674\n","2023-06-28T22:23:19.189101: step 2141, loss 0.127506, acc 0.9375, learning_rate 0.000573208\n","2023-06-28T22:23:19.296366: step 2142, loss 0.531583, acc 0.90625, learning_rate 0.000572742\n","2023-06-28T22:23:19.406874: step 2143, loss 0.0695919, acc 1, learning_rate 0.000572276\n","2023-06-28T22:23:19.516746: step 2144, loss 0.408926, acc 0.84375, learning_rate 0.000571811\n","2023-06-28T22:23:19.615005: step 2145, loss 0.0542051, acc 0.96875, learning_rate 0.000571347\n","2023-06-28T22:23:19.719949: step 2146, loss 0.189364, acc 0.90625, learning_rate 0.000570883\n","2023-06-28T22:23:19.819664: step 2147, loss 0.112104, acc 0.96875, learning_rate 0.000570419\n","2023-06-28T22:23:19.933065: step 2148, loss 0.173483, acc 0.90625, learning_rate 0.000569956\n","2023-06-28T22:23:20.028779: step 2149, loss 0.238849, acc 0.90625, learning_rate 0.000569493\n","2023-06-28T22:23:20.134898: step 2150, loss 0.167034, acc 0.9375, learning_rate 0.000569031\n","2023-06-28T22:23:20.241695: step 2151, loss 0.0757153, acc 0.9375, learning_rate 0.000568569\n","2023-06-28T22:23:20.338962: step 2152, loss 0.0659934, acc 1, learning_rate 0.000568107\n","2023-06-28T22:23:20.443095: step 2153, loss 0.190535, acc 0.9375, learning_rate 0.000567647\n","2023-06-28T22:23:20.539483: step 2154, loss 0.0395981, acc 1, learning_rate 0.000567186\n","2023-06-28T22:23:20.635671: step 2155, loss 0.0869612, acc 0.96875, learning_rate 0.000566726\n","2023-06-28T22:23:20.746061: step 2156, loss 0.498186, acc 0.8125, learning_rate 0.000566267\n","2023-06-28T22:23:20.842018: step 2157, loss 0.567581, acc 0.90625, learning_rate 0.000565807\n","2023-06-28T22:23:20.959695: step 2158, loss 0.0292835, acc 1, learning_rate 0.000565349\n","2023-06-28T22:23:21.056614: step 2159, loss 0.35873, acc 0.90625, learning_rate 0.000564891\n","2023-06-28T22:23:21.161432: step 2160, loss 0.118669, acc 0.96875, learning_rate 0.000564433\n","2023-06-28T22:23:21.258714: step 2161, loss 0.27331, acc 0.90625, learning_rate 0.000563975\n","2023-06-28T22:23:21.358659: step 2162, loss 0.974053, acc 0.875, learning_rate 0.000563519\n","2023-06-28T22:23:21.467724: step 2163, loss 0.328548, acc 0.90625, learning_rate 0.000563062\n","2023-06-28T22:23:21.581798: step 2164, loss 0.110957, acc 0.96875, learning_rate 0.000562606\n","2023-06-28T22:23:21.681479: step 2165, loss 0.0533901, acc 0.96875, learning_rate 0.000562151\n","2023-06-28T22:23:21.782288: step 2166, loss 0.149409, acc 0.90625, learning_rate 0.000561696\n","2023-06-28T22:23:21.917562: step 2167, loss 0.101449, acc 0.9375, learning_rate 0.000561241\n","2023-06-28T22:23:22.018076: step 2168, loss 0.306061, acc 0.84375, learning_rate 0.000560787\n","2023-06-28T22:23:22.123820: step 2169, loss 0.459125, acc 0.84375, learning_rate 0.000560333\n","2023-06-28T22:23:22.229156: step 2170, loss 0.299388, acc 0.875, learning_rate 0.00055988\n","2023-06-28T22:23:22.331752: step 2171, loss 0.085343, acc 1, learning_rate 0.000559427\n","2023-06-28T22:23:22.428619: step 2172, loss 0.523728, acc 0.90625, learning_rate 0.000558975\n","2023-06-28T22:23:22.526529: step 2173, loss 0.304409, acc 0.90625, learning_rate 0.000558523\n","2023-06-28T22:23:22.627871: step 2174, loss 0.243445, acc 0.875, learning_rate 0.000558071\n","2023-06-28T22:23:22.727101: step 2175, loss 0.111805, acc 0.90625, learning_rate 0.00055762\n","2023-06-28T22:23:22.831321: step 2176, loss 0.220385, acc 0.96875, learning_rate 0.00055717\n","2023-06-28T22:23:22.944653: step 2177, loss 0.143822, acc 0.9375, learning_rate 0.000556719\n","2023-06-28T22:23:23.039735: step 2178, loss 0.362179, acc 0.9375, learning_rate 0.00055627\n","2023-06-28T22:23:23.141191: step 2179, loss 0.428498, acc 0.875, learning_rate 0.00055582\n","2023-06-28T22:23:23.245675: step 2180, loss 0.327611, acc 0.875, learning_rate 0.000555372\n","2023-06-28T22:23:23.352686: step 2181, loss 0.412201, acc 0.875, learning_rate 0.000554923\n","2023-06-28T22:23:23.449934: step 2182, loss 0.23556, acc 0.875, learning_rate 0.000554475\n","2023-06-28T22:23:23.561928: step 2183, loss 0.241088, acc 0.90625, learning_rate 0.000554028\n","2023-06-28T22:23:23.659598: step 2184, loss 0.285035, acc 0.9375, learning_rate 0.000553581\n","2023-06-28T22:23:23.762269: step 2185, loss 0.114814, acc 0.96875, learning_rate 0.000553134\n","2023-06-28T22:23:23.860488: step 2186, loss 0.195671, acc 0.90625, learning_rate 0.000552688\n","2023-06-28T22:23:23.970002: step 2187, loss 0.113443, acc 0.9375, learning_rate 0.000552242\n","2023-06-28T22:23:24.074282: step 2188, loss 0.124111, acc 0.96875, learning_rate 0.000551797\n","2023-06-28T22:23:24.179463: step 2189, loss 0.41878, acc 0.875, learning_rate 0.000551352\n","2023-06-28T22:23:24.288678: step 2190, loss 0.101946, acc 0.96875, learning_rate 0.000550908\n","2023-06-28T22:23:24.384296: step 2191, loss 0.120211, acc 0.96875, learning_rate 0.000550464\n","2023-06-28T22:23:24.477226: step 2192, loss 0.108495, acc 0.9375, learning_rate 0.00055002\n","2023-06-28T22:23:24.578238: step 2193, loss 0.215436, acc 0.9375, learning_rate 0.000549577\n","2023-06-28T22:23:24.675312: step 2194, loss 0.310919, acc 0.9375, learning_rate 0.000549134\n","2023-06-28T22:23:24.778082: step 2195, loss 0.14736, acc 0.9375, learning_rate 0.000548692\n","2023-06-28T22:23:24.888179: step 2196, loss 0.0905253, acc 0.96875, learning_rate 0.00054825\n","2023-06-28T22:23:24.992845: step 2197, loss 0.169895, acc 0.90625, learning_rate 0.000547809\n","2023-06-28T22:23:25.120563: step 2198, loss 0.112281, acc 0.96875, learning_rate 0.000547368\n","2023-06-28T22:23:25.275303: step 2199, loss 0.241695, acc 0.84375, learning_rate 0.000546927\n","\n","Evaluation:\n","2023-06-28T22:23:26.626510: step 2200, loss 0.747372, acc 0.835591\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2200\n","\n","2023-06-28T22:23:26.914213: step 2200, loss 0.221582, acc 0.84375, learning_rate 0.000546487\n","2023-06-28T22:23:27.096319: step 2201, loss 0.125267, acc 0.96875, learning_rate 0.000546048\n","2023-06-28T22:23:27.277823: step 2202, loss 0.202862, acc 0.9375, learning_rate 0.000545608\n","2023-06-28T22:23:27.450651: step 2203, loss 0.0757966, acc 0.9375, learning_rate 0.00054517\n","2023-06-28T22:23:27.627233: step 2204, loss 0.558457, acc 0.90625, learning_rate 0.000544731\n","2023-06-28T22:23:27.802810: step 2205, loss 0.15207, acc 0.9375, learning_rate 0.000544293\n","2023-06-28T22:23:27.969125: step 2206, loss 0.86294, acc 0.9375, learning_rate 0.000543856\n","2023-06-28T22:23:28.167160: step 2207, loss 0.373566, acc 0.84375, learning_rate 0.000543419\n","2023-06-28T22:23:28.361029: step 2208, loss 0.214567, acc 0.90625, learning_rate 0.000542982\n","2023-06-28T22:23:28.534305: step 2209, loss 0.457187, acc 0.9375, learning_rate 0.000542546\n","2023-06-28T22:23:28.702113: step 2210, loss 0.364604, acc 0.84375, learning_rate 0.00054211\n","2023-06-28T22:23:28.857846: step 2211, loss 0.344587, acc 0.84375, learning_rate 0.000541675\n","2023-06-28T22:23:29.018513: step 2212, loss 0.409111, acc 0.90625, learning_rate 0.00054124\n","2023-06-28T22:23:29.185266: step 2213, loss 0.127348, acc 0.9375, learning_rate 0.000540806\n","2023-06-28T22:23:29.343115: step 2214, loss 0.27133, acc 0.9375, learning_rate 0.000540372\n","2023-06-28T22:23:29.503928: step 2215, loss 0.159139, acc 0.90625, learning_rate 0.000539938\n","2023-06-28T22:23:29.677769: step 2216, loss 0.124985, acc 0.9375, learning_rate 0.000539505\n","2023-06-28T22:23:29.839516: step 2217, loss 0.245996, acc 0.9375, learning_rate 0.000539072\n","2023-06-28T22:23:30.009370: step 2218, loss 0.128451, acc 0.90625, learning_rate 0.00053864\n","2023-06-28T22:23:30.188050: step 2219, loss 0.157314, acc 0.96875, learning_rate 0.000538208\n","2023-06-28T22:23:30.375487: step 2220, loss 0.0980161, acc 0.96875, learning_rate 0.000537776\n","2023-06-28T22:23:30.548449: step 2221, loss 0.0623489, acc 1, learning_rate 0.000537345\n","2023-06-28T22:23:30.709298: step 2222, loss 0.380503, acc 0.9375, learning_rate 0.000536915\n","2023-06-28T22:23:30.887425: step 2223, loss 0.161692, acc 0.90625, learning_rate 0.000536484\n","2023-06-28T22:23:31.054345: step 2224, loss 0.0674482, acc 1, learning_rate 0.000536055\n","2023-06-28T22:23:31.225859: step 2225, loss 0.121201, acc 0.96875, learning_rate 0.000535625\n","2023-06-28T22:23:31.376765: step 2226, loss 0.185609, acc 0.96875, learning_rate 0.000535196\n","2023-06-28T22:23:31.548590: step 2227, loss 0.10993, acc 0.9375, learning_rate 0.000534768\n","2023-06-28T22:23:31.712237: step 2228, loss 0.138028, acc 0.9375, learning_rate 0.00053434\n","2023-06-28T22:23:31.875077: step 2229, loss 0.0962455, acc 1, learning_rate 0.000533912\n","2023-06-28T22:23:32.040240: step 2230, loss 0.534278, acc 0.84375, learning_rate 0.000533485\n","2023-06-28T22:23:32.217580: step 2231, loss 0.0812175, acc 0.96875, learning_rate 0.000533058\n","2023-06-28T22:23:32.379854: step 2232, loss 0.126305, acc 0.9375, learning_rate 0.000532631\n","2023-06-28T22:23:32.546000: step 2233, loss 0.189348, acc 0.9375, learning_rate 0.000532205\n","2023-06-28T22:23:32.716967: step 2234, loss 0.205399, acc 0.96875, learning_rate 0.00053178\n","2023-06-28T22:23:32.861867: step 2235, loss 0.391857, acc 0.78125, learning_rate 0.000531355\n","2023-06-28T22:23:33.043121: step 2236, loss 0.22753, acc 0.8125, learning_rate 0.00053093\n","2023-06-28T22:23:33.217274: step 2237, loss 0.111847, acc 0.9375, learning_rate 0.000530506\n","2023-06-28T22:23:33.413378: step 2238, loss 0.137205, acc 0.96875, learning_rate 0.000530082\n","2023-06-28T22:23:33.592698: step 2239, loss 0.489253, acc 0.90625, learning_rate 0.000529658\n","2023-06-28T22:23:33.752847: step 2240, loss 0.344967, acc 0.8125, learning_rate 0.000529235\n","2023-06-28T22:23:33.929232: step 2241, loss 0.607013, acc 0.9375, learning_rate 0.000528813\n","2023-06-28T22:23:34.104193: step 2242, loss 0.172453, acc 0.9375, learning_rate 0.00052839\n","2023-06-28T22:23:34.292410: step 2243, loss 0.0615355, acc 1, learning_rate 0.000527969\n","2023-06-28T22:23:34.446161: step 2244, loss 0.257087, acc 0.90625, learning_rate 0.000527547\n","2023-06-28T22:23:34.620978: step 2245, loss 0.141278, acc 0.9375, learning_rate 0.000527126\n","2023-06-28T22:23:34.788454: step 2246, loss 0.238229, acc 0.9375, learning_rate 0.000526706\n","2023-06-28T22:23:34.947691: step 2247, loss 0.186199, acc 0.9375, learning_rate 0.000526285\n","2023-06-28T22:23:35.105072: step 2248, loss 0.221056, acc 0.875, learning_rate 0.000525866\n","2023-06-28T22:23:35.285480: step 2249, loss 0.157904, acc 0.9375, learning_rate 0.000525446\n","2023-06-28T22:23:35.463535: step 2250, loss 0.212015, acc 0.9375, learning_rate 0.000525027\n","2023-06-28T22:23:35.636825: step 2251, loss 0.203717, acc 0.90625, learning_rate 0.000524609\n","2023-06-28T22:23:35.802735: step 2252, loss 0.272501, acc 0.90625, learning_rate 0.000524191\n","2023-06-28T22:23:35.937462: step 2253, loss 0.46257, acc 0.9375, learning_rate 0.000523773\n","2023-06-28T22:23:36.038755: step 2254, loss 0.159979, acc 0.9375, learning_rate 0.000523356\n","2023-06-28T22:23:36.144039: step 2255, loss 0.215722, acc 0.875, learning_rate 0.000522939\n","2023-06-28T22:23:36.256151: step 2256, loss 0.368453, acc 0.90625, learning_rate 0.000522523\n","2023-06-28T22:23:36.353013: step 2257, loss 0.133698, acc 0.9375, learning_rate 0.000522107\n","2023-06-28T22:23:36.464167: step 2258, loss 0.167879, acc 0.90625, learning_rate 0.000521691\n","2023-06-28T22:23:36.573454: step 2259, loss 0.30034, acc 0.875, learning_rate 0.000521276\n","2023-06-28T22:23:36.670406: step 2260, loss 0.169112, acc 0.9375, learning_rate 0.000520861\n","2023-06-28T22:23:36.777923: step 2261, loss 0.528039, acc 0.875, learning_rate 0.000520446\n","2023-06-28T22:23:36.877684: step 2262, loss 0.335843, acc 0.9375, learning_rate 0.000520032\n","2023-06-28T22:23:36.976276: step 2263, loss 0.861304, acc 0.90625, learning_rate 0.000519619\n","2023-06-28T22:23:37.082801: step 2264, loss 0.218597, acc 0.9375, learning_rate 0.000519206\n","2023-06-28T22:23:37.187413: step 2265, loss 0.169474, acc 0.90625, learning_rate 0.000518793\n","2023-06-28T22:23:37.297300: step 2266, loss 0.634072, acc 0.84375, learning_rate 0.000518381\n","2023-06-28T22:23:37.404662: step 2267, loss 0.34714, acc 0.9375, learning_rate 0.000517969\n","2023-06-28T22:23:37.519001: step 2268, loss 0.127295, acc 0.9375, learning_rate 0.000517557\n","2023-06-28T22:23:37.616793: step 2269, loss 0.0730126, acc 0.96875, learning_rate 0.000517146\n","2023-06-28T22:23:37.717449: step 2270, loss 0.323764, acc 0.90625, learning_rate 0.000516735\n","2023-06-28T22:23:37.821709: step 2271, loss 0.313769, acc 0.875, learning_rate 0.000516325\n","2023-06-28T22:23:37.919841: step 2272, loss 0.440665, acc 0.84375, learning_rate 0.000515915\n","2023-06-28T22:23:38.023326: step 2273, loss 0.115118, acc 0.9375, learning_rate 0.000515505\n","2023-06-28T22:23:38.125212: step 2274, loss 0.201413, acc 0.875, learning_rate 0.000515096\n","2023-06-28T22:23:38.222823: step 2275, loss 0.257939, acc 0.96875, learning_rate 0.000514687\n","2023-06-28T22:23:38.334779: step 2276, loss 0.035072, acc 1, learning_rate 0.000514279\n","2023-06-28T22:23:38.434147: step 2277, loss 0.447394, acc 0.875, learning_rate 0.000513871\n","2023-06-28T22:23:38.557735: step 2278, loss 0.359914, acc 0.9375, learning_rate 0.000513464\n","2023-06-28T22:23:38.656996: step 2279, loss 0.218818, acc 0.90625, learning_rate 0.000513057\n","2023-06-28T22:23:38.754452: step 2280, loss 0.126835, acc 0.9375, learning_rate 0.00051265\n","2023-06-28T22:23:38.879257: step 2281, loss 0.482781, acc 0.875, learning_rate 0.000512243\n","2023-06-28T22:23:38.986305: step 2282, loss 0.0894048, acc 1, learning_rate 0.000511838\n","2023-06-28T22:23:39.094284: step 2283, loss 0.143992, acc 0.90625, learning_rate 0.000511432\n","2023-06-28T22:23:39.210615: step 2284, loss 0.21961, acc 0.90625, learning_rate 0.000511027\n","2023-06-28T22:23:39.312437: step 2285, loss 0.121198, acc 0.96875, learning_rate 0.000510622\n","2023-06-28T22:23:39.408460: step 2286, loss 0.156347, acc 0.96875, learning_rate 0.000510218\n","2023-06-28T22:23:39.527945: step 2287, loss 0.728798, acc 0.875, learning_rate 0.000509814\n","2023-06-28T22:23:39.627111: step 2288, loss 0.134849, acc 0.9375, learning_rate 0.00050941\n","2023-06-28T22:23:39.723111: step 2289, loss 0.481208, acc 0.875, learning_rate 0.000509007\n","2023-06-28T22:23:39.831392: step 2290, loss 0.255973, acc 0.84375, learning_rate 0.000508605\n","2023-06-28T22:23:39.928584: step 2291, loss 0.173778, acc 0.90625, learning_rate 0.000508202\n","2023-06-28T22:23:40.028034: step 2292, loss 0.195935, acc 0.9375, learning_rate 0.0005078\n","2023-06-28T22:23:40.143303: step 2293, loss 0.429503, acc 0.90625, learning_rate 0.000507399\n","2023-06-28T22:23:40.252040: step 2294, loss 0.265387, acc 0.9375, learning_rate 0.000506998\n","2023-06-28T22:23:40.352389: step 2295, loss 0.181869, acc 0.90625, learning_rate 0.000506597\n","2023-06-28T22:23:40.452802: step 2296, loss 0.241112, acc 0.90625, learning_rate 0.000506196\n","2023-06-28T22:23:40.563977: step 2297, loss 0.0292191, acc 1, learning_rate 0.000505797\n","2023-06-28T22:23:40.674615: step 2298, loss 0.0455709, acc 1, learning_rate 0.000505397\n","2023-06-28T22:23:40.774658: step 2299, loss 0.0972233, acc 1, learning_rate 0.000504998\n","\n","Evaluation:\n","2023-06-28T22:23:41.473902: step 2300, loss 0.797428, acc 0.832204\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2300\n","\n","2023-06-28T22:23:41.693495: step 2300, loss 0.627226, acc 0.90625, learning_rate 0.000504599\n","2023-06-28T22:23:41.791495: step 2301, loss 0.328386, acc 0.90625, learning_rate 0.000504201\n","2023-06-28T22:23:41.893748: step 2302, loss 0.115017, acc 0.9375, learning_rate 0.000503803\n","2023-06-28T22:23:41.995008: step 2303, loss 0.279863, acc 0.9375, learning_rate 0.000503405\n","2023-06-28T22:23:42.092454: step 2304, loss 0.262122, acc 0.875, learning_rate 0.000503008\n","2023-06-28T22:23:42.193202: step 2305, loss 0.145204, acc 0.9375, learning_rate 0.000502611\n","2023-06-28T22:23:42.299463: step 2306, loss 0.160651, acc 0.9375, learning_rate 0.000502214\n","2023-06-28T22:23:42.401690: step 2307, loss 0.523878, acc 0.90625, learning_rate 0.000501818\n","2023-06-28T22:23:42.498459: step 2308, loss 0.349715, acc 0.875, learning_rate 0.000501423\n","2023-06-28T22:23:42.615235: step 2309, loss 0.242735, acc 0.875, learning_rate 0.000501028\n","2023-06-28T22:23:42.721074: step 2310, loss 0.179626, acc 0.90625, learning_rate 0.000500633\n","2023-06-28T22:23:42.826573: step 2311, loss 0.277812, acc 0.90625, learning_rate 0.000500238\n","2023-06-28T22:23:42.927121: step 2312, loss 0.175856, acc 0.90625, learning_rate 0.000499844\n","2023-06-28T22:23:43.027221: step 2313, loss 0.150317, acc 0.96875, learning_rate 0.00049945\n","2023-06-28T22:23:43.137464: step 2314, loss 0.448451, acc 0.875, learning_rate 0.000499057\n","2023-06-28T22:23:43.234938: step 2315, loss 0.287444, acc 0.90625, learning_rate 0.000498664\n","2023-06-28T22:23:43.337887: step 2316, loss 0.259521, acc 0.90625, learning_rate 0.000498272\n","2023-06-28T22:23:43.445531: step 2317, loss 0.583669, acc 0.84375, learning_rate 0.000497879\n","2023-06-28T22:23:43.541884: step 2318, loss 0.143929, acc 0.90625, learning_rate 0.000497488\n","2023-06-28T22:23:43.647957: step 2319, loss 0.111516, acc 0.96875, learning_rate 0.000497096\n","2023-06-28T22:23:43.746438: step 2320, loss 0.124186, acc 0.9375, learning_rate 0.000496705\n","2023-06-28T22:23:43.841078: step 2321, loss 0.298474, acc 0.875, learning_rate 0.000496315\n","2023-06-28T22:23:43.942907: step 2322, loss 0.0628921, acc 0.96875, learning_rate 0.000495924\n","2023-06-28T22:23:44.044297: step 2323, loss 0.168066, acc 0.9375, learning_rate 0.000495535\n","2023-06-28T22:23:44.144445: step 2324, loss 0.287047, acc 0.9375, learning_rate 0.000495145\n","2023-06-28T22:23:44.247492: step 2325, loss 0.133901, acc 0.96875, learning_rate 0.000494756\n","2023-06-28T22:23:44.346642: step 2326, loss 0.0708583, acc 1, learning_rate 0.000494367\n","2023-06-28T22:23:44.445697: step 2327, loss 0.116047, acc 0.96875, learning_rate 0.000493979\n","2023-06-28T22:23:44.547820: step 2328, loss 0.123157, acc 0.9375, learning_rate 0.000493591\n","2023-06-28T22:23:44.651936: step 2329, loss 0.581239, acc 0.875, learning_rate 0.000493203\n","2023-06-28T22:23:44.771838: step 2330, loss 0.117244, acc 0.96875, learning_rate 0.000492816\n","2023-06-28T22:23:44.875804: step 2331, loss 0.234288, acc 0.875, learning_rate 0.000492429\n","2023-06-28T22:23:44.981063: step 2332, loss 0.211793, acc 0.90625, learning_rate 0.000492043\n","2023-06-28T22:23:45.075847: step 2333, loss 0.184387, acc 0.9375, learning_rate 0.000491657\n","2023-06-28T22:23:45.181153: step 2334, loss 0.333282, acc 0.84375, learning_rate 0.000491271\n","2023-06-28T22:23:45.280209: step 2335, loss 0.162431, acc 0.90625, learning_rate 0.000490886\n","2023-06-28T22:23:45.374831: step 2336, loss 0.209723, acc 0.90625, learning_rate 0.000490501\n","2023-06-28T22:23:45.475525: step 2337, loss 0.0807645, acc 0.96875, learning_rate 0.000490117\n","2023-06-28T22:23:45.575362: step 2338, loss 0.329668, acc 0.90625, learning_rate 0.000489733\n","2023-06-28T22:23:45.687267: step 2339, loss 0.167724, acc 0.96875, learning_rate 0.000489349\n","2023-06-28T22:23:45.792629: step 2340, loss 0.172488, acc 0.96875, learning_rate 0.000488965\n","2023-06-28T22:23:45.889724: step 2341, loss 0.150965, acc 0.9375, learning_rate 0.000488582\n","2023-06-28T22:23:46.037622: step 2342, loss 0.227423, acc 0.875, learning_rate 0.0004882\n","2023-06-28T22:23:46.203316: step 2343, loss 0.507625, acc 0.875, learning_rate 0.000487818\n","2023-06-28T22:23:46.373686: step 2344, loss 0.157169, acc 0.96875, learning_rate 0.000487436\n","2023-06-28T22:23:46.559735: step 2345, loss 0.27852, acc 0.90625, learning_rate 0.000487054\n","2023-06-28T22:23:46.745267: step 2346, loss 0.33307, acc 0.90625, learning_rate 0.000486673\n","2023-06-28T22:23:46.929929: step 2347, loss 0.38969, acc 0.84375, learning_rate 0.000486292\n","2023-06-28T22:23:47.112745: step 2348, loss 0.176479, acc 0.875, learning_rate 0.000485912\n","2023-06-28T22:23:47.290371: step 2349, loss 0.0873706, acc 0.96875, learning_rate 0.000485532\n","2023-06-28T22:23:47.458138: step 2350, loss 0.193461, acc 0.90625, learning_rate 0.000485152\n","2023-06-28T22:23:47.625945: step 2351, loss 0.445352, acc 0.9375, learning_rate 0.000484773\n","2023-06-28T22:23:47.804493: step 2352, loss 0.357399, acc 0.9375, learning_rate 0.000484394\n","2023-06-28T22:23:47.983003: step 2353, loss 0.369561, acc 0.84375, learning_rate 0.000484016\n","2023-06-28T22:23:48.158696: step 2354, loss 0.0655489, acc 0.96875, learning_rate 0.000483638\n","2023-06-28T22:23:48.339111: step 2355, loss 0.45368, acc 0.90625, learning_rate 0.00048326\n","2023-06-28T22:23:48.511298: step 2356, loss 0.232866, acc 0.875, learning_rate 0.000482883\n","2023-06-28T22:23:48.691684: step 2357, loss 0.409615, acc 0.78125, learning_rate 0.000482506\n","2023-06-28T22:23:48.840769: step 2358, loss 0.156965, acc 0.9375, learning_rate 0.000482129\n","2023-06-28T22:23:49.014291: step 2359, loss 0.465544, acc 0.9375, learning_rate 0.000481753\n","2023-06-28T22:23:49.192476: step 2360, loss 0.253992, acc 0.90625, learning_rate 0.000481377\n","2023-06-28T22:23:49.420481: step 2361, loss 0.119723, acc 0.96875, learning_rate 0.000481001\n","2023-06-28T22:23:49.606451: step 2362, loss 0.0860263, acc 1, learning_rate 0.000480626\n","2023-06-28T22:23:49.780838: step 2363, loss 0.295357, acc 0.84375, learning_rate 0.000480251\n","2023-06-28T22:23:49.953208: step 2364, loss 0.539778, acc 0.78125, learning_rate 0.000479877\n","2023-06-28T22:23:50.124720: step 2365, loss 0.0993874, acc 0.96875, learning_rate 0.000479503\n","2023-06-28T22:23:50.289988: step 2366, loss 0.28941, acc 0.875, learning_rate 0.000479129\n","2023-06-28T22:23:50.448372: step 2367, loss 0.386255, acc 0.875, learning_rate 0.000478756\n","2023-06-28T22:23:50.608838: step 2368, loss 0.166754, acc 0.90625, learning_rate 0.000478383\n","2023-06-28T22:23:50.771340: step 2369, loss 0.0548881, acc 0.96875, learning_rate 0.00047801\n","2023-06-28T22:23:50.924989: step 2370, loss 0.0891673, acc 0.96875, learning_rate 0.000477638\n","2023-06-28T22:23:51.097030: step 2371, loss 0.361574, acc 0.90625, learning_rate 0.000477266\n","2023-06-28T22:23:51.268331: step 2372, loss 0.126801, acc 0.9375, learning_rate 0.000476895\n","2023-06-28T22:23:51.443583: step 2373, loss 0.143181, acc 0.96875, learning_rate 0.000476524\n","2023-06-28T22:23:51.609532: step 2374, loss 0.451223, acc 0.875, learning_rate 0.000476153\n","2023-06-28T22:23:51.786346: step 2375, loss 0.101584, acc 0.96875, learning_rate 0.000475782\n","2023-06-28T22:23:51.979504: step 2376, loss 0.128333, acc 0.9375, learning_rate 0.000475412\n","2023-06-28T22:23:52.152524: step 2377, loss 0.525921, acc 0.90625, learning_rate 0.000475043\n","2023-06-28T22:23:52.348650: step 2378, loss 0.282149, acc 0.96875, learning_rate 0.000474674\n","2023-06-28T22:23:52.543928: step 2379, loss 0.168153, acc 0.9375, learning_rate 0.000474305\n","2023-06-28T22:23:52.719509: step 2380, loss 0.114406, acc 0.9375, learning_rate 0.000473936\n","2023-06-28T22:23:52.918628: step 2381, loss 0.253789, acc 0.875, learning_rate 0.000473568\n","2023-06-28T22:23:53.103671: step 2382, loss 0.161863, acc 0.9375, learning_rate 0.0004732\n","2023-06-28T22:23:53.296121: step 2383, loss 0.302104, acc 0.875, learning_rate 0.000472833\n","2023-06-28T22:23:53.503491: step 2384, loss 0.156481, acc 0.9375, learning_rate 0.000472465\n","2023-06-28T22:23:53.740481: step 2385, loss 0.0612427, acc 1, learning_rate 0.000472099\n","2023-06-28T22:23:53.959552: step 2386, loss 0.273951, acc 0.90625, learning_rate 0.000471732\n","2023-06-28T22:23:54.154831: step 2387, loss 0.566268, acc 0.875, learning_rate 0.000471366\n","2023-06-28T22:23:54.366000: step 2388, loss 0.0583848, acc 0.96875, learning_rate 0.000471001\n","2023-06-28T22:23:54.610754: step 2389, loss 0.353617, acc 0.84375, learning_rate 0.000470635\n","2023-06-28T22:23:54.805132: step 2390, loss 0.220336, acc 0.9375, learning_rate 0.00047027\n","2023-06-28T22:23:54.982606: step 2391, loss 0.371398, acc 0.84375, learning_rate 0.000469906\n","2023-06-28T22:23:55.195508: step 2392, loss 0.37721, acc 0.875, learning_rate 0.000469541\n","2023-06-28T22:23:55.373172: step 2393, loss 0.0776281, acc 0.9375, learning_rate 0.000469178\n","2023-06-28T22:23:55.573945: step 2394, loss 0.155325, acc 0.90625, learning_rate 0.000468814\n","2023-06-28T22:23:55.778399: step 2395, loss 0.233967, acc 0.875, learning_rate 0.000468451\n","2023-06-28T22:23:55.994269: step 2396, loss 0.109138, acc 0.9375, learning_rate 0.000468088\n","2023-06-28T22:23:56.225087: step 2397, loss 0.279284, acc 0.9375, learning_rate 0.000467726\n","2023-06-28T22:23:56.418615: step 2398, loss 0.242898, acc 0.9375, learning_rate 0.000467364\n","2023-06-28T22:23:56.666424: step 2399, loss 0.113922, acc 0.96875, learning_rate 0.000467002\n","\n","Evaluation:\n","2023-06-28T22:23:58.046619: step 2400, loss 0.766906, acc 0.834052\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2400\n","\n","2023-06-28T22:23:58.444761: step 2400, loss 0.175189, acc 0.96875, learning_rate 0.00046664\n","2023-06-28T22:23:58.623692: step 2401, loss 0.188349, acc 0.9375, learning_rate 0.000466279\n","2023-06-28T22:23:58.798231: step 2402, loss 0.268238, acc 0.84375, learning_rate 0.000465919\n","2023-06-28T22:23:58.976956: step 2403, loss 0.113537, acc 0.9375, learning_rate 0.000465559\n","2023-06-28T22:23:59.160199: step 2404, loss 0.518577, acc 0.84375, learning_rate 0.000465199\n","2023-06-28T22:23:59.368881: step 2405, loss 0.158067, acc 0.9375, learning_rate 0.000464839\n","2023-06-28T22:23:59.543711: step 2406, loss 0.20821, acc 0.9375, learning_rate 0.00046448\n","2023-06-28T22:23:59.750086: step 2407, loss 0.0995209, acc 0.96875, learning_rate 0.000464121\n","2023-06-28T22:23:59.931260: step 2408, loss 0.237704, acc 0.96875, learning_rate 0.000463762\n","2023-06-28T22:24:00.102042: step 2409, loss 0.221728, acc 0.9375, learning_rate 0.000463404\n","2023-06-28T22:24:00.273708: step 2410, loss 0.107792, acc 0.9375, learning_rate 0.000463046\n","2023-06-28T22:24:00.449637: step 2411, loss 0.480589, acc 0.84375, learning_rate 0.000462689\n","2023-06-28T22:24:00.616635: step 2412, loss 0.366701, acc 0.875, learning_rate 0.000462332\n","2023-06-28T22:24:00.780350: step 2413, loss 0.0681169, acc 1, learning_rate 0.000461975\n","2023-06-28T22:24:00.951249: step 2414, loss 0.126918, acc 0.96875, learning_rate 0.000461618\n","2023-06-28T22:24:01.120309: step 2415, loss 0.305128, acc 0.875, learning_rate 0.000461262\n","2023-06-28T22:24:01.293627: step 2416, loss 0.0813273, acc 1, learning_rate 0.000460907\n","2023-06-28T22:24:01.486040: step 2417, loss 0.149091, acc 0.96875, learning_rate 0.000460551\n","2023-06-28T22:24:01.635310: step 2418, loss 0.280322, acc 0.96875, learning_rate 0.000460196\n","2023-06-28T22:24:01.810264: step 2419, loss 0.613156, acc 0.90625, learning_rate 0.000459842\n","2023-06-28T22:24:01.982807: step 2420, loss 0.0979294, acc 0.9375, learning_rate 0.000459487\n","2023-06-28T22:24:02.175331: step 2421, loss 0.432258, acc 0.84375, learning_rate 0.000459133\n","2023-06-28T22:24:02.331089: step 2422, loss 0.092243, acc 0.96875, learning_rate 0.00045878\n","2023-06-28T22:24:02.512637: step 2423, loss 0.265348, acc 0.96875, learning_rate 0.000458426\n","2023-06-28T22:24:02.663239: step 2424, loss 0.132562, acc 0.9375, learning_rate 0.000458074\n","2023-06-28T22:24:02.841184: step 2425, loss 0.389977, acc 0.90625, learning_rate 0.000457721\n","2023-06-28T22:24:03.010357: step 2426, loss 0.375583, acc 0.875, learning_rate 0.000457369\n","2023-06-28T22:24:03.181087: step 2427, loss 0.0766374, acc 0.96875, learning_rate 0.000457017\n","2023-06-28T22:24:03.344132: step 2428, loss 0.221322, acc 0.90625, learning_rate 0.000456665\n","2023-06-28T22:24:03.515447: step 2429, loss 0.0570796, acc 1, learning_rate 0.000456314\n","2023-06-28T22:24:03.696307: step 2430, loss 0.205569, acc 0.9375, learning_rate 0.000455963\n","2023-06-28T22:24:03.869616: step 2431, loss 0.204273, acc 0.9375, learning_rate 0.000455613\n","2023-06-28T22:24:04.036785: step 2432, loss 0.256708, acc 0.875, learning_rate 0.000455263\n","2023-06-28T22:24:04.207428: step 2433, loss 0.721481, acc 0.8125, learning_rate 0.000454913\n","2023-06-28T22:24:04.406359: step 2434, loss 0.169787, acc 0.90625, learning_rate 0.000454563\n","2023-06-28T22:24:04.571944: step 2435, loss 0.149567, acc 0.96875, learning_rate 0.000454214\n","2023-06-28T22:24:04.742762: step 2436, loss 0.288465, acc 0.84375, learning_rate 0.000453865\n","2023-06-28T22:24:04.914701: step 2437, loss 0.0554662, acc 1, learning_rate 0.000453517\n","2023-06-28T22:24:05.099140: step 2438, loss 0.160228, acc 0.9375, learning_rate 0.000453169\n","2023-06-28T22:24:05.267065: step 2439, loss 0.0786646, acc 0.96875, learning_rate 0.000452821\n","2023-06-28T22:24:05.434715: step 2440, loss 0.0667727, acc 0.96875, learning_rate 0.000452474\n","2023-06-28T22:24:05.540013: step 2441, loss 0.00042248, acc 1, learning_rate 0.000452127\n","2023-06-28T22:24:05.711174: step 2442, loss 0.0845105, acc 0.96875, learning_rate 0.00045178\n","2023-06-28T22:24:05.854709: step 2443, loss 0.0736721, acc 0.96875, learning_rate 0.000451434\n","2023-06-28T22:24:06.016533: step 2444, loss 0.0846034, acc 0.96875, learning_rate 0.000451087\n","2023-06-28T22:24:06.173870: step 2445, loss 0.657334, acc 0.875, learning_rate 0.000450742\n","2023-06-28T22:24:06.311790: step 2446, loss 0.052232, acc 1, learning_rate 0.000450396\n","2023-06-28T22:24:06.414488: step 2447, loss 0.151181, acc 0.9375, learning_rate 0.000450051\n","2023-06-28T22:24:06.512175: step 2448, loss 0.0846687, acc 0.96875, learning_rate 0.000449707\n","2023-06-28T22:24:06.629552: step 2449, loss 0.0456889, acc 1, learning_rate 0.000449362\n","2023-06-28T22:24:06.726894: step 2450, loss 0.220433, acc 0.9375, learning_rate 0.000449018\n","2023-06-28T22:24:06.828278: step 2451, loss 0.0658334, acc 1, learning_rate 0.000448675\n","2023-06-28T22:24:06.923751: step 2452, loss 0.1491, acc 0.9375, learning_rate 0.000448331\n","2023-06-28T22:24:07.028789: step 2453, loss 0.383209, acc 0.90625, learning_rate 0.000447988\n","2023-06-28T22:24:07.133463: step 2454, loss 0.158377, acc 0.96875, learning_rate 0.000447646\n","2023-06-28T22:24:07.229723: step 2455, loss 0.160999, acc 0.9375, learning_rate 0.000447303\n","2023-06-28T22:24:07.329946: step 2456, loss 0.123928, acc 0.9375, learning_rate 0.000446961\n","2023-06-28T22:24:07.435473: step 2457, loss 0.347968, acc 0.90625, learning_rate 0.00044662\n","2023-06-28T22:24:07.531559: step 2458, loss 0.252501, acc 0.9375, learning_rate 0.000446278\n","2023-06-28T22:24:07.636581: step 2459, loss 0.1758, acc 0.90625, learning_rate 0.000445938\n","2023-06-28T22:24:07.727934: step 2460, loss 0.167985, acc 0.9375, learning_rate 0.000445597\n","2023-06-28T22:24:07.829035: step 2461, loss 0.0596418, acc 0.96875, learning_rate 0.000445257\n","2023-06-28T22:24:07.929006: step 2462, loss 0.0711213, acc 0.96875, learning_rate 0.000444917\n","2023-06-28T22:24:08.025026: step 2463, loss 0.194771, acc 0.90625, learning_rate 0.000444577\n","2023-06-28T22:24:08.123757: step 2464, loss 0.350657, acc 0.875, learning_rate 0.000444238\n","2023-06-28T22:24:08.226236: step 2465, loss 0.157226, acc 0.9375, learning_rate 0.000443899\n","2023-06-28T22:24:08.323910: step 2466, loss 0.341185, acc 0.84375, learning_rate 0.00044356\n","2023-06-28T22:24:08.421324: step 2467, loss 0.141236, acc 0.90625, learning_rate 0.000443222\n","2023-06-28T22:24:08.544574: step 2468, loss 0.180838, acc 0.9375, learning_rate 0.000442884\n","2023-06-28T22:24:08.646276: step 2469, loss 0.323085, acc 0.90625, learning_rate 0.000442546\n","2023-06-28T22:24:08.747915: step 2470, loss 0.137365, acc 0.9375, learning_rate 0.000442209\n","2023-06-28T22:24:08.845840: step 2471, loss 0.0596503, acc 0.96875, learning_rate 0.000441872\n","2023-06-28T22:24:08.944714: step 2472, loss 0.0977985, acc 0.96875, learning_rate 0.000441535\n","2023-06-28T22:24:09.051090: step 2473, loss 0.214424, acc 0.9375, learning_rate 0.000441199\n","2023-06-28T22:24:09.154397: step 2474, loss 0.0347391, acc 1, learning_rate 0.000440863\n","2023-06-28T22:24:09.250343: step 2475, loss 0.23657, acc 0.875, learning_rate 0.000440527\n","2023-06-28T22:24:09.356545: step 2476, loss 0.197892, acc 0.9375, learning_rate 0.000440192\n","2023-06-28T22:24:09.455589: step 2477, loss 0.182818, acc 0.9375, learning_rate 0.000439857\n","2023-06-28T22:24:09.553437: step 2478, loss 0.221003, acc 0.875, learning_rate 0.000439523\n","2023-06-28T22:24:09.660514: step 2479, loss 0.143096, acc 0.96875, learning_rate 0.000439188\n","2023-06-28T22:24:09.756831: step 2480, loss 0.208847, acc 0.90625, learning_rate 0.000438854\n","2023-06-28T22:24:09.850967: step 2481, loss 0.165711, acc 0.90625, learning_rate 0.000438521\n","2023-06-28T22:24:09.955550: step 2482, loss 0.117168, acc 0.96875, learning_rate 0.000438187\n","2023-06-28T22:24:10.053372: step 2483, loss 0.125933, acc 0.9375, learning_rate 0.000437854\n","2023-06-28T22:24:10.158925: step 2484, loss 0.180666, acc 0.96875, learning_rate 0.000437522\n","2023-06-28T22:24:10.293726: step 2485, loss 0.206825, acc 0.875, learning_rate 0.000437189\n","2023-06-28T22:24:10.445321: step 2486, loss 0.201473, acc 0.9375, learning_rate 0.000436857\n","2023-06-28T22:24:10.612509: step 2487, loss 0.0553654, acc 1, learning_rate 0.000436526\n","2023-06-28T22:24:10.787906: step 2488, loss 0.0977675, acc 0.96875, learning_rate 0.000436194\n","2023-06-28T22:24:10.957986: step 2489, loss 0.317187, acc 0.9375, learning_rate 0.000435863\n","2023-06-28T22:24:11.130618: step 2490, loss 0.0532416, acc 1, learning_rate 0.000435532\n","2023-06-28T22:24:11.301800: step 2491, loss 0.124409, acc 0.96875, learning_rate 0.000435202\n","2023-06-28T22:24:11.501893: step 2492, loss 0.159129, acc 0.96875, learning_rate 0.000434872\n","2023-06-28T22:24:11.678837: step 2493, loss 0.270824, acc 0.90625, learning_rate 0.000434542\n","2023-06-28T22:24:11.855012: step 2494, loss 0.49751, acc 0.875, learning_rate 0.000434213\n","2023-06-28T22:24:12.020405: step 2495, loss 0.236489, acc 0.96875, learning_rate 0.000433884\n","2023-06-28T22:24:12.196260: step 2496, loss 0.10208, acc 0.96875, learning_rate 0.000433555\n","2023-06-28T22:24:12.363646: step 2497, loss 0.0528352, acc 1, learning_rate 0.000433227\n","2023-06-28T22:24:12.548292: step 2498, loss 0.134573, acc 0.9375, learning_rate 0.000432898\n","2023-06-28T22:24:12.744229: step 2499, loss 0.0417878, acc 1, learning_rate 0.000432571\n","\n","Evaluation:\n","2023-06-28T22:24:14.152872: step 2500, loss 0.749632, acc 0.835591\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2500\n","\n","2023-06-28T22:24:14.435022: step 2500, loss 0.191491, acc 0.875, learning_rate 0.000432243\n","2023-06-28T22:24:14.590135: step 2501, loss 0.770505, acc 0.90625, learning_rate 0.000431916\n","2023-06-28T22:24:14.750605: step 2502, loss 0.0978362, acc 0.9375, learning_rate 0.000431589\n","2023-06-28T22:24:14.919917: step 2503, loss 0.122456, acc 1, learning_rate 0.000431263\n","2023-06-28T22:24:15.096876: step 2504, loss 0.10638, acc 0.96875, learning_rate 0.000430937\n","2023-06-28T22:24:15.262269: step 2505, loss 0.271324, acc 0.96875, learning_rate 0.000430611\n","2023-06-28T22:24:15.421868: step 2506, loss 0.124093, acc 0.9375, learning_rate 0.000430285\n","2023-06-28T22:24:15.583744: step 2507, loss 0.269455, acc 0.90625, learning_rate 0.00042996\n","2023-06-28T22:24:15.738730: step 2508, loss 0.154186, acc 0.90625, learning_rate 0.000429635\n","2023-06-28T22:24:15.904657: step 2509, loss 0.16131, acc 0.90625, learning_rate 0.00042931\n","2023-06-28T22:24:16.063931: step 2510, loss 0.240637, acc 0.90625, learning_rate 0.000428986\n","2023-06-28T22:24:16.232897: step 2511, loss 0.0722627, acc 0.96875, learning_rate 0.000428662\n","2023-06-28T22:24:16.385357: step 2512, loss 0.146283, acc 0.9375, learning_rate 0.000428339\n","2023-06-28T22:24:16.546896: step 2513, loss 0.428816, acc 0.96875, learning_rate 0.000428015\n","2023-06-28T22:24:16.698788: step 2514, loss 0.106813, acc 0.96875, learning_rate 0.000427692\n","2023-06-28T22:24:16.864301: step 2515, loss 0.102182, acc 0.96875, learning_rate 0.00042737\n","2023-06-28T22:24:17.038665: step 2516, loss 0.28829, acc 0.9375, learning_rate 0.000427047\n","2023-06-28T22:24:17.197703: step 2517, loss 0.186652, acc 0.9375, learning_rate 0.000426725\n","2023-06-28T22:24:17.362807: step 2518, loss 0.0479473, acc 1, learning_rate 0.000426404\n","2023-06-28T22:24:17.527476: step 2519, loss 0.121237, acc 0.9375, learning_rate 0.000426082\n","2023-06-28T22:24:17.704091: step 2520, loss 0.187141, acc 0.90625, learning_rate 0.000425761\n","2023-06-28T22:24:17.854751: step 2521, loss 0.277157, acc 0.875, learning_rate 0.00042544\n","2023-06-28T22:24:18.025284: step 2522, loss 0.311977, acc 0.96875, learning_rate 0.00042512\n","2023-06-28T22:24:18.198487: step 2523, loss 0.435297, acc 0.8125, learning_rate 0.0004248\n","2023-06-28T22:24:18.356132: step 2524, loss 0.0566252, acc 1, learning_rate 0.00042448\n","2023-06-28T22:24:18.514211: step 2525, loss 0.158221, acc 0.9375, learning_rate 0.00042416\n","2023-06-28T22:24:18.662172: step 2526, loss 0.186854, acc 0.9375, learning_rate 0.000423841\n","2023-06-28T22:24:18.821163: step 2527, loss 0.139405, acc 0.9375, learning_rate 0.000423522\n","2023-06-28T22:24:18.975885: step 2528, loss 0.0512815, acc 1, learning_rate 0.000423204\n","2023-06-28T22:24:19.142766: step 2529, loss 0.0906611, acc 1, learning_rate 0.000422886\n","2023-06-28T22:24:19.347524: step 2530, loss 0.121349, acc 0.96875, learning_rate 0.000422568\n","2023-06-28T22:24:19.518266: step 2531, loss 0.102408, acc 0.96875, learning_rate 0.00042225\n","2023-06-28T22:24:19.690534: step 2532, loss 0.189274, acc 0.875, learning_rate 0.000421933\n","2023-06-28T22:24:19.846468: step 2533, loss 0.158655, acc 0.9375, learning_rate 0.000421616\n","2023-06-28T22:24:19.992819: step 2534, loss 0.10661, acc 0.96875, learning_rate 0.000421299\n","2023-06-28T22:24:20.151555: step 2535, loss 0.188835, acc 0.96875, learning_rate 0.000420983\n","2023-06-28T22:24:20.298154: step 2536, loss 0.170469, acc 0.875, learning_rate 0.000420667\n","2023-06-28T22:24:20.464460: step 2537, loss 0.197156, acc 0.90625, learning_rate 0.000420351\n","2023-06-28T22:24:20.604111: step 2538, loss 0.223261, acc 0.90625, learning_rate 0.000420035\n","2023-06-28T22:24:20.757987: step 2539, loss 0.115465, acc 0.96875, learning_rate 0.00041972\n","2023-06-28T22:24:20.911500: step 2540, loss 0.194214, acc 0.9375, learning_rate 0.000419405\n","2023-06-28T22:24:21.020343: step 2541, loss 0.333554, acc 0.9375, learning_rate 0.000419091\n","2023-06-28T22:24:21.129675: step 2542, loss 0.288016, acc 0.9375, learning_rate 0.000418777\n","2023-06-28T22:24:21.225521: step 2543, loss 0.188689, acc 0.90625, learning_rate 0.000418463\n","2023-06-28T22:24:21.333962: step 2544, loss 0.167942, acc 0.96875, learning_rate 0.000418149\n","2023-06-28T22:24:21.429421: step 2545, loss 0.076078, acc 0.96875, learning_rate 0.000417836\n","2023-06-28T22:24:21.532495: step 2546, loss 0.0783922, acc 0.96875, learning_rate 0.000417523\n","2023-06-28T22:24:21.636884: step 2547, loss 0.130137, acc 0.9375, learning_rate 0.00041721\n","2023-06-28T22:24:21.733193: step 2548, loss 0.189669, acc 0.90625, learning_rate 0.000416898\n","2023-06-28T22:24:21.840093: step 2549, loss 0.0454385, acc 1, learning_rate 0.000416586\n","2023-06-28T22:24:21.933096: step 2550, loss 0.782377, acc 0.84375, learning_rate 0.000416274\n","2023-06-28T22:24:22.047586: step 2551, loss 0.273193, acc 0.90625, learning_rate 0.000415963\n","2023-06-28T22:24:22.154611: step 2552, loss 0.285403, acc 0.875, learning_rate 0.000415652\n","2023-06-28T22:24:22.259531: step 2553, loss 0.0551259, acc 1, learning_rate 0.000415341\n","2023-06-28T22:24:22.395051: step 2554, loss 0.264852, acc 0.9375, learning_rate 0.00041503\n","2023-06-28T22:24:22.493693: step 2555, loss 0.00169682, acc 1, learning_rate 0.00041472\n","2023-06-28T22:24:22.587800: step 2556, loss 0.131816, acc 0.90625, learning_rate 0.00041441\n","2023-06-28T22:24:22.695076: step 2557, loss 0.205191, acc 0.90625, learning_rate 0.000414101\n","2023-06-28T22:24:22.792784: step 2558, loss 0.0617618, acc 0.96875, learning_rate 0.000413792\n","2023-06-28T22:24:22.887269: step 2559, loss 0.161188, acc 0.90625, learning_rate 0.000413483\n","2023-06-28T22:24:22.985048: step 2560, loss 0.134735, acc 0.9375, learning_rate 0.000413174\n","2023-06-28T22:24:23.080184: step 2561, loss 0.347029, acc 0.875, learning_rate 0.000412865\n","2023-06-28T22:24:23.187007: step 2562, loss 0.0615674, acc 1, learning_rate 0.000412557\n","2023-06-28T22:24:23.299104: step 2563, loss 0.170343, acc 0.90625, learning_rate 0.00041225\n","2023-06-28T22:24:23.398922: step 2564, loss 0.280123, acc 0.90625, learning_rate 0.000411942\n","2023-06-28T22:24:23.504595: step 2565, loss 0.155232, acc 0.9375, learning_rate 0.000411635\n","2023-06-28T22:24:23.612288: step 2566, loss 0.230296, acc 0.90625, learning_rate 0.000411328\n","2023-06-28T22:24:23.709063: step 2567, loss 0.216271, acc 0.96875, learning_rate 0.000411022\n","2023-06-28T22:24:23.809210: step 2568, loss 0.206167, acc 0.96875, learning_rate 0.000410715\n","2023-06-28T22:24:23.907257: step 2569, loss 0.10933, acc 0.9375, learning_rate 0.000410409\n","2023-06-28T22:24:24.011781: step 2570, loss 0.189425, acc 0.9375, learning_rate 0.000410104\n","2023-06-28T22:24:24.118884: step 2571, loss 0.0876626, acc 0.96875, learning_rate 0.000409798\n","2023-06-28T22:24:24.232841: step 2572, loss 0.160039, acc 0.90625, learning_rate 0.000409493\n","2023-06-28T22:24:24.333732: step 2573, loss 0.216869, acc 0.90625, learning_rate 0.000409189\n","2023-06-28T22:24:24.432326: step 2574, loss 0.0972976, acc 0.9375, learning_rate 0.000408884\n","2023-06-28T22:24:24.546726: step 2575, loss 0.310603, acc 0.90625, learning_rate 0.00040858\n","2023-06-28T22:24:24.645676: step 2576, loss 0.0511965, acc 0.96875, learning_rate 0.000408276\n","2023-06-28T22:24:24.739954: step 2577, loss 0.37995, acc 0.90625, learning_rate 0.000407973\n","2023-06-28T22:24:24.846188: step 2578, loss 0.490092, acc 0.875, learning_rate 0.000407669\n","2023-06-28T22:24:24.942277: step 2579, loss 0.226847, acc 0.9375, learning_rate 0.000407366\n","2023-06-28T22:24:25.047150: step 2580, loss 0.0930054, acc 0.96875, learning_rate 0.000407064\n","2023-06-28T22:24:25.146781: step 2581, loss 0.273776, acc 0.90625, learning_rate 0.000406761\n","2023-06-28T22:24:25.262335: step 2582, loss 0.324605, acc 0.9375, learning_rate 0.000406459\n","2023-06-28T22:24:25.362823: step 2583, loss 0.0982208, acc 0.96875, learning_rate 0.000406158\n","2023-06-28T22:24:25.463378: step 2584, loss 0.445882, acc 0.84375, learning_rate 0.000405856\n","2023-06-28T22:24:25.564216: step 2585, loss 0.064015, acc 1, learning_rate 0.000405555\n","2023-06-28T22:24:25.663446: step 2586, loss 0.0876182, acc 0.96875, learning_rate 0.000405254\n","2023-06-28T22:24:25.755839: step 2587, loss 0.242828, acc 0.90625, learning_rate 0.000404954\n","2023-06-28T22:24:25.849416: step 2588, loss 0.229085, acc 0.90625, learning_rate 0.000404653\n","2023-06-28T22:24:25.954158: step 2589, loss 0.155765, acc 0.875, learning_rate 0.000404353\n","2023-06-28T22:24:26.051218: step 2590, loss 0.0683584, acc 0.96875, learning_rate 0.000404054\n","2023-06-28T22:24:26.151919: step 2591, loss 0.0366302, acc 0.96875, learning_rate 0.000403754\n","2023-06-28T22:24:26.262724: step 2592, loss 0.135854, acc 0.9375, learning_rate 0.000403455\n","2023-06-28T22:24:26.365794: step 2593, loss 0.170755, acc 0.9375, learning_rate 0.000403156\n","2023-06-28T22:24:26.463416: step 2594, loss 0.106849, acc 0.9375, learning_rate 0.000402858\n","2023-06-28T22:24:26.561318: step 2595, loss 0.153556, acc 0.9375, learning_rate 0.00040256\n","2023-06-28T22:24:26.663364: step 2596, loss 0.0455422, acc 1, learning_rate 0.000402262\n","2023-06-28T22:24:26.761575: step 2597, loss 0.231047, acc 0.84375, learning_rate 0.000401964\n","2023-06-28T22:24:26.855810: step 2598, loss 0.0978235, acc 1, learning_rate 0.000401667\n","2023-06-28T22:24:26.953826: step 2599, loss 0.0826994, acc 1, learning_rate 0.00040137\n","\n","Evaluation:\n","2023-06-28T22:24:27.676489: step 2600, loss 0.823586, acc 0.833436\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2600\n","\n","2023-06-28T22:24:27.893418: step 2600, loss 0.165142, acc 0.96875, learning_rate 0.000401073\n","2023-06-28T22:24:27.997590: step 2601, loss 0.0652089, acc 0.96875, learning_rate 0.000400777\n","2023-06-28T22:24:28.100873: step 2602, loss 0.19438, acc 0.90625, learning_rate 0.00040048\n","2023-06-28T22:24:28.195927: step 2603, loss 0.155692, acc 0.96875, learning_rate 0.000400184\n","2023-06-28T22:24:28.313700: step 2604, loss 0.138056, acc 0.90625, learning_rate 0.000399889\n","2023-06-28T22:24:28.421060: step 2605, loss 0.0341838, acc 1, learning_rate 0.000399594\n","2023-06-28T22:24:28.521640: step 2606, loss 0.0362878, acc 1, learning_rate 0.000399299\n","2023-06-28T22:24:28.621745: step 2607, loss 0.215378, acc 0.96875, learning_rate 0.000399004\n","2023-06-28T22:24:28.718914: step 2608, loss 0.0247316, acc 1, learning_rate 0.000398709\n","2023-06-28T22:24:28.821335: step 2609, loss 0.196352, acc 0.90625, learning_rate 0.000398415\n","2023-06-28T22:24:28.919580: step 2610, loss 0.144606, acc 0.9375, learning_rate 0.000398122\n","2023-06-28T22:24:29.015235: step 2611, loss 0.127621, acc 0.9375, learning_rate 0.000397828\n","2023-06-28T22:24:29.116295: step 2612, loss 0.231833, acc 0.96875, learning_rate 0.000397535\n","2023-06-28T22:24:29.218214: step 2613, loss 0.197557, acc 0.9375, learning_rate 0.000397242\n","2023-06-28T22:24:29.321564: step 2614, loss 0.0849763, acc 0.96875, learning_rate 0.000396949\n","2023-06-28T22:24:29.432820: step 2615, loss 0.101185, acc 0.96875, learning_rate 0.000396657\n","2023-06-28T22:24:29.533503: step 2616, loss 0.189523, acc 0.9375, learning_rate 0.000396365\n","2023-06-28T22:24:29.634810: step 2617, loss 0.0510171, acc 1, learning_rate 0.000396073\n","2023-06-28T22:24:29.730695: step 2618, loss 0.126636, acc 0.9375, learning_rate 0.000395781\n","2023-06-28T22:24:29.826621: step 2619, loss 0.27129, acc 0.875, learning_rate 0.00039549\n","2023-06-28T22:24:29.929727: step 2620, loss 0.118959, acc 0.9375, learning_rate 0.000395199\n","2023-06-28T22:24:30.030581: step 2621, loss 0.115952, acc 0.9375, learning_rate 0.000394908\n","2023-06-28T22:24:30.137018: step 2622, loss 0.205877, acc 0.9375, learning_rate 0.000394618\n","2023-06-28T22:24:30.238439: step 2623, loss 0.103345, acc 0.96875, learning_rate 0.000394328\n","2023-06-28T22:24:30.334934: step 2624, loss 0.232711, acc 0.9375, learning_rate 0.000394038\n","2023-06-28T22:24:30.443055: step 2625, loss 0.0562336, acc 1, learning_rate 0.000393749\n","2023-06-28T22:24:30.541832: step 2626, loss 0.105358, acc 0.96875, learning_rate 0.000393459\n","2023-06-28T22:24:30.637110: step 2627, loss 0.395698, acc 0.90625, learning_rate 0.00039317\n","2023-06-28T22:24:30.742282: step 2628, loss 0.198842, acc 0.9375, learning_rate 0.000392882\n","2023-06-28T22:24:30.836306: step 2629, loss 0.20204, acc 0.90625, learning_rate 0.000392593\n","2023-06-28T22:24:30.931668: step 2630, loss 0.142173, acc 0.9375, learning_rate 0.000392305\n","2023-06-28T22:24:31.078761: step 2631, loss 0.236523, acc 0.90625, learning_rate 0.000392017\n","2023-06-28T22:24:31.244120: step 2632, loss 0.301499, acc 0.90625, learning_rate 0.00039173\n","2023-06-28T22:24:31.412949: step 2633, loss 0.274588, acc 0.90625, learning_rate 0.000391443\n","2023-06-28T22:24:31.588337: step 2634, loss 0.229741, acc 0.90625, learning_rate 0.000391156\n","2023-06-28T22:24:31.753962: step 2635, loss 0.191682, acc 0.9375, learning_rate 0.000390869\n","2023-06-28T22:24:31.936766: step 2636, loss 0.164728, acc 0.96875, learning_rate 0.000390582\n","2023-06-28T22:24:32.108175: step 2637, loss 0.0885191, acc 0.9375, learning_rate 0.000390296\n","2023-06-28T22:24:32.294711: step 2638, loss 0.121907, acc 1, learning_rate 0.00039001\n","2023-06-28T22:24:32.478221: step 2639, loss 0.240992, acc 0.90625, learning_rate 0.000389725\n","2023-06-28T22:24:32.665536: step 2640, loss 0.220568, acc 0.90625, learning_rate 0.00038944\n","2023-06-28T22:24:32.818414: step 2641, loss 0.174644, acc 0.90625, learning_rate 0.000389155\n","2023-06-28T22:24:32.996427: step 2642, loss 0.265421, acc 0.875, learning_rate 0.00038887\n","2023-06-28T22:24:33.173890: step 2643, loss 0.219385, acc 0.9375, learning_rate 0.000388585\n","2023-06-28T22:24:33.336545: step 2644, loss 0.0420434, acc 0.96875, learning_rate 0.000388301\n","2023-06-28T22:24:33.510655: step 2645, loss 0.203766, acc 0.90625, learning_rate 0.000388017\n","2023-06-28T22:24:33.684757: step 2646, loss 0.0912287, acc 0.9375, learning_rate 0.000387734\n","2023-06-28T22:24:33.850516: step 2647, loss 0.191028, acc 0.9375, learning_rate 0.000387451\n","2023-06-28T22:24:34.010349: step 2648, loss 0.270315, acc 0.875, learning_rate 0.000387167\n","2023-06-28T22:24:34.185478: step 2649, loss 0.2879, acc 0.90625, learning_rate 0.000386885\n","2023-06-28T22:24:34.383267: step 2650, loss 0.127107, acc 0.90625, learning_rate 0.000386602\n","2023-06-28T22:24:34.549692: step 2651, loss 0.243632, acc 0.8125, learning_rate 0.00038632\n","2023-06-28T22:24:34.723040: step 2652, loss 0.238002, acc 0.9375, learning_rate 0.000386038\n","2023-06-28T22:24:34.902663: step 2653, loss 0.0704187, acc 0.96875, learning_rate 0.000385756\n","2023-06-28T22:24:35.076685: step 2654, loss 0.0783883, acc 1, learning_rate 0.000385475\n","2023-06-28T22:24:35.262042: step 2655, loss 0.179823, acc 0.96875, learning_rate 0.000385194\n","2023-06-28T22:24:35.429071: step 2656, loss 0.0590677, acc 1, learning_rate 0.000384913\n","2023-06-28T22:24:35.575431: step 2657, loss 0.11076, acc 0.9375, learning_rate 0.000384633\n","2023-06-28T22:24:35.744210: step 2658, loss 0.0406325, acc 1, learning_rate 0.000384352\n","2023-06-28T22:24:35.904475: step 2659, loss 0.297423, acc 0.9375, learning_rate 0.000384072\n","2023-06-28T22:24:36.065194: step 2660, loss 0.0484393, acc 1, learning_rate 0.000383793\n","2023-06-28T22:24:36.238703: step 2661, loss 0.0789104, acc 0.96875, learning_rate 0.000383513\n","2023-06-28T22:24:36.423005: step 2662, loss 0.144214, acc 0.96875, learning_rate 0.000383234\n","2023-06-28T22:24:36.588814: step 2663, loss 0.145576, acc 0.9375, learning_rate 0.000382955\n","2023-06-28T22:24:36.738142: step 2664, loss 0.0937356, acc 0.9375, learning_rate 0.000382677\n","2023-06-28T22:24:36.896243: step 2665, loss 0.560095, acc 0.84375, learning_rate 0.000382398\n","2023-06-28T22:24:37.057477: step 2666, loss 0.098302, acc 0.96875, learning_rate 0.00038212\n","2023-06-28T22:24:37.213597: step 2667, loss 0.279453, acc 0.875, learning_rate 0.000381842\n","2023-06-28T22:24:37.380724: step 2668, loss 0.162599, acc 0.9375, learning_rate 0.000381565\n","2023-06-28T22:24:37.545127: step 2669, loss 0.106607, acc 0.96875, learning_rate 0.000381288\n","2023-06-28T22:24:37.703572: step 2670, loss 0.0914711, acc 0.96875, learning_rate 0.000381011\n","2023-06-28T22:24:37.856210: step 2671, loss 0.0611603, acc 0.96875, learning_rate 0.000380734\n","2023-06-28T22:24:38.016984: step 2672, loss 0.0394374, acc 1, learning_rate 0.000380457\n","2023-06-28T22:24:38.186831: step 2673, loss 0.0818651, acc 0.96875, learning_rate 0.000380181\n","2023-06-28T22:24:38.354580: step 2674, loss 0.0605684, acc 1, learning_rate 0.000379905\n","2023-06-28T22:24:38.508191: step 2675, loss 0.228313, acc 0.875, learning_rate 0.00037963\n","2023-06-28T22:24:38.649839: step 2676, loss 0.0732269, acc 0.96875, learning_rate 0.000379354\n","2023-06-28T22:24:38.805942: step 2677, loss 0.373109, acc 0.875, learning_rate 0.000379079\n","2023-06-28T22:24:38.966815: step 2678, loss 0.0563975, acc 0.96875, learning_rate 0.000378805\n","2023-06-28T22:24:39.118003: step 2679, loss 0.263869, acc 0.9375, learning_rate 0.00037853\n","2023-06-28T22:24:39.268975: step 2680, loss 0.365647, acc 0.90625, learning_rate 0.000378256\n","2023-06-28T22:24:39.431732: step 2681, loss 0.0785434, acc 0.96875, learning_rate 0.000377982\n","2023-06-28T22:24:39.576027: step 2682, loss 0.101982, acc 0.9375, learning_rate 0.000377708\n","2023-06-28T22:24:39.739553: step 2683, loss 0.398484, acc 0.96875, learning_rate 0.000377435\n","2023-06-28T22:24:39.899113: step 2684, loss 0.773517, acc 0.84375, learning_rate 0.000377162\n","2023-06-28T22:24:40.074597: step 2685, loss 0.163408, acc 0.9375, learning_rate 0.000376889\n","2023-06-28T22:24:40.244460: step 2686, loss 0.45175, acc 0.90625, learning_rate 0.000376616\n","2023-06-28T22:24:40.402958: step 2687, loss 0.233405, acc 0.9375, learning_rate 0.000376344\n","2023-06-28T22:24:40.557120: step 2688, loss 0.176108, acc 0.90625, learning_rate 0.000376071\n","2023-06-28T22:24:40.720716: step 2689, loss 0.283422, acc 0.90625, learning_rate 0.0003758\n","2023-06-28T22:24:40.902774: step 2690, loss 0.393741, acc 0.90625, learning_rate 0.000375528\n","2023-06-28T22:24:41.066212: step 2691, loss 0.260091, acc 0.9375, learning_rate 0.000375257\n","2023-06-28T22:24:41.244685: step 2692, loss 0.197969, acc 0.90625, learning_rate 0.000374986\n","2023-06-28T22:24:41.428479: step 2693, loss 0.0886398, acc 0.96875, learning_rate 0.000374715\n","2023-06-28T22:24:41.590007: step 2694, loss 0.0573485, acc 0.96875, learning_rate 0.000374444\n","2023-06-28T22:24:41.754679: step 2695, loss 0.06611, acc 0.96875, learning_rate 0.000374174\n","2023-06-28T22:24:41.867200: step 2696, loss 0.185931, acc 0.9375, learning_rate 0.000373904\n","2023-06-28T22:24:41.962213: step 2697, loss 0.140999, acc 0.9375, learning_rate 0.000373635\n","2023-06-28T22:24:42.080213: step 2698, loss 0.279872, acc 0.90625, learning_rate 0.000373365\n","2023-06-28T22:24:42.174724: step 2699, loss 0.225148, acc 0.875, learning_rate 0.000373096\n","\n","Evaluation:\n","2023-06-28T22:24:42.904633: step 2700, loss 0.99961, acc 0.83282\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2700\n","\n","2023-06-28T22:24:43.098041: step 2700, loss 0.0304775, acc 1, learning_rate 0.000372827\n","2023-06-28T22:24:43.209034: step 2701, loss 0.221162, acc 0.9375, learning_rate 0.000372558\n","2023-06-28T22:24:43.306685: step 2702, loss 0.0103987, acc 1, learning_rate 0.00037229\n","2023-06-28T22:24:43.404786: step 2703, loss 0.302632, acc 0.90625, learning_rate 0.000372022\n","2023-06-28T22:24:43.505071: step 2704, loss 0.227419, acc 0.9375, learning_rate 0.000371754\n","2023-06-28T22:24:43.624517: step 2705, loss 1.02889, acc 0.875, learning_rate 0.000371486\n","2023-06-28T22:24:43.721960: step 2706, loss 0.324071, acc 0.84375, learning_rate 0.000371219\n","2023-06-28T22:24:43.825940: step 2707, loss 0.463066, acc 0.9375, learning_rate 0.000370952\n","2023-06-28T22:24:43.929858: step 2708, loss 0.0786101, acc 0.9375, learning_rate 0.000370685\n","2023-06-28T22:24:44.031254: step 2709, loss 0.288873, acc 0.96875, learning_rate 0.000370419\n","2023-06-28T22:24:44.132698: step 2710, loss 0.109528, acc 0.96875, learning_rate 0.000370152\n","2023-06-28T22:24:44.252778: step 2711, loss 0.113448, acc 0.96875, learning_rate 0.000369886\n","2023-06-28T22:24:44.359272: step 2712, loss 0.304658, acc 0.84375, learning_rate 0.000369621\n","2023-06-28T22:24:44.458716: step 2713, loss 0.165572, acc 0.9375, learning_rate 0.000369355\n","2023-06-28T22:24:44.562946: step 2714, loss 0.0735501, acc 0.96875, learning_rate 0.00036909\n","2023-06-28T22:24:44.659162: step 2715, loss 0.0604801, acc 0.96875, learning_rate 0.000368825\n","2023-06-28T22:24:44.758091: step 2716, loss 0.0807147, acc 0.9375, learning_rate 0.00036856\n","2023-06-28T22:24:44.857673: step 2717, loss 0.123042, acc 0.96875, learning_rate 0.000368296\n","2023-06-28T22:24:44.960469: step 2718, loss 0.177278, acc 0.9375, learning_rate 0.000368032\n","2023-06-28T22:24:45.065993: step 2719, loss 0.145865, acc 0.9375, learning_rate 0.000367768\n","2023-06-28T22:24:45.171562: step 2720, loss 0.246503, acc 0.875, learning_rate 0.000367504\n","2023-06-28T22:24:45.268521: step 2721, loss 0.747703, acc 0.9375, learning_rate 0.000367241\n","2023-06-28T22:24:45.364402: step 2722, loss 0.313619, acc 0.8125, learning_rate 0.000366978\n","2023-06-28T22:24:45.466127: step 2723, loss 0.113658, acc 0.9375, learning_rate 0.000366715\n","2023-06-28T22:24:45.563858: step 2724, loss 0.0944788, acc 0.96875, learning_rate 0.000366452\n","2023-06-28T22:24:45.664249: step 2725, loss 0.22355, acc 0.9375, learning_rate 0.00036619\n","2023-06-28T22:24:45.764255: step 2726, loss 0.121427, acc 0.9375, learning_rate 0.000365928\n","2023-06-28T22:24:45.868279: step 2727, loss 0.464223, acc 0.90625, learning_rate 0.000365666\n","2023-06-28T22:24:45.980673: step 2728, loss 0.163169, acc 0.96875, learning_rate 0.000365404\n","2023-06-28T22:24:46.080513: step 2729, loss 0.173036, acc 0.9375, learning_rate 0.000365143\n","2023-06-28T22:24:46.189318: step 2730, loss 0.283161, acc 0.875, learning_rate 0.000364882\n","2023-06-28T22:24:46.288762: step 2731, loss 0.171054, acc 0.96875, learning_rate 0.000364621\n","2023-06-28T22:24:46.394976: step 2732, loss 0.143783, acc 0.96875, learning_rate 0.00036436\n","2023-06-28T22:24:46.502406: step 2733, loss 0.0159842, acc 1, learning_rate 0.0003641\n","2023-06-28T22:24:46.599288: step 2734, loss 0.078584, acc 0.96875, learning_rate 0.00036384\n","2023-06-28T22:24:46.704509: step 2735, loss 0.152938, acc 0.96875, learning_rate 0.00036358\n","2023-06-28T22:24:46.801937: step 2736, loss 0.111971, acc 0.90625, learning_rate 0.000363321\n","2023-06-28T22:24:46.902871: step 2737, loss 0.327243, acc 0.875, learning_rate 0.000363061\n","2023-06-28T22:24:47.007501: step 2738, loss 0.0945155, acc 0.9375, learning_rate 0.000362802\n","2023-06-28T22:24:47.112779: step 2739, loss 0.140959, acc 0.9375, learning_rate 0.000362544\n","2023-06-28T22:24:47.218919: step 2740, loss 0.224214, acc 0.9375, learning_rate 0.000362285\n","2023-06-28T22:24:47.319730: step 2741, loss 0.203146, acc 0.9375, learning_rate 0.000362027\n","2023-06-28T22:24:47.438132: step 2742, loss 0.264283, acc 0.875, learning_rate 0.000361769\n","2023-06-28T22:24:47.537070: step 2743, loss 0.064099, acc 1, learning_rate 0.000361511\n","2023-06-28T22:24:47.632739: step 2744, loss 0.254367, acc 0.9375, learning_rate 0.000361254\n","2023-06-28T22:24:47.730170: step 2745, loss 0.205688, acc 0.90625, learning_rate 0.000360996\n","2023-06-28T22:24:47.837946: step 2746, loss 0.210687, acc 0.90625, learning_rate 0.000360739\n","2023-06-28T22:24:47.933786: step 2747, loss 0.138246, acc 0.9375, learning_rate 0.000360483\n","2023-06-28T22:24:48.039653: step 2748, loss 0.0982069, acc 0.9375, learning_rate 0.000360226\n","2023-06-28T22:24:48.145461: step 2749, loss 0.320783, acc 0.84375, learning_rate 0.00035997\n","2023-06-28T22:24:48.245590: step 2750, loss 0.298363, acc 0.9375, learning_rate 0.000359714\n","2023-06-28T22:24:48.340711: step 2751, loss 0.115371, acc 0.90625, learning_rate 0.000359458\n","2023-06-28T22:24:48.446612: step 2752, loss 0.330074, acc 0.90625, learning_rate 0.000359203\n","2023-06-28T22:24:48.545303: step 2753, loss 0.206016, acc 0.9375, learning_rate 0.000358947\n","2023-06-28T22:24:48.637825: step 2754, loss 0.122091, acc 0.9375, learning_rate 0.000358693\n","2023-06-28T22:24:48.738039: step 2755, loss 0.307817, acc 0.90625, learning_rate 0.000358438\n","2023-06-28T22:24:48.830304: step 2756, loss 0.0885013, acc 0.96875, learning_rate 0.000358183\n","2023-06-28T22:24:48.922633: step 2757, loss 0.290426, acc 0.875, learning_rate 0.000357929\n","2023-06-28T22:24:49.029675: step 2758, loss 0.280481, acc 0.84375, learning_rate 0.000357675\n","2023-06-28T22:24:49.130309: step 2759, loss 0.420628, acc 0.75, learning_rate 0.000357421\n","2023-06-28T22:24:49.222460: step 2760, loss 0.502671, acc 0.84375, learning_rate 0.000357168\n","2023-06-28T22:24:49.350898: step 2761, loss 0.152552, acc 0.9375, learning_rate 0.000356915\n","2023-06-28T22:24:49.449164: step 2762, loss 0.346821, acc 0.875, learning_rate 0.000356662\n","2023-06-28T22:24:49.545475: step 2763, loss 0.0901438, acc 0.96875, learning_rate 0.000356409\n","2023-06-28T22:24:49.647479: step 2764, loss 0.816409, acc 0.8125, learning_rate 0.000356157\n","2023-06-28T22:24:49.762962: step 2765, loss 0.202022, acc 0.90625, learning_rate 0.000355904\n","2023-06-28T22:24:49.858429: step 2766, loss 0.199209, acc 0.90625, learning_rate 0.000355652\n","2023-06-28T22:24:49.958719: step 2767, loss 0.10865, acc 0.9375, learning_rate 0.000355401\n","2023-06-28T22:24:50.061657: step 2768, loss 0.49097, acc 0.875, learning_rate 0.000355149\n","2023-06-28T22:24:50.162363: step 2769, loss 0.134435, acc 0.96875, learning_rate 0.000354898\n","2023-06-28T22:24:50.257086: step 2770, loss 0.33429, acc 0.875, learning_rate 0.000354647\n","2023-06-28T22:24:50.357298: step 2771, loss 0.437118, acc 0.90625, learning_rate 0.000354396\n","2023-06-28T22:24:50.455771: step 2772, loss 0.594253, acc 0.875, learning_rate 0.000354146\n","2023-06-28T22:24:50.554443: step 2773, loss 0.279917, acc 0.84375, learning_rate 0.000353895\n","2023-06-28T22:24:50.658743: step 2774, loss 0.0721394, acc 0.96875, learning_rate 0.000353645\n","2023-06-28T22:24:50.752319: step 2775, loss 0.137119, acc 0.9375, learning_rate 0.000353396\n","2023-06-28T22:24:50.848134: step 2776, loss 0.157417, acc 0.9375, learning_rate 0.000353146\n","2023-06-28T22:24:50.951977: step 2777, loss 0.304326, acc 0.90625, learning_rate 0.000352897\n","2023-06-28T22:24:51.051772: step 2778, loss 0.377882, acc 0.90625, learning_rate 0.000352648\n","2023-06-28T22:24:51.157443: step 2779, loss 0.237093, acc 0.90625, learning_rate 0.000352399\n","2023-06-28T22:24:51.253627: step 2780, loss 0.170199, acc 0.90625, learning_rate 0.000352151\n","2023-06-28T22:24:51.349366: step 2781, loss 0.207773, acc 0.9375, learning_rate 0.000351902\n","2023-06-28T22:24:51.458457: step 2782, loss 0.218291, acc 0.90625, learning_rate 0.000351654\n","2023-06-28T22:24:51.559768: step 2783, loss 0.302317, acc 0.96875, learning_rate 0.000351406\n","2023-06-28T22:24:51.657385: step 2784, loss 0.0439273, acc 1, learning_rate 0.000351159\n","2023-06-28T22:24:51.757026: step 2785, loss 0.411965, acc 0.90625, learning_rate 0.000350912\n","2023-06-28T22:24:51.875717: step 2786, loss 0.187176, acc 0.90625, learning_rate 0.000350665\n","2023-06-28T22:24:52.041505: step 2787, loss 0.115303, acc 0.90625, learning_rate 0.000350418\n","2023-06-28T22:24:52.203552: step 2788, loss 0.0720498, acc 0.96875, learning_rate 0.000350171\n","2023-06-28T22:24:52.380702: step 2789, loss 0.0699137, acc 1, learning_rate 0.000349925\n","2023-06-28T22:24:52.562299: step 2790, loss 0.270744, acc 0.875, learning_rate 0.000349679\n","2023-06-28T22:24:52.741192: step 2791, loss 0.231502, acc 0.96875, learning_rate 0.000349433\n","2023-06-28T22:24:52.913124: step 2792, loss 0.0809209, acc 0.96875, learning_rate 0.000349187\n","2023-06-28T22:24:53.086684: step 2793, loss 0.0416648, acc 0.96875, learning_rate 0.000348942\n","2023-06-28T22:24:53.262008: step 2794, loss 0.0788836, acc 0.9375, learning_rate 0.000348697\n","2023-06-28T22:24:53.430305: step 2795, loss 0.154752, acc 0.9375, learning_rate 0.000348452\n","2023-06-28T22:24:53.624672: step 2796, loss 0.12731, acc 0.96875, learning_rate 0.000348207\n","2023-06-28T22:24:53.792448: step 2797, loss 0.0422508, acc 0.96875, learning_rate 0.000347963\n","2023-06-28T22:24:53.951361: step 2798, loss 0.219316, acc 0.9375, learning_rate 0.000347719\n","2023-06-28T22:24:54.123031: step 2799, loss 0.132633, acc 0.96875, learning_rate 0.000347475\n","\n","Evaluation:\n","2023-06-28T22:24:55.465801: step 2800, loss 0.895799, acc 0.840209\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2800\n","\n","2023-06-28T22:24:55.784632: step 2800, loss 0.104201, acc 0.9375, learning_rate 0.000347231\n","2023-06-28T22:24:55.946095: step 2801, loss 0.121802, acc 0.96875, learning_rate 0.000346988\n","2023-06-28T22:24:56.123367: step 2802, loss 0.133236, acc 0.9375, learning_rate 0.000346744\n","2023-06-28T22:24:56.295218: step 2803, loss 0.0511256, acc 1, learning_rate 0.000346501\n","2023-06-28T22:24:56.439020: step 2804, loss 0.609354, acc 0.78125, learning_rate 0.000346259\n","2023-06-28T22:24:56.585272: step 2805, loss 0.0261209, acc 1, learning_rate 0.000346016\n","2023-06-28T22:24:56.729684: step 2806, loss 0.285914, acc 0.875, learning_rate 0.000345774\n","2023-06-28T22:24:56.899190: step 2807, loss 0.59771, acc 0.90625, learning_rate 0.000345532\n","2023-06-28T22:24:57.062967: step 2808, loss 0.138614, acc 0.9375, learning_rate 0.00034529\n","2023-06-28T22:24:57.231886: step 2809, loss 0.238213, acc 0.9375, learning_rate 0.000345049\n","2023-06-28T22:24:57.395210: step 2810, loss 0.124803, acc 0.9375, learning_rate 0.000344807\n","2023-06-28T22:24:57.554063: step 2811, loss 0.21987, acc 0.90625, learning_rate 0.000344566\n","2023-06-28T22:24:57.705743: step 2812, loss 0.240638, acc 0.875, learning_rate 0.000344326\n","2023-06-28T22:24:57.861993: step 2813, loss 0.205345, acc 0.90625, learning_rate 0.000344085\n","2023-06-28T22:24:58.024594: step 2814, loss 0.209421, acc 0.9375, learning_rate 0.000343845\n","2023-06-28T22:24:58.202724: step 2815, loss 0.219069, acc 0.90625, learning_rate 0.000343605\n","2023-06-28T22:24:58.371456: step 2816, loss 0.0902938, acc 0.96875, learning_rate 0.000343365\n","2023-06-28T22:24:58.547674: step 2817, loss 0.168331, acc 0.9375, learning_rate 0.000343125\n","2023-06-28T22:24:58.706113: step 2818, loss 0.267602, acc 0.875, learning_rate 0.000342886\n","2023-06-28T22:24:58.867545: step 2819, loss 0.270508, acc 0.96875, learning_rate 0.000342647\n","2023-06-28T22:24:59.048713: step 2820, loss 0.200852, acc 0.96875, learning_rate 0.000342408\n","2023-06-28T22:24:59.206951: step 2821, loss 0.13808, acc 0.96875, learning_rate 0.000342169\n","2023-06-28T22:24:59.378215: step 2822, loss 0.39537, acc 0.8125, learning_rate 0.00034193\n","2023-06-28T22:24:59.544955: step 2823, loss 0.0427757, acc 1, learning_rate 0.000341692\n","2023-06-28T22:24:59.690563: step 2824, loss 0.10245, acc 0.9375, learning_rate 0.000341454\n","2023-06-28T22:24:59.859222: step 2825, loss 0.058567, acc 1, learning_rate 0.000341216\n","2023-06-28T22:25:00.025556: step 2826, loss 0.0571806, acc 0.96875, learning_rate 0.000340979\n","2023-06-28T22:25:00.207368: step 2827, loss 0.0372877, acc 1, learning_rate 0.000340742\n","2023-06-28T22:25:00.362594: step 2828, loss 0.0769107, acc 0.96875, learning_rate 0.000340505\n","2023-06-28T22:25:00.516814: step 2829, loss 0.0698787, acc 1, learning_rate 0.000340268\n","2023-06-28T22:25:00.680737: step 2830, loss 0.344091, acc 0.90625, learning_rate 0.000340031\n","2023-06-28T22:25:00.842283: step 2831, loss 0.0513145, acc 0.96875, learning_rate 0.000339795\n","2023-06-28T22:25:01.010651: step 2832, loss 0.0561357, acc 0.96875, learning_rate 0.000339559\n","2023-06-28T22:25:01.179066: step 2833, loss 0.0985108, acc 0.9375, learning_rate 0.000339323\n","2023-06-28T22:25:01.328459: step 2834, loss 0.166331, acc 0.875, learning_rate 0.000339087\n","2023-06-28T22:25:01.498944: step 2835, loss 0.637881, acc 0.8125, learning_rate 0.000338852\n","2023-06-28T22:25:01.664917: step 2836, loss 1.16785, acc 0.84375, learning_rate 0.000338617\n","2023-06-28T22:25:01.800979: step 2837, loss 0.137959, acc 0.96875, learning_rate 0.000338382\n","2023-06-28T22:25:01.936345: step 2838, loss 0.236736, acc 0.90625, learning_rate 0.000338147\n","2023-06-28T22:25:02.113677: step 2839, loss 0.064128, acc 0.96875, learning_rate 0.000337912\n","2023-06-28T22:25:02.288819: step 2840, loss 0.421258, acc 0.875, learning_rate 0.000337678\n","2023-06-28T22:25:02.441045: step 2841, loss 0.124742, acc 1, learning_rate 0.000337444\n","2023-06-28T22:25:02.603448: step 2842, loss 0.151064, acc 0.90625, learning_rate 0.00033721\n","2023-06-28T22:25:02.714259: step 2843, loss 0.462023, acc 0.9375, learning_rate 0.000336977\n","2023-06-28T22:25:02.821430: step 2844, loss 0.171512, acc 0.9375, learning_rate 0.000336743\n","2023-06-28T22:25:02.921172: step 2845, loss 0.186889, acc 0.90625, learning_rate 0.00033651\n","2023-06-28T22:25:03.019396: step 2846, loss 0.455063, acc 0.84375, learning_rate 0.000336277\n","2023-06-28T22:25:03.121001: step 2847, loss 0.206706, acc 0.875, learning_rate 0.000336045\n","2023-06-28T22:25:03.183199: step 2848, loss 0.0964022, acc 1, learning_rate 0.000335812\n","2023-06-28T22:25:03.286771: step 2849, loss 0.143546, acc 0.96875, learning_rate 0.00033558\n","2023-06-28T22:25:03.387015: step 2850, loss 0.127763, acc 0.9375, learning_rate 0.000335348\n","2023-06-28T22:25:03.489658: step 2851, loss 0.271297, acc 0.90625, learning_rate 0.000335116\n","2023-06-28T22:25:03.596132: step 2852, loss 0.187118, acc 0.90625, learning_rate 0.000334885\n","2023-06-28T22:25:03.690883: step 2853, loss 0.239177, acc 0.875, learning_rate 0.000334654\n","2023-06-28T22:25:03.793237: step 2854, loss 0.121172, acc 0.96875, learning_rate 0.000334423\n","2023-06-28T22:25:03.902417: step 2855, loss 0.00667693, acc 1, learning_rate 0.000334192\n","2023-06-28T22:25:03.999513: step 2856, loss 0.0629085, acc 1, learning_rate 0.000333961\n","2023-06-28T22:25:04.120002: step 2857, loss 0.490819, acc 0.9375, learning_rate 0.000333731\n","2023-06-28T22:25:04.227019: step 2858, loss 0.0924371, acc 0.96875, learning_rate 0.000333501\n","2023-06-28T22:25:04.340466: step 2859, loss 0.0513554, acc 1, learning_rate 0.000333271\n","2023-06-28T22:25:04.456230: step 2860, loss 0.133974, acc 0.9375, learning_rate 0.000333041\n","2023-06-28T22:25:04.551098: step 2861, loss 0.203897, acc 0.9375, learning_rate 0.000332812\n","2023-06-28T22:25:04.666262: step 2862, loss 0.104325, acc 0.96875, learning_rate 0.000332582\n","2023-06-28T22:25:04.767155: step 2863, loss 0.0832711, acc 0.96875, learning_rate 0.000332353\n","2023-06-28T22:25:04.867839: step 2864, loss 0.159425, acc 0.90625, learning_rate 0.000332125\n","2023-06-28T22:25:04.980148: step 2865, loss 0.258566, acc 0.9375, learning_rate 0.000331896\n","2023-06-28T22:25:05.082485: step 2866, loss 0.277568, acc 0.90625, learning_rate 0.000331668\n","2023-06-28T22:25:05.180152: step 2867, loss 0.167692, acc 0.96875, learning_rate 0.00033144\n","2023-06-28T22:25:05.272542: step 2868, loss 0.0205893, acc 1, learning_rate 0.000331212\n","2023-06-28T22:25:05.386470: step 2869, loss 0.383155, acc 0.875, learning_rate 0.000330984\n","2023-06-28T22:25:05.483278: step 2870, loss 0.0683647, acc 0.96875, learning_rate 0.000330757\n","2023-06-28T22:25:05.579586: step 2871, loss 0.172255, acc 0.96875, learning_rate 0.000330529\n","2023-06-28T22:25:05.690921: step 2872, loss 0.17533, acc 0.90625, learning_rate 0.000330302\n","2023-06-28T22:25:05.784880: step 2873, loss 0.165791, acc 0.90625, learning_rate 0.000330076\n","2023-06-28T22:25:05.887841: step 2874, loss 0.172442, acc 0.9375, learning_rate 0.000329849\n","2023-06-28T22:25:05.993215: step 2875, loss 0.202616, acc 0.90625, learning_rate 0.000329623\n","2023-06-28T22:25:06.089296: step 2876, loss 0.0740117, acc 0.96875, learning_rate 0.000329397\n","2023-06-28T22:25:06.188809: step 2877, loss 0.155906, acc 0.9375, learning_rate 0.000329171\n","2023-06-28T22:25:06.295513: step 2878, loss 0.0399584, acc 1, learning_rate 0.000328945\n","2023-06-28T22:25:06.391625: step 2879, loss 0.19321, acc 0.90625, learning_rate 0.00032872\n","2023-06-28T22:25:06.492529: step 2880, loss 0.887048, acc 0.90625, learning_rate 0.000328494\n","2023-06-28T22:25:06.597906: step 2881, loss 0.0718557, acc 1, learning_rate 0.000328269\n","2023-06-28T22:25:06.703507: step 2882, loss 0.114166, acc 0.9375, learning_rate 0.000328045\n","2023-06-28T22:25:06.799314: step 2883, loss 0.144237, acc 0.9375, learning_rate 0.00032782\n","2023-06-28T22:25:06.905434: step 2884, loss 0.0949301, acc 0.96875, learning_rate 0.000327596\n","2023-06-28T22:25:07.017405: step 2885, loss 0.0461899, acc 1, learning_rate 0.000327372\n","2023-06-28T22:25:07.117560: step 2886, loss 0.135488, acc 0.9375, learning_rate 0.000327148\n","2023-06-28T22:25:07.219592: step 2887, loss 0.0411209, acc 1, learning_rate 0.000326924\n","2023-06-28T22:25:07.323264: step 2888, loss 0.102599, acc 0.96875, learning_rate 0.000326701\n","2023-06-28T22:25:07.425615: step 2889, loss 0.279109, acc 0.96875, learning_rate 0.000326477\n","2023-06-28T22:25:07.525382: step 2890, loss 0.0626127, acc 1, learning_rate 0.000326254\n","2023-06-28T22:25:07.631289: step 2891, loss 0.0948418, acc 0.96875, learning_rate 0.000326032\n","2023-06-28T22:25:07.734915: step 2892, loss 0.154671, acc 0.9375, learning_rate 0.000325809\n","2023-06-28T22:25:07.831812: step 2893, loss 0.317682, acc 0.84375, learning_rate 0.000325587\n","2023-06-28T22:25:07.928821: step 2894, loss 0.191708, acc 0.90625, learning_rate 0.000325365\n","2023-06-28T22:25:08.025657: step 2895, loss 0.296742, acc 0.90625, learning_rate 0.000325143\n","2023-06-28T22:25:08.133924: step 2896, loss 0.113343, acc 0.9375, learning_rate 0.000324921\n","2023-06-28T22:25:08.236215: step 2897, loss 0.105291, acc 1, learning_rate 0.0003247\n","2023-06-28T22:25:08.331881: step 2898, loss 0.16685, acc 0.9375, learning_rate 0.000324478\n","2023-06-28T22:25:08.433105: step 2899, loss 0.00442783, acc 1, learning_rate 0.000324257\n","\n","Evaluation:\n","2023-06-28T22:25:09.140835: step 2900, loss 0.882859, acc 0.839286\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-2900\n","\n","2023-06-28T22:25:09.341116: step 2900, loss 0.054599, acc 0.96875, learning_rate 0.000324036\n","2023-06-28T22:25:09.444853: step 2901, loss 0.185453, acc 0.9375, learning_rate 0.000323816\n","2023-06-28T22:25:09.541319: step 2902, loss 0.12689, acc 0.9375, learning_rate 0.000323595\n","2023-06-28T22:25:09.651081: step 2903, loss 0.303764, acc 0.875, learning_rate 0.000323375\n","2023-06-28T22:25:09.756989: step 2904, loss 0.1978, acc 0.9375, learning_rate 0.000323155\n","2023-06-28T22:25:09.852490: step 2905, loss 0.0423485, acc 1, learning_rate 0.000322936\n","2023-06-28T22:25:09.960260: step 2906, loss 0.153574, acc 0.90625, learning_rate 0.000322716\n","2023-06-28T22:25:10.064798: step 2907, loss 0.0537177, acc 1, learning_rate 0.000322497\n","2023-06-28T22:25:10.168558: step 2908, loss 0.212875, acc 0.9375, learning_rate 0.000322278\n","2023-06-28T22:25:10.286827: step 2909, loss 0.0873241, acc 0.96875, learning_rate 0.000322059\n","2023-06-28T22:25:10.385069: step 2910, loss 0.447819, acc 0.90625, learning_rate 0.00032184\n","2023-06-28T22:25:10.490794: step 2911, loss 0.226607, acc 0.875, learning_rate 0.000321622\n","2023-06-28T22:25:10.587129: step 2912, loss 0.127414, acc 0.9375, learning_rate 0.000321404\n","2023-06-28T22:25:10.679585: step 2913, loss 0.285611, acc 0.875, learning_rate 0.000321186\n","2023-06-28T22:25:10.785398: step 2914, loss 0.0764837, acc 0.96875, learning_rate 0.000320968\n","2023-06-28T22:25:10.892695: step 2915, loss 0.206018, acc 0.875, learning_rate 0.00032075\n","2023-06-28T22:25:10.989386: step 2916, loss 0.0665413, acc 0.96875, learning_rate 0.000320533\n","2023-06-28T22:25:11.096842: step 2917, loss 0.0360492, acc 1, learning_rate 0.000320316\n","2023-06-28T22:25:11.192493: step 2918, loss 0.13318, acc 0.9375, learning_rate 0.000320099\n","2023-06-28T22:25:11.293669: step 2919, loss 0.150829, acc 0.9375, learning_rate 0.000319882\n","2023-06-28T22:25:11.391051: step 2920, loss 0.0972284, acc 0.96875, learning_rate 0.000319666\n","2023-06-28T22:25:11.489839: step 2921, loss 0.0834178, acc 0.96875, learning_rate 0.000319449\n","2023-06-28T22:25:11.587711: step 2922, loss 0.368884, acc 0.96875, learning_rate 0.000319233\n","2023-06-28T22:25:11.684932: step 2923, loss 0.0522233, acc 0.96875, learning_rate 0.000319017\n","2023-06-28T22:25:11.786842: step 2924, loss 0.0880287, acc 0.9375, learning_rate 0.000318802\n","2023-06-28T22:25:11.891186: step 2925, loss 0.404973, acc 0.90625, learning_rate 0.000318586\n","2023-06-28T22:25:11.986476: step 2926, loss 0.0427238, acc 1, learning_rate 0.000318371\n","2023-06-28T22:25:12.083621: step 2927, loss 0.252208, acc 0.84375, learning_rate 0.000318156\n","2023-06-28T22:25:12.188424: step 2928, loss 0.0701957, acc 0.96875, learning_rate 0.000317941\n","2023-06-28T22:25:12.288766: step 2929, loss 0.212767, acc 0.9375, learning_rate 0.000317726\n","2023-06-28T22:25:12.397239: step 2930, loss 0.242646, acc 0.9375, learning_rate 0.000317512\n","2023-06-28T22:25:12.505098: step 2931, loss 0.0818759, acc 0.96875, learning_rate 0.000317298\n","2023-06-28T22:25:12.603375: step 2932, loss 0.312326, acc 0.96875, learning_rate 0.000317084\n","2023-06-28T22:25:12.767949: step 2933, loss 0.0563296, acc 1, learning_rate 0.00031687\n","2023-06-28T22:25:12.958962: step 2934, loss 0.046044, acc 1, learning_rate 0.000316657\n","2023-06-28T22:25:13.170878: step 2935, loss 0.112398, acc 0.96875, learning_rate 0.000316443\n","2023-06-28T22:25:13.351492: step 2936, loss 0.0732625, acc 0.96875, learning_rate 0.00031623\n","2023-06-28T22:25:13.530114: step 2937, loss 0.0814876, acc 0.9375, learning_rate 0.000316017\n","2023-06-28T22:25:13.676356: step 2938, loss 0.055298, acc 1, learning_rate 0.000315805\n","2023-06-28T22:25:13.843443: step 2939, loss 0.246306, acc 0.9375, learning_rate 0.000315592\n","2023-06-28T22:25:14.010601: step 2940, loss 0.184163, acc 0.90625, learning_rate 0.00031538\n","2023-06-28T22:25:14.193748: step 2941, loss 0.357894, acc 0.84375, learning_rate 0.000315168\n","2023-06-28T22:25:14.416171: step 2942, loss 0.0731023, acc 0.9375, learning_rate 0.000314956\n","2023-06-28T22:25:14.612252: step 2943, loss 0.10818, acc 0.9375, learning_rate 0.000314744\n","2023-06-28T22:25:14.793808: step 2944, loss 0.0804015, acc 0.96875, learning_rate 0.000314533\n","2023-06-28T22:25:14.985871: step 2945, loss 0.421423, acc 0.875, learning_rate 0.000314322\n","2023-06-28T22:25:15.190631: step 2946, loss 0.0801288, acc 0.96875, learning_rate 0.00031411\n","2023-06-28T22:25:15.370667: step 2947, loss 0.333516, acc 0.90625, learning_rate 0.0003139\n","2023-06-28T22:25:15.547235: step 2948, loss 0.086745, acc 0.96875, learning_rate 0.000313689\n","2023-06-28T22:25:15.727601: step 2949, loss 0.144418, acc 0.9375, learning_rate 0.000313479\n","2023-06-28T22:25:15.907672: step 2950, loss 0.234683, acc 0.875, learning_rate 0.000313268\n","2023-06-28T22:25:16.103758: step 2951, loss 0.173992, acc 0.9375, learning_rate 0.000313058\n","2023-06-28T22:25:16.287728: step 2952, loss 0.158799, acc 0.9375, learning_rate 0.000312849\n","2023-06-28T22:25:16.452207: step 2953, loss 0.0450935, acc 0.96875, learning_rate 0.000312639\n","2023-06-28T22:25:16.633303: step 2954, loss 0.441024, acc 0.96875, learning_rate 0.00031243\n","2023-06-28T22:25:16.781800: step 2955, loss 0.186987, acc 0.90625, learning_rate 0.000312221\n","2023-06-28T22:25:16.955709: step 2956, loss 0.241614, acc 0.90625, learning_rate 0.000312012\n","2023-06-28T22:25:17.139054: step 2957, loss 0.262578, acc 0.875, learning_rate 0.000311803\n","2023-06-28T22:25:17.317940: step 2958, loss 0.13154, acc 1, learning_rate 0.000311594\n","2023-06-28T22:25:17.481411: step 2959, loss 0.150552, acc 0.9375, learning_rate 0.000311386\n","2023-06-28T22:25:17.656215: step 2960, loss 0.0736271, acc 0.96875, learning_rate 0.000311178\n","2023-06-28T22:25:17.821612: step 2961, loss 0.0472617, acc 1, learning_rate 0.00031097\n","2023-06-28T22:25:17.988306: step 2962, loss 0.0706902, acc 0.96875, learning_rate 0.000310762\n","2023-06-28T22:25:18.166406: step 2963, loss 0.279352, acc 0.875, learning_rate 0.000310555\n","2023-06-28T22:25:18.338051: step 2964, loss 0.468636, acc 0.90625, learning_rate 0.000310347\n","2023-06-28T22:25:18.526499: step 2965, loss 0.317684, acc 0.96875, learning_rate 0.00031014\n","2023-06-28T22:25:18.697273: step 2966, loss 0.190085, acc 0.90625, learning_rate 0.000309933\n","2023-06-28T22:25:18.867992: step 2967, loss 0.0503942, acc 0.96875, learning_rate 0.000309726\n","2023-06-28T22:25:19.027596: step 2968, loss 0.0850026, acc 0.96875, learning_rate 0.00030952\n","2023-06-28T22:25:19.212498: step 2969, loss 0.149319, acc 0.90625, learning_rate 0.000309314\n","2023-06-28T22:25:19.380246: step 2970, loss 0.292791, acc 0.90625, learning_rate 0.000309108\n","2023-06-28T22:25:19.547628: step 2971, loss 0.0477206, acc 0.96875, learning_rate 0.000308902\n","2023-06-28T22:25:19.709435: step 2972, loss 0.0255524, acc 1, learning_rate 0.000308696\n","2023-06-28T22:25:19.888875: step 2973, loss 0.15698, acc 0.9375, learning_rate 0.00030849\n","2023-06-28T22:25:20.053497: step 2974, loss 0.118651, acc 0.9375, learning_rate 0.000308285\n","2023-06-28T22:25:20.221042: step 2975, loss 0.332107, acc 0.9375, learning_rate 0.00030808\n","2023-06-28T22:25:20.384478: step 2976, loss 0.0822583, acc 0.96875, learning_rate 0.000307875\n","2023-06-28T22:25:20.550727: step 2977, loss 0.0626803, acc 1, learning_rate 0.00030767\n","2023-06-28T22:25:20.718040: step 2978, loss 0.30148, acc 0.84375, learning_rate 0.000307466\n","2023-06-28T22:25:20.878452: step 2979, loss 0.0551909, acc 1, learning_rate 0.000307262\n","2023-06-28T22:25:21.043480: step 2980, loss 0.0290856, acc 1, learning_rate 0.000307058\n","2023-06-28T22:25:21.214614: step 2981, loss 0.104657, acc 0.9375, learning_rate 0.000306854\n","2023-06-28T22:25:21.360636: step 2982, loss 0.630269, acc 0.90625, learning_rate 0.00030665\n","2023-06-28T22:25:21.520798: step 2983, loss 0.154933, acc 0.9375, learning_rate 0.000306447\n","2023-06-28T22:25:21.677793: step 2984, loss 0.159645, acc 0.90625, learning_rate 0.000306243\n","2023-06-28T22:25:21.850357: step 2985, loss 0.0525708, acc 0.96875, learning_rate 0.00030604\n","2023-06-28T22:25:22.017157: step 2986, loss 0.0534488, acc 1, learning_rate 0.000305837\n","2023-06-28T22:25:22.166037: step 2987, loss 0.276568, acc 0.9375, learning_rate 0.000305635\n","2023-06-28T22:25:22.327603: step 2988, loss 0.129172, acc 0.9375, learning_rate 0.000305432\n","2023-06-28T22:25:22.506821: step 2989, loss 0.140598, acc 0.9375, learning_rate 0.00030523\n","2023-06-28T22:25:22.665749: step 2990, loss 0.526973, acc 0.8125, learning_rate 0.000305028\n","2023-06-28T22:25:22.823073: step 2991, loss 0.169699, acc 0.9375, learning_rate 0.000304826\n","2023-06-28T22:25:22.983032: step 2992, loss 0.136619, acc 0.9375, learning_rate 0.000304624\n","2023-06-28T22:25:23.154104: step 2993, loss 0.487186, acc 0.78125, learning_rate 0.000304423\n","2023-06-28T22:25:23.336617: step 2994, loss 0.0853263, acc 0.96875, learning_rate 0.000304221\n","2023-06-28T22:25:23.485844: step 2995, loss 0.125227, acc 0.96875, learning_rate 0.00030402\n","2023-06-28T22:25:23.588422: step 2996, loss 0.157698, acc 0.90625, learning_rate 0.00030382\n","2023-06-28T22:25:23.698049: step 2997, loss 0.289075, acc 0.96875, learning_rate 0.000303619\n","2023-06-28T22:25:23.802763: step 2998, loss 0.252443, acc 0.90625, learning_rate 0.000303418\n","2023-06-28T22:25:23.903460: step 2999, loss 0.138873, acc 0.9375, learning_rate 0.000303218\n","\n","Evaluation:\n","2023-06-28T22:25:24.615896: step 3000, loss 0.939949, acc 0.840209\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3000\n","\n","2023-06-28T22:25:24.813159: step 3000, loss 0.0402225, acc 1, learning_rate 0.000303018\n","2023-06-28T22:25:24.911363: step 3001, loss 0.0428229, acc 1, learning_rate 0.000302818\n","2023-06-28T22:25:25.007103: step 3002, loss 0.0290309, acc 1, learning_rate 0.000302618\n","2023-06-28T22:25:25.114208: step 3003, loss 0.057075, acc 0.96875, learning_rate 0.000302419\n","2023-06-28T22:25:25.212027: step 3004, loss 0.0939186, acc 0.9375, learning_rate 0.000302219\n","2023-06-28T22:25:25.310151: step 3005, loss 0.0710018, acc 0.96875, learning_rate 0.00030202\n","2023-06-28T22:25:25.421442: step 3006, loss 0.1742, acc 0.9375, learning_rate 0.000301821\n","2023-06-28T22:25:25.518634: step 3007, loss 0.107203, acc 0.96875, learning_rate 0.000301623\n","2023-06-28T22:25:25.613603: step 3008, loss 0.130533, acc 0.96875, learning_rate 0.000301424\n","2023-06-28T22:25:25.723860: step 3009, loss 0.174249, acc 0.96875, learning_rate 0.000301226\n","2023-06-28T22:25:25.820832: step 3010, loss 0.155481, acc 0.9375, learning_rate 0.000301028\n","2023-06-28T22:25:25.924805: step 3011, loss 0.137517, acc 0.9375, learning_rate 0.00030083\n","2023-06-28T22:25:26.023871: step 3012, loss 0.206297, acc 0.90625, learning_rate 0.000300632\n","2023-06-28T22:25:26.124110: step 3013, loss 0.258241, acc 0.9375, learning_rate 0.000300434\n","2023-06-28T22:25:26.245170: step 3014, loss 0.381581, acc 0.90625, learning_rate 0.000300237\n","2023-06-28T22:25:26.344003: step 3015, loss 0.177338, acc 0.90625, learning_rate 0.00030004\n","2023-06-28T22:25:26.448015: step 3016, loss 0.115861, acc 0.96875, learning_rate 0.000299843\n","2023-06-28T22:25:26.553198: step 3017, loss 0.188321, acc 0.9375, learning_rate 0.000299646\n","2023-06-28T22:25:26.650388: step 3018, loss 0.0299941, acc 1, learning_rate 0.00029945\n","2023-06-28T22:25:26.759743: step 3019, loss 0.10212, acc 0.9375, learning_rate 0.000299253\n","2023-06-28T22:25:26.864316: step 3020, loss 0.45606, acc 0.9375, learning_rate 0.000299057\n","2023-06-28T22:25:26.963993: step 3021, loss 0.0800829, acc 0.96875, learning_rate 0.000298861\n","2023-06-28T22:25:27.062185: step 3022, loss 0.186576, acc 0.875, learning_rate 0.000298665\n","2023-06-28T22:25:27.164653: step 3023, loss 0.500294, acc 0.90625, learning_rate 0.00029847\n","2023-06-28T22:25:27.258257: step 3024, loss 0.470389, acc 0.9375, learning_rate 0.000298274\n","2023-06-28T22:25:27.354910: step 3025, loss 0.0650591, acc 1, learning_rate 0.000298079\n","2023-06-28T22:25:27.469116: step 3026, loss 0.0417225, acc 1, learning_rate 0.000297884\n","2023-06-28T22:25:27.565920: step 3027, loss 0.0404878, acc 1, learning_rate 0.000297689\n","2023-06-28T22:25:27.661734: step 3028, loss 0.216892, acc 0.9375, learning_rate 0.000297494\n","2023-06-28T22:25:27.766895: step 3029, loss 0.0763888, acc 1, learning_rate 0.0002973\n","2023-06-28T22:25:27.862074: step 3030, loss 0.139479, acc 0.90625, learning_rate 0.000297106\n","2023-06-28T22:25:27.964772: step 3031, loss 0.287126, acc 0.9375, learning_rate 0.000296912\n","2023-06-28T22:25:28.067886: step 3032, loss 0.0976958, acc 0.9375, learning_rate 0.000296718\n","2023-06-28T22:25:28.174568: step 3033, loss 0.149153, acc 0.96875, learning_rate 0.000296524\n","2023-06-28T22:25:28.280995: step 3034, loss 0.157075, acc 0.90625, learning_rate 0.00029633\n","2023-06-28T22:25:28.380111: step 3035, loss 0.122504, acc 0.9375, learning_rate 0.000296137\n","2023-06-28T22:25:28.490028: step 3036, loss 0.0727487, acc 0.96875, learning_rate 0.000295944\n","2023-06-28T22:25:28.593155: step 3037, loss 0.112048, acc 0.96875, learning_rate 0.000295751\n","2023-06-28T22:25:28.687283: step 3038, loss 0.104968, acc 0.96875, learning_rate 0.000295558\n","2023-06-28T22:25:28.784072: step 3039, loss 0.38089, acc 0.84375, learning_rate 0.000295366\n","2023-06-28T22:25:28.886800: step 3040, loss 0.177515, acc 0.9375, learning_rate 0.000295173\n","2023-06-28T22:25:28.982025: step 3041, loss 0.0450756, acc 0.96875, learning_rate 0.000294981\n","2023-06-28T22:25:29.088941: step 3042, loss 0.203814, acc 0.96875, learning_rate 0.000294789\n","2023-06-28T22:25:29.185111: step 3043, loss 0.147886, acc 0.90625, learning_rate 0.000294597\n","2023-06-28T22:25:29.279014: step 3044, loss 0.145853, acc 0.96875, learning_rate 0.000294406\n","2023-06-28T22:25:29.407468: step 3045, loss 0.298719, acc 0.90625, learning_rate 0.000294214\n","2023-06-28T22:25:29.522948: step 3046, loss 0.181747, acc 0.90625, learning_rate 0.000294023\n","2023-06-28T22:25:29.621674: step 3047, loss 0.08373, acc 0.96875, learning_rate 0.000293832\n","2023-06-28T22:25:29.724492: step 3048, loss 0.242376, acc 0.9375, learning_rate 0.000293641\n","2023-06-28T22:25:29.819755: step 3049, loss 0.0738714, acc 0.96875, learning_rate 0.000293451\n","2023-06-28T22:25:29.919539: step 3050, loss 0.548993, acc 0.9375, learning_rate 0.00029326\n","2023-06-28T22:25:30.017007: step 3051, loss 0.089693, acc 0.9375, learning_rate 0.00029307\n","2023-06-28T22:25:30.117782: step 3052, loss 0.195899, acc 0.9375, learning_rate 0.00029288\n","2023-06-28T22:25:30.223756: step 3053, loss 0.190042, acc 0.90625, learning_rate 0.00029269\n","2023-06-28T22:25:30.325212: step 3054, loss 0.239109, acc 0.9375, learning_rate 0.0002925\n","2023-06-28T22:25:30.424924: step 3055, loss 0.354367, acc 0.96875, learning_rate 0.000292311\n","2023-06-28T22:25:30.536083: step 3056, loss 0.127402, acc 0.9375, learning_rate 0.000292121\n","2023-06-28T22:25:30.632991: step 3057, loss 0.273119, acc 0.96875, learning_rate 0.000291932\n","2023-06-28T22:25:30.728214: step 3058, loss 0.120805, acc 0.9375, learning_rate 0.000291743\n","2023-06-28T22:25:30.832959: step 3059, loss 0.0679694, acc 0.96875, learning_rate 0.000291554\n","2023-06-28T22:25:30.929312: step 3060, loss 0.300869, acc 0.96875, learning_rate 0.000291366\n","2023-06-28T22:25:31.035216: step 3061, loss 0.102229, acc 1, learning_rate 0.000291177\n","2023-06-28T22:25:31.133468: step 3062, loss 0.220091, acc 0.9375, learning_rate 0.000290989\n","2023-06-28T22:25:31.243085: step 3063, loss 0.270257, acc 0.9375, learning_rate 0.000290801\n","2023-06-28T22:25:31.345347: step 3064, loss 0.0981776, acc 1, learning_rate 0.000290613\n","2023-06-28T22:25:31.449773: step 3065, loss 0.112744, acc 0.96875, learning_rate 0.000290425\n","2023-06-28T22:25:31.561827: step 3066, loss 0.0814696, acc 1, learning_rate 0.000290238\n","2023-06-28T22:25:31.659522: step 3067, loss 0.146978, acc 0.90625, learning_rate 0.00029005\n","2023-06-28T22:25:31.762304: step 3068, loss 0.0177459, acc 1, learning_rate 0.000289863\n","2023-06-28T22:25:31.857838: step 3069, loss 0.195196, acc 0.96875, learning_rate 0.000289676\n","2023-06-28T22:25:31.961944: step 3070, loss 0.145438, acc 0.96875, learning_rate 0.00028949\n","2023-06-28T22:25:32.070255: step 3071, loss 0.0944803, acc 0.96875, learning_rate 0.000289303\n","2023-06-28T22:25:32.173805: step 3072, loss 0.161049, acc 0.9375, learning_rate 0.000289117\n","2023-06-28T22:25:32.284563: step 3073, loss 0.843927, acc 0.84375, learning_rate 0.00028893\n","2023-06-28T22:25:32.381561: step 3074, loss 0.419361, acc 0.9375, learning_rate 0.000288744\n","2023-06-28T22:25:32.479522: step 3075, loss 0.129384, acc 0.9375, learning_rate 0.000288559\n","2023-06-28T22:25:32.591248: step 3076, loss 0.210422, acc 0.9375, learning_rate 0.000288373\n","2023-06-28T22:25:32.688086: step 3077, loss 0.14846, acc 0.90625, learning_rate 0.000288187\n","2023-06-28T22:25:32.800244: step 3078, loss 0.227922, acc 0.9375, learning_rate 0.000288002\n","2023-06-28T22:25:32.897314: step 3079, loss 0.0858332, acc 0.96875, learning_rate 0.000287817\n","2023-06-28T22:25:33.002203: step 3080, loss 0.203555, acc 0.90625, learning_rate 0.000287632\n","2023-06-28T22:25:33.102840: step 3081, loss 0.16316, acc 0.9375, learning_rate 0.000287447\n","2023-06-28T22:25:33.197533: step 3082, loss 0.0831043, acc 0.96875, learning_rate 0.000287263\n","2023-06-28T22:25:33.298282: step 3083, loss 0.104059, acc 0.96875, learning_rate 0.000287078\n","2023-06-28T22:25:33.394778: step 3084, loss 0.0653457, acc 0.96875, learning_rate 0.000286894\n","2023-06-28T22:25:33.505584: step 3085, loss 0.108835, acc 1, learning_rate 0.00028671\n","2023-06-28T22:25:33.679302: step 3086, loss 0.515924, acc 0.875, learning_rate 0.000286526\n","2023-06-28T22:25:33.837917: step 3087, loss 0.138645, acc 0.96875, learning_rate 0.000286343\n","2023-06-28T22:25:34.008520: step 3088, loss 0.136957, acc 0.96875, learning_rate 0.000286159\n","2023-06-28T22:25:34.199644: step 3089, loss 0.0929385, acc 0.96875, learning_rate 0.000285976\n","2023-06-28T22:25:34.420181: step 3090, loss 0.103563, acc 0.9375, learning_rate 0.000285793\n","2023-06-28T22:25:34.588973: step 3091, loss 0.302736, acc 0.90625, learning_rate 0.00028561\n","2023-06-28T22:25:34.768021: step 3092, loss 0.150897, acc 0.9375, learning_rate 0.000285427\n","2023-06-28T22:25:34.940221: step 3093, loss 0.101565, acc 0.96875, learning_rate 0.000285244\n","2023-06-28T22:25:35.108602: step 3094, loss 0.108642, acc 1, learning_rate 0.000285062\n","2023-06-28T22:25:35.281351: step 3095, loss 0.0584385, acc 0.96875, learning_rate 0.00028488\n","2023-06-28T22:25:35.449394: step 3096, loss 0.0692288, acc 0.96875, learning_rate 0.000284698\n","2023-06-28T22:25:35.615290: step 3097, loss 0.111302, acc 1, learning_rate 0.000284516\n","2023-06-28T22:25:35.782434: step 3098, loss 0.0662958, acc 1, learning_rate 0.000284334\n","2023-06-28T22:25:35.952592: step 3099, loss 0.161605, acc 0.90625, learning_rate 0.000284153\n","\n","Evaluation:\n","2023-06-28T22:25:37.321026: step 3100, loss 1.00807, acc 0.837438\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3100\n","\n","2023-06-28T22:25:37.653248: step 3100, loss 0.147702, acc 0.96875, learning_rate 0.000283971\n","2023-06-28T22:25:37.818577: step 3101, loss 0.0383093, acc 1, learning_rate 0.00028379\n","2023-06-28T22:25:37.982338: step 3102, loss 0.0408047, acc 1, learning_rate 0.000283609\n","2023-06-28T22:25:38.150428: step 3103, loss 0.109813, acc 0.9375, learning_rate 0.000283428\n","2023-06-28T22:25:38.313478: step 3104, loss 0.63953, acc 0.875, learning_rate 0.000283248\n","2023-06-28T22:25:38.477557: step 3105, loss 0.294283, acc 0.90625, learning_rate 0.000283067\n","2023-06-28T22:25:38.653403: step 3106, loss 0.119398, acc 0.9375, learning_rate 0.000282887\n","2023-06-28T22:25:38.821366: step 3107, loss 0.180999, acc 0.90625, learning_rate 0.000282707\n","2023-06-28T22:25:38.993368: step 3108, loss 0.0757586, acc 0.96875, learning_rate 0.000282527\n","2023-06-28T22:25:39.166799: step 3109, loss 0.065331, acc 0.96875, learning_rate 0.000282347\n","2023-06-28T22:25:39.328495: step 3110, loss 0.222958, acc 0.90625, learning_rate 0.000282168\n","2023-06-28T22:25:39.501560: step 3111, loss 1.13181, acc 0.8125, learning_rate 0.000281988\n","2023-06-28T22:25:39.667848: step 3112, loss 0.122635, acc 0.9375, learning_rate 0.000281809\n","2023-06-28T22:25:39.834286: step 3113, loss 0.106383, acc 0.96875, learning_rate 0.00028163\n","2023-06-28T22:25:39.999418: step 3114, loss 0.0313707, acc 1, learning_rate 0.000281451\n","2023-06-28T22:25:40.185898: step 3115, loss 0.325206, acc 0.875, learning_rate 0.000281273\n","2023-06-28T22:25:40.352566: step 3116, loss 0.0776115, acc 0.9375, learning_rate 0.000281094\n","2023-06-28T22:25:40.516958: step 3117, loss 0.14131, acc 0.96875, learning_rate 0.000280916\n","2023-06-28T22:25:40.678126: step 3118, loss 0.0385307, acc 1, learning_rate 0.000280738\n","2023-06-28T22:25:40.854079: step 3119, loss 0.137852, acc 0.9375, learning_rate 0.00028056\n","2023-06-28T22:25:41.009194: step 3120, loss 0.177376, acc 0.90625, learning_rate 0.000280382\n","2023-06-28T22:25:41.175725: step 3121, loss 0.0301059, acc 1, learning_rate 0.000280204\n","2023-06-28T22:25:41.336581: step 3122, loss 0.375855, acc 0.90625, learning_rate 0.000280027\n","2023-06-28T22:25:41.504327: step 3123, loss 0.205261, acc 0.90625, learning_rate 0.00027985\n","2023-06-28T22:25:41.667240: step 3124, loss 0.307682, acc 0.84375, learning_rate 0.000279673\n","2023-06-28T22:25:41.820546: step 3125, loss 0.0912997, acc 1, learning_rate 0.000279496\n","2023-06-28T22:25:41.982503: step 3126, loss 0.147826, acc 0.96875, learning_rate 0.000279319\n","2023-06-28T22:25:42.153982: step 3127, loss 0.191852, acc 0.9375, learning_rate 0.000279142\n","2023-06-28T22:25:42.301825: step 3128, loss 0.0687761, acc 0.96875, learning_rate 0.000278966\n","2023-06-28T22:25:42.467079: step 3129, loss 0.0896441, acc 0.9375, learning_rate 0.00027879\n","2023-06-28T22:25:42.625608: step 3130, loss 0.251787, acc 0.90625, learning_rate 0.000278614\n","2023-06-28T22:25:42.793449: step 3131, loss 0.0982974, acc 0.9375, learning_rate 0.000278438\n","2023-06-28T22:25:42.960437: step 3132, loss 0.514345, acc 0.9375, learning_rate 0.000278262\n","2023-06-28T22:25:43.145207: step 3133, loss 0.0743805, acc 0.96875, learning_rate 0.000278087\n","2023-06-28T22:25:43.306161: step 3134, loss 0.0187013, acc 1, learning_rate 0.000277911\n","2023-06-28T22:25:43.475070: step 3135, loss 0.389297, acc 0.9375, learning_rate 0.000277736\n","2023-06-28T22:25:43.639624: step 3136, loss 0.389364, acc 0.84375, learning_rate 0.000277561\n","2023-06-28T22:25:43.803299: step 3137, loss 0.295656, acc 0.90625, learning_rate 0.000277386\n","2023-06-28T22:25:43.961938: step 3138, loss 0.0600652, acc 0.96875, learning_rate 0.000277212\n","2023-06-28T22:25:44.149352: step 3139, loss 0.175592, acc 0.90625, learning_rate 0.000277037\n","2023-06-28T22:25:44.317011: step 3140, loss 0.119322, acc 0.9375, learning_rate 0.000276863\n","2023-06-28T22:25:44.462507: step 3141, loss 0.0605031, acc 1, learning_rate 0.000276689\n","2023-06-28T22:25:44.559017: step 3142, loss 0.0733176, acc 0.96875, learning_rate 0.000276515\n","2023-06-28T22:25:44.655993: step 3143, loss 0.315464, acc 0.875, learning_rate 0.000276341\n","2023-06-28T22:25:44.758569: step 3144, loss 0.148249, acc 0.90625, learning_rate 0.000276167\n","2023-06-28T22:25:44.854666: step 3145, loss 0.39551, acc 0.875, learning_rate 0.000275994\n","2023-06-28T22:25:44.967136: step 3146, loss 0.253693, acc 0.9375, learning_rate 0.00027582\n","2023-06-28T22:25:45.075685: step 3147, loss 0.147787, acc 0.9375, learning_rate 0.000275647\n","2023-06-28T22:25:45.181346: step 3148, loss 0.181759, acc 0.9375, learning_rate 0.000275474\n","2023-06-28T22:25:45.288895: step 3149, loss 0.306857, acc 0.90625, learning_rate 0.000275302\n","2023-06-28T22:25:45.385914: step 3150, loss 0.306628, acc 0.875, learning_rate 0.000275129\n","2023-06-28T22:25:45.481505: step 3151, loss 0.128125, acc 0.9375, learning_rate 0.000274956\n","2023-06-28T22:25:45.592434: step 3152, loss 0.434036, acc 0.90625, learning_rate 0.000274784\n","2023-06-28T22:25:45.691601: step 3153, loss 0.424234, acc 0.875, learning_rate 0.000274612\n","2023-06-28T22:25:45.795504: step 3154, loss 0.299958, acc 0.90625, learning_rate 0.00027444\n","2023-06-28T22:25:45.897636: step 3155, loss 0.432843, acc 0.84375, learning_rate 0.000274268\n","2023-06-28T22:25:45.989858: step 3156, loss 0.201536, acc 0.9375, learning_rate 0.000274097\n","2023-06-28T22:25:46.112909: step 3157, loss 0.0850003, acc 0.9375, learning_rate 0.000273925\n","2023-06-28T22:25:46.216559: step 3158, loss 0.116824, acc 0.96875, learning_rate 0.000273754\n","2023-06-28T22:25:46.324697: step 3159, loss 0.230638, acc 0.90625, learning_rate 0.000273583\n","2023-06-28T22:25:46.422812: step 3160, loss 0.161938, acc 0.9375, learning_rate 0.000273412\n","2023-06-28T22:25:46.519735: step 3161, loss 0.0184349, acc 1, learning_rate 0.000273241\n","2023-06-28T22:25:46.623728: step 3162, loss 0.128373, acc 0.9375, learning_rate 0.000273071\n","2023-06-28T22:25:46.721315: step 3163, loss 0.086418, acc 1, learning_rate 0.0002729\n","2023-06-28T22:25:46.821192: step 3164, loss 0.8223, acc 0.78125, learning_rate 0.00027273\n","2023-06-28T22:25:46.921715: step 3165, loss 0.19092, acc 0.90625, learning_rate 0.00027256\n","2023-06-28T22:25:47.022682: step 3166, loss 0.145392, acc 0.9375, learning_rate 0.00027239\n","2023-06-28T22:25:47.136562: step 3167, loss 0.256796, acc 0.9375, learning_rate 0.00027222\n","2023-06-28T22:25:47.240338: step 3168, loss 0.0498337, acc 0.96875, learning_rate 0.000272051\n","2023-06-28T22:25:47.351202: step 3169, loss 0.0523694, acc 1, learning_rate 0.000271881\n","2023-06-28T22:25:47.446493: step 3170, loss 0.0870204, acc 1, learning_rate 0.000271712\n","2023-06-28T22:25:47.552419: step 3171, loss 0.229754, acc 0.90625, learning_rate 0.000271543\n","2023-06-28T22:25:47.654736: step 3172, loss 0.0979235, acc 0.96875, learning_rate 0.000271374\n","2023-06-28T22:25:47.750896: step 3173, loss 0.187545, acc 0.875, learning_rate 0.000271205\n","2023-06-28T22:25:47.851183: step 3174, loss 0.0953526, acc 0.96875, learning_rate 0.000271037\n","2023-06-28T22:25:47.945122: step 3175, loss 0.240227, acc 0.9375, learning_rate 0.000270868\n","2023-06-28T22:25:48.049819: step 3176, loss 0.0526013, acc 0.96875, learning_rate 0.0002707\n","2023-06-28T22:25:48.167662: step 3177, loss 0.202318, acc 0.90625, learning_rate 0.000270532\n","2023-06-28T22:25:48.268682: step 3178, loss 0.152365, acc 0.9375, learning_rate 0.000270364\n","2023-06-28T22:25:48.376837: step 3179, loss 0.0849162, acc 0.96875, learning_rate 0.000270196\n","2023-06-28T22:25:48.481214: step 3180, loss 0.0418786, acc 0.96875, learning_rate 0.000270029\n","2023-06-28T22:25:48.579607: step 3181, loss 0.156529, acc 0.9375, learning_rate 0.000269861\n","2023-06-28T22:25:48.682061: step 3182, loss 0.0977871, acc 0.9375, learning_rate 0.000269694\n","2023-06-28T22:25:48.790115: step 3183, loss 0.0282036, acc 1, learning_rate 0.000269527\n","2023-06-28T22:25:48.898283: step 3184, loss 0.662789, acc 0.8125, learning_rate 0.00026936\n","2023-06-28T22:25:49.005933: step 3185, loss 0.0698488, acc 1, learning_rate 0.000269193\n","2023-06-28T22:25:49.122941: step 3186, loss 0.226552, acc 0.9375, learning_rate 0.000269027\n","2023-06-28T22:25:49.240185: step 3187, loss 0.0875449, acc 0.96875, learning_rate 0.00026886\n","2023-06-28T22:25:49.344734: step 3188, loss 0.13759, acc 0.96875, learning_rate 0.000268694\n","2023-06-28T22:25:49.441505: step 3189, loss 0.212163, acc 0.96875, learning_rate 0.000268528\n","2023-06-28T22:25:49.548139: step 3190, loss 0.167234, acc 0.9375, learning_rate 0.000268362\n","2023-06-28T22:25:49.653719: step 3191, loss 0.0954246, acc 0.96875, learning_rate 0.000268196\n","2023-06-28T22:25:49.758263: step 3192, loss 0.459175, acc 0.90625, learning_rate 0.000268031\n","2023-06-28T22:25:49.856466: step 3193, loss 0.134572, acc 0.96875, learning_rate 0.000267865\n","2023-06-28T22:25:49.952055: step 3194, loss 0.499685, acc 0.875, learning_rate 0.0002677\n","2023-06-28T22:25:50.055936: step 3195, loss 0.00987879, acc 1, learning_rate 0.000267535\n","2023-06-28T22:25:50.164319: step 3196, loss 0.109518, acc 0.96875, learning_rate 0.00026737\n","2023-06-28T22:25:50.265077: step 3197, loss 0.111941, acc 0.9375, learning_rate 0.000267205\n","2023-06-28T22:25:50.378598: step 3198, loss 0.0388906, acc 1, learning_rate 0.00026704\n","2023-06-28T22:25:50.480880: step 3199, loss 0.198448, acc 0.90625, learning_rate 0.000266876\n","\n","Evaluation:\n","2023-06-28T22:25:51.201884: step 3200, loss 0.9966, acc 0.835591\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3200\n","\n","2023-06-28T22:25:51.396847: step 3200, loss 0.109931, acc 0.96875, learning_rate 0.000266712\n","2023-06-28T22:25:51.500829: step 3201, loss 0.109308, acc 0.96875, learning_rate 0.000266547\n","2023-06-28T22:25:51.602926: step 3202, loss 0.816139, acc 0.8125, learning_rate 0.000266383\n","2023-06-28T22:25:51.706718: step 3203, loss 0.0959889, acc 0.96875, learning_rate 0.00026622\n","2023-06-28T22:25:51.801931: step 3204, loss 0.231474, acc 0.90625, learning_rate 0.000266056\n","2023-06-28T22:25:51.911224: step 3205, loss 0.175017, acc 0.90625, learning_rate 0.000265892\n","2023-06-28T22:25:52.014224: step 3206, loss 0.0171639, acc 1, learning_rate 0.000265729\n","2023-06-28T22:25:52.120267: step 3207, loss 0.214274, acc 0.9375, learning_rate 0.000265566\n","2023-06-28T22:25:52.224861: step 3208, loss 0.772951, acc 0.9375, learning_rate 0.000265403\n","2023-06-28T22:25:52.333884: step 3209, loss 0.184742, acc 0.90625, learning_rate 0.00026524\n","2023-06-28T22:25:52.431042: step 3210, loss 0.247418, acc 0.9375, learning_rate 0.000265077\n","2023-06-28T22:25:52.536249: step 3211, loss 0.137251, acc 0.96875, learning_rate 0.000264915\n","2023-06-28T22:25:52.632784: step 3212, loss 0.51687, acc 0.90625, learning_rate 0.000264752\n","2023-06-28T22:25:52.728323: step 3213, loss 0.0728895, acc 0.96875, learning_rate 0.00026459\n","2023-06-28T22:25:52.827791: step 3214, loss 0.0742303, acc 1, learning_rate 0.000264428\n","2023-06-28T22:25:52.926267: step 3215, loss 0.118138, acc 0.9375, learning_rate 0.000264266\n","2023-06-28T22:25:53.022538: step 3216, loss 0.0700783, acc 0.96875, learning_rate 0.000264104\n","2023-06-28T22:25:53.131060: step 3217, loss 0.0843511, acc 0.96875, learning_rate 0.000263943\n","2023-06-28T22:25:53.238210: step 3218, loss 0.185957, acc 0.9375, learning_rate 0.000263781\n","2023-06-28T22:25:53.341051: step 3219, loss 0.0744321, acc 1, learning_rate 0.00026362\n","2023-06-28T22:25:53.454767: step 3220, loss 0.0272521, acc 1, learning_rate 0.000263459\n","2023-06-28T22:25:53.568227: step 3221, loss 0.360212, acc 0.875, learning_rate 0.000263298\n","2023-06-28T22:25:53.667056: step 3222, loss 0.160247, acc 0.9375, learning_rate 0.000263137\n","2023-06-28T22:25:53.770503: step 3223, loss 0.19698, acc 0.9375, learning_rate 0.000262977\n","2023-06-28T22:25:53.864711: step 3224, loss 0.0928358, acc 0.96875, learning_rate 0.000262816\n","2023-06-28T22:25:53.963366: step 3225, loss 0.144238, acc 0.9375, learning_rate 0.000262656\n","2023-06-28T22:25:54.070741: step 3226, loss 0.0811032, acc 0.9375, learning_rate 0.000262496\n","2023-06-28T22:25:54.180126: step 3227, loss 0.0956789, acc 0.9375, learning_rate 0.000262336\n","2023-06-28T22:25:54.286213: step 3228, loss 0.285591, acc 0.875, learning_rate 0.000262176\n","2023-06-28T22:25:54.405588: step 3229, loss 0.0876346, acc 1, learning_rate 0.000262016\n","2023-06-28T22:25:54.552650: step 3230, loss 0.592644, acc 0.84375, learning_rate 0.000261857\n","2023-06-28T22:25:54.725764: step 3231, loss 0.494608, acc 0.84375, learning_rate 0.000261697\n","2023-06-28T22:25:54.899072: step 3232, loss 0.126939, acc 0.96875, learning_rate 0.000261538\n","2023-06-28T22:25:55.069014: step 3233, loss 0.383919, acc 0.90625, learning_rate 0.000261379\n","2023-06-28T22:25:55.242428: step 3234, loss 0.600057, acc 0.90625, learning_rate 0.00026122\n","2023-06-28T22:25:55.428460: step 3235, loss 0.158553, acc 0.9375, learning_rate 0.000261061\n","2023-06-28T22:25:55.609787: step 3236, loss 0.0069872, acc 1, learning_rate 0.000260903\n","2023-06-28T22:25:55.781707: step 3237, loss 0.394852, acc 0.875, learning_rate 0.000260744\n","2023-06-28T22:25:55.957567: step 3238, loss 0.354473, acc 0.9375, learning_rate 0.000260586\n","2023-06-28T22:25:56.123987: step 3239, loss 0.136592, acc 0.9375, learning_rate 0.000260428\n","2023-06-28T22:25:56.292903: step 3240, loss 0.102269, acc 0.96875, learning_rate 0.00026027\n","2023-06-28T22:25:56.476563: step 3241, loss 0.22978, acc 0.90625, learning_rate 0.000260112\n","2023-06-28T22:25:56.647674: step 3242, loss 0.400273, acc 0.90625, learning_rate 0.000259954\n","2023-06-28T22:25:56.818024: step 3243, loss 0.26835, acc 0.875, learning_rate 0.000259797\n","2023-06-28T22:25:56.989950: step 3244, loss 0.230694, acc 0.875, learning_rate 0.00025964\n","2023-06-28T22:25:57.150369: step 3245, loss 0.0192934, acc 1, learning_rate 0.000259482\n","2023-06-28T22:25:57.336420: step 3246, loss 0.107318, acc 0.9375, learning_rate 0.000259325\n","2023-06-28T22:25:57.509151: step 3247, loss 0.254394, acc 0.875, learning_rate 0.000259168\n","2023-06-28T22:25:57.706778: step 3248, loss 0.141401, acc 0.96875, learning_rate 0.000259012\n","2023-06-28T22:25:57.871465: step 3249, loss 0.100295, acc 0.96875, learning_rate 0.000258855\n","2023-06-28T22:25:58.050594: step 3250, loss 1.00915, acc 0.8125, learning_rate 0.000258699\n","2023-06-28T22:25:58.233947: step 3251, loss 0.30719, acc 0.90625, learning_rate 0.000258542\n","2023-06-28T22:25:58.395607: step 3252, loss 0.131969, acc 0.9375, learning_rate 0.000258386\n","2023-06-28T22:25:58.558429: step 3253, loss 0.367159, acc 0.96875, learning_rate 0.00025823\n","2023-06-28T22:25:58.734596: step 3254, loss 0.234774, acc 0.84375, learning_rate 0.000258075\n","2023-06-28T22:25:58.843839: step 3255, loss 0.153781, acc 1, learning_rate 0.000257919\n","2023-06-28T22:25:59.019938: step 3256, loss 0.343768, acc 0.9375, learning_rate 0.000257763\n","2023-06-28T22:25:59.189726: step 3257, loss 0.040554, acc 0.96875, learning_rate 0.000257608\n","2023-06-28T22:25:59.413430: step 3258, loss 0.0115386, acc 1, learning_rate 0.000257453\n","2023-06-28T22:25:59.584251: step 3259, loss 0.0556337, acc 1, learning_rate 0.000257298\n","2023-06-28T22:25:59.760062: step 3260, loss 0.234647, acc 0.875, learning_rate 0.000257143\n","2023-06-28T22:25:59.936616: step 3261, loss 0.0557004, acc 1, learning_rate 0.000256988\n","2023-06-28T22:26:00.120190: step 3262, loss 0.107106, acc 0.9375, learning_rate 0.000256834\n","2023-06-28T22:26:00.290901: step 3263, loss 0.203343, acc 0.90625, learning_rate 0.000256679\n","2023-06-28T22:26:00.471175: step 3264, loss 0.0793523, acc 1, learning_rate 0.000256525\n","2023-06-28T22:26:00.641056: step 3265, loss 0.160519, acc 0.875, learning_rate 0.000256371\n","2023-06-28T22:26:00.808685: step 3266, loss 0.0715844, acc 0.96875, learning_rate 0.000256217\n","2023-06-28T22:26:00.962259: step 3267, loss 0.31815, acc 0.875, learning_rate 0.000256063\n","2023-06-28T22:26:01.155061: step 3268, loss 0.056596, acc 1, learning_rate 0.000255909\n","2023-06-28T22:26:01.321656: step 3269, loss 0.183832, acc 0.96875, learning_rate 0.000255756\n","2023-06-28T22:26:01.503634: step 3270, loss 0.0748397, acc 0.96875, learning_rate 0.000255603\n","2023-06-28T22:26:01.671789: step 3271, loss 0.168263, acc 0.96875, learning_rate 0.000255449\n","2023-06-28T22:26:01.818252: step 3272, loss 0.148264, acc 0.9375, learning_rate 0.000255296\n","2023-06-28T22:26:01.983899: step 3273, loss 0.394996, acc 0.9375, learning_rate 0.000255143\n","2023-06-28T22:26:02.161340: step 3274, loss 0.454358, acc 0.90625, learning_rate 0.000254991\n","2023-06-28T22:26:02.330687: step 3275, loss 0.321838, acc 0.96875, learning_rate 0.000254838\n","2023-06-28T22:26:02.497256: step 3276, loss 0.294113, acc 0.84375, learning_rate 0.000254686\n","2023-06-28T22:26:02.648626: step 3277, loss 0.0759895, acc 0.96875, learning_rate 0.000254533\n","2023-06-28T22:26:02.792649: step 3278, loss 0.0814367, acc 0.96875, learning_rate 0.000254381\n","2023-06-28T22:26:02.943230: step 3279, loss 0.13765, acc 0.96875, learning_rate 0.000254229\n","2023-06-28T22:26:03.104723: step 3280, loss 0.13437, acc 0.9375, learning_rate 0.000254077\n","2023-06-28T22:26:03.269172: step 3281, loss 0.0948591, acc 0.96875, learning_rate 0.000253925\n","2023-06-28T22:26:03.414434: step 3282, loss 0.13375, acc 0.9375, learning_rate 0.000253774\n","2023-06-28T22:26:03.578636: step 3283, loss 0.147701, acc 0.9375, learning_rate 0.000253622\n","2023-06-28T22:26:03.755936: step 3284, loss 0.169968, acc 0.90625, learning_rate 0.000253471\n","2023-06-28T22:26:03.918954: step 3285, loss 0.0483784, acc 0.96875, learning_rate 0.00025332\n","2023-06-28T22:26:04.081019: step 3286, loss 0.304472, acc 0.84375, learning_rate 0.000253169\n","2023-06-28T22:26:04.251474: step 3287, loss 0.0955318, acc 0.9375, learning_rate 0.000253018\n","2023-06-28T22:26:04.413841: step 3288, loss 0.127642, acc 0.9375, learning_rate 0.000252868\n","2023-06-28T22:26:04.601201: step 3289, loss 0.0297995, acc 1, learning_rate 0.000252717\n","2023-06-28T22:26:04.758114: step 3290, loss 0.329615, acc 0.96875, learning_rate 0.000252567\n","2023-06-28T22:26:04.929646: step 3291, loss 0.183848, acc 0.96875, learning_rate 0.000252416\n","2023-06-28T22:26:05.092104: step 3292, loss 0.0645687, acc 0.96875, learning_rate 0.000252266\n","2023-06-28T22:26:05.196883: step 3293, loss 0.539388, acc 0.90625, learning_rate 0.000252116\n","2023-06-28T22:26:05.304672: step 3294, loss 0.0852937, acc 0.96875, learning_rate 0.000251967\n","2023-06-28T22:26:05.402855: step 3295, loss 0.0780818, acc 0.96875, learning_rate 0.000251817\n","2023-06-28T22:26:05.505205: step 3296, loss 0.0926476, acc 0.96875, learning_rate 0.000251668\n","2023-06-28T22:26:05.615462: step 3297, loss 0.0152181, acc 1, learning_rate 0.000251518\n","2023-06-28T22:26:05.717352: step 3298, loss 0.169001, acc 0.9375, learning_rate 0.000251369\n","2023-06-28T22:26:05.813745: step 3299, loss 0.0437798, acc 1, learning_rate 0.00025122\n","\n","Evaluation:\n","2023-06-28T22:26:06.533057: step 3300, loss 1.10569, acc 0.830665\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3300\n","\n","2023-06-28T22:26:06.725623: step 3300, loss 0.728227, acc 0.96875, learning_rate 0.000251071\n","2023-06-28T22:26:06.841689: step 3301, loss 0.0838275, acc 0.9375, learning_rate 0.000250922\n","2023-06-28T22:26:06.939777: step 3302, loss 0.0309724, acc 1, learning_rate 0.000250774\n","2023-06-28T22:26:07.041588: step 3303, loss 0.0252475, acc 1, learning_rate 0.000250625\n","2023-06-28T22:26:07.147232: step 3304, loss 0.13236, acc 0.9375, learning_rate 0.000250477\n","2023-06-28T22:26:07.244902: step 3305, loss 0.417222, acc 0.875, learning_rate 0.000250329\n","2023-06-28T22:26:07.343408: step 3306, loss 0.191251, acc 0.9375, learning_rate 0.000250181\n","2023-06-28T22:26:07.450462: step 3307, loss 0.0857864, acc 0.96875, learning_rate 0.000250033\n","2023-06-28T22:26:07.548452: step 3308, loss 0.226141, acc 0.9375, learning_rate 0.000249885\n","2023-06-28T22:26:07.643961: step 3309, loss 0.296288, acc 0.875, learning_rate 0.000249738\n","2023-06-28T22:26:07.746522: step 3310, loss 0.078117, acc 0.96875, learning_rate 0.00024959\n","2023-06-28T22:26:07.853770: step 3311, loss 0.179492, acc 0.9375, learning_rate 0.000249443\n","2023-06-28T22:26:07.952780: step 3312, loss 0.111298, acc 0.96875, learning_rate 0.000249296\n","2023-06-28T22:26:08.049347: step 3313, loss 0.108058, acc 0.9375, learning_rate 0.000249149\n","2023-06-28T22:26:08.175317: step 3314, loss 0.175067, acc 0.90625, learning_rate 0.000249002\n","2023-06-28T22:26:08.275237: step 3315, loss 0.194632, acc 0.90625, learning_rate 0.000248855\n","2023-06-28T22:26:08.371012: step 3316, loss 0.287141, acc 0.9375, learning_rate 0.000248709\n","2023-06-28T22:26:08.474593: step 3317, loss 0.198054, acc 0.9375, learning_rate 0.000248562\n","2023-06-28T22:26:08.569582: step 3318, loss 0.105744, acc 0.96875, learning_rate 0.000248416\n","2023-06-28T22:26:08.670698: step 3319, loss 0.0687972, acc 0.96875, learning_rate 0.00024827\n","2023-06-28T22:26:08.780576: step 3320, loss 0.705833, acc 0.875, learning_rate 0.000248124\n","2023-06-28T22:26:08.881716: step 3321, loss 0.0876384, acc 0.9375, learning_rate 0.000247978\n","2023-06-28T22:26:08.980071: step 3322, loss 0.100566, acc 0.96875, learning_rate 0.000247832\n","2023-06-28T22:26:09.083797: step 3323, loss 0.168073, acc 0.90625, learning_rate 0.000247687\n","2023-06-28T22:26:09.183439: step 3324, loss 0.0978612, acc 0.9375, learning_rate 0.000247541\n","2023-06-28T22:26:09.283671: step 3325, loss 0.0362952, acc 1, learning_rate 0.000247396\n","2023-06-28T22:26:09.403734: step 3326, loss 0.16595, acc 0.96875, learning_rate 0.000247251\n","2023-06-28T22:26:09.502601: step 3327, loss 0.148451, acc 0.9375, learning_rate 0.000247106\n","2023-06-28T22:26:09.612386: step 3328, loss 0.802951, acc 0.90625, learning_rate 0.000246961\n","2023-06-28T22:26:09.715538: step 3329, loss 0.592825, acc 0.84375, learning_rate 0.000246816\n","2023-06-28T22:26:09.823702: step 3330, loss 0.120293, acc 0.90625, learning_rate 0.000246672\n","2023-06-28T22:26:09.920939: step 3331, loss 0.0907603, acc 1, learning_rate 0.000246527\n","2023-06-28T22:26:10.037671: step 3332, loss 0.0701678, acc 0.96875, learning_rate 0.000246383\n","2023-06-28T22:26:10.139817: step 3333, loss 0.852248, acc 0.875, learning_rate 0.000246239\n","2023-06-28T22:26:10.243241: step 3334, loss 0.053128, acc 1, learning_rate 0.000246095\n","2023-06-28T22:26:10.353126: step 3335, loss 0.0712286, acc 1, learning_rate 0.000245951\n","2023-06-28T22:26:10.451289: step 3336, loss 0.0960318, acc 0.96875, learning_rate 0.000245807\n","2023-06-28T22:26:10.551406: step 3337, loss 0.0595263, acc 0.96875, learning_rate 0.000245664\n","2023-06-28T22:26:10.651019: step 3338, loss 0.237756, acc 0.9375, learning_rate 0.00024552\n","2023-06-28T22:26:10.748961: step 3339, loss 0.182227, acc 0.90625, learning_rate 0.000245377\n","2023-06-28T22:26:10.854502: step 3340, loss 0.18021, acc 0.9375, learning_rate 0.000245234\n","2023-06-28T22:26:10.953808: step 3341, loss 0.0250726, acc 1, learning_rate 0.000245091\n","2023-06-28T22:26:11.060052: step 3342, loss 0.140772, acc 0.9375, learning_rate 0.000244948\n","2023-06-28T22:26:11.163834: step 3343, loss 0.223165, acc 0.90625, learning_rate 0.000244805\n","2023-06-28T22:26:11.264542: step 3344, loss 0.331364, acc 0.96875, learning_rate 0.000244663\n","2023-06-28T22:26:11.394327: step 3345, loss 0.100431, acc 0.9375, learning_rate 0.00024452\n","2023-06-28T22:26:11.519334: step 3346, loss 0.269911, acc 0.9375, learning_rate 0.000244378\n","2023-06-28T22:26:11.612915: step 3347, loss 0.15693, acc 0.90625, learning_rate 0.000244236\n","2023-06-28T22:26:11.718609: step 3348, loss 0.123009, acc 0.9375, learning_rate 0.000244094\n","2023-06-28T22:26:11.812986: step 3349, loss 0.308209, acc 0.9375, learning_rate 0.000243952\n","2023-06-28T22:26:11.916158: step 3350, loss 0.0589222, acc 1, learning_rate 0.00024381\n","2023-06-28T22:26:12.018927: step 3351, loss 0.0913364, acc 1, learning_rate 0.000243668\n","2023-06-28T22:26:12.126281: step 3352, loss 0.0630958, acc 0.96875, learning_rate 0.000243527\n","2023-06-28T22:26:12.227262: step 3353, loss 0.136856, acc 0.96875, learning_rate 0.000243386\n","2023-06-28T22:26:12.337172: step 3354, loss 0.0388318, acc 1, learning_rate 0.000243244\n","2023-06-28T22:26:12.436985: step 3355, loss 0.128852, acc 0.96875, learning_rate 0.000243103\n","2023-06-28T22:26:12.536856: step 3356, loss 0.188312, acc 0.9375, learning_rate 0.000242963\n","2023-06-28T22:26:12.642638: step 3357, loss 0.223126, acc 0.9375, learning_rate 0.000242822\n","2023-06-28T22:26:12.745956: step 3358, loss 0.150917, acc 0.96875, learning_rate 0.000242681\n","2023-06-28T22:26:12.845149: step 3359, loss 0.0333949, acc 1, learning_rate 0.000242541\n","2023-06-28T22:26:12.957206: step 3360, loss 0.310765, acc 0.9375, learning_rate 0.0002424\n","2023-06-28T22:26:13.047708: step 3361, loss 0.0421556, acc 1, learning_rate 0.00024226\n","2023-06-28T22:26:13.145670: step 3362, loss 0.060236, acc 0.96875, learning_rate 0.00024212\n","2023-06-28T22:26:13.249995: step 3363, loss 0.21613, acc 0.90625, learning_rate 0.00024198\n","2023-06-28T22:26:13.352521: step 3364, loss 0.0454771, acc 1, learning_rate 0.00024184\n","2023-06-28T22:26:13.453928: step 3365, loss 0.512466, acc 0.90625, learning_rate 0.000241701\n","2023-06-28T22:26:13.549588: step 3366, loss 0.286923, acc 0.875, learning_rate 0.000241561\n","2023-06-28T22:26:13.644592: step 3367, loss 0.0177592, acc 1, learning_rate 0.000241422\n","2023-06-28T22:26:13.751549: step 3368, loss 0.352084, acc 0.84375, learning_rate 0.000241282\n","2023-06-28T22:26:13.847871: step 3369, loss 0.0344512, acc 1, learning_rate 0.000241143\n","2023-06-28T22:26:13.972816: step 3370, loss 0.154196, acc 0.96875, learning_rate 0.000241004\n","2023-06-28T22:26:14.074629: step 3371, loss 0.279523, acc 0.875, learning_rate 0.000240865\n","2023-06-28T22:26:14.173589: step 3372, loss 0.0614574, acc 0.96875, learning_rate 0.000240727\n","2023-06-28T22:26:14.269262: step 3373, loss 0.029211, acc 1, learning_rate 0.000240588\n","2023-06-28T22:26:14.382613: step 3374, loss 0.0458154, acc 0.96875, learning_rate 0.00024045\n","2023-06-28T22:26:14.492496: step 3375, loss 0.137397, acc 0.9375, learning_rate 0.000240311\n","2023-06-28T22:26:14.599720: step 3376, loss 0.150703, acc 0.90625, learning_rate 0.000240173\n","2023-06-28T22:26:14.709079: step 3377, loss 0.0269028, acc 1, learning_rate 0.000240035\n","2023-06-28T22:26:14.808530: step 3378, loss 0.17241, acc 0.90625, learning_rate 0.000239897\n","2023-06-28T22:26:14.906633: step 3379, loss 0.125881, acc 0.9375, learning_rate 0.00023976\n","2023-06-28T22:26:15.031513: step 3380, loss 0.147672, acc 0.9375, learning_rate 0.000239622\n","2023-06-28T22:26:15.148807: step 3381, loss 0.110117, acc 0.96875, learning_rate 0.000239485\n","2023-06-28T22:26:15.317983: step 3382, loss 0.180025, acc 0.90625, learning_rate 0.000239347\n","2023-06-28T22:26:15.473246: step 3383, loss 0.153766, acc 0.9375, learning_rate 0.00023921\n","2023-06-28T22:26:15.649325: step 3384, loss 0.093545, acc 0.9375, learning_rate 0.000239073\n","2023-06-28T22:26:15.803483: step 3385, loss 0.252265, acc 0.84375, learning_rate 0.000238936\n","2023-06-28T22:26:15.974801: step 3386, loss 0.0781529, acc 0.96875, learning_rate 0.000238799\n","2023-06-28T22:26:16.149328: step 3387, loss 0.118773, acc 0.9375, learning_rate 0.000238662\n","2023-06-28T22:26:16.315691: step 3388, loss 0.0552173, acc 0.96875, learning_rate 0.000238526\n","2023-06-28T22:26:16.487565: step 3389, loss 0.176667, acc 0.9375, learning_rate 0.00023839\n","2023-06-28T22:26:16.655491: step 3390, loss 0.175294, acc 0.90625, learning_rate 0.000238253\n","2023-06-28T22:26:16.829761: step 3391, loss 0.0422882, acc 1, learning_rate 0.000238117\n","2023-06-28T22:26:16.988718: step 3392, loss 0.0445373, acc 1, learning_rate 0.000237981\n","2023-06-28T22:26:17.170754: step 3393, loss 0.112612, acc 0.96875, learning_rate 0.000237845\n","2023-06-28T22:26:17.333994: step 3394, loss 0.0840472, acc 0.96875, learning_rate 0.00023771\n","2023-06-28T22:26:17.520790: step 3395, loss 0.0755687, acc 1, learning_rate 0.000237574\n","2023-06-28T22:26:17.689977: step 3396, loss 0.160604, acc 0.96875, learning_rate 0.000237438\n","2023-06-28T22:26:17.865170: step 3397, loss 0.108296, acc 0.96875, learning_rate 0.000237303\n","2023-06-28T22:26:18.040150: step 3398, loss 0.0839157, acc 0.9375, learning_rate 0.000237168\n","2023-06-28T22:26:18.209652: step 3399, loss 0.0826397, acc 0.96875, learning_rate 0.000237033\n","\n","Evaluation:\n","2023-06-28T22:26:19.480952: step 3400, loss 1.04814, acc 0.836207\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3400\n","\n","2023-06-28T22:26:19.800555: step 3400, loss 0.286754, acc 0.96875, learning_rate 0.000236898\n","2023-06-28T22:26:19.951605: step 3401, loss 0.39759, acc 0.90625, learning_rate 0.000236763\n","2023-06-28T22:26:20.144765: step 3402, loss 0.0935107, acc 0.96875, learning_rate 0.000236629\n","2023-06-28T22:26:20.300288: step 3403, loss 0.0486788, acc 0.96875, learning_rate 0.000236494\n","2023-06-28T22:26:20.470677: step 3404, loss 0.179005, acc 0.90625, learning_rate 0.00023636\n","2023-06-28T22:26:20.651013: step 3405, loss 0.271445, acc 0.90625, learning_rate 0.000236225\n","2023-06-28T22:26:20.829565: step 3406, loss 0.0843059, acc 0.96875, learning_rate 0.000236091\n","2023-06-28T22:26:20.989003: step 3407, loss 0.103617, acc 0.9375, learning_rate 0.000235957\n","2023-06-28T22:26:21.159014: step 3408, loss 0.0440542, acc 1, learning_rate 0.000235823\n","2023-06-28T22:26:21.326448: step 3409, loss 0.507952, acc 0.9375, learning_rate 0.00023569\n","2023-06-28T22:26:21.495847: step 3410, loss 0.0389656, acc 1, learning_rate 0.000235556\n","2023-06-28T22:26:21.657606: step 3411, loss 0.0621043, acc 1, learning_rate 0.000235422\n","2023-06-28T22:26:21.828149: step 3412, loss 0.125216, acc 0.9375, learning_rate 0.000235289\n","2023-06-28T22:26:21.979985: step 3413, loss 0.0467109, acc 1, learning_rate 0.000235156\n","2023-06-28T22:26:22.166245: step 3414, loss 0.305075, acc 0.9375, learning_rate 0.000235023\n","2023-06-28T22:26:22.337183: step 3415, loss 0.107281, acc 0.9375, learning_rate 0.00023489\n","2023-06-28T22:26:22.501407: step 3416, loss 0.324332, acc 0.90625, learning_rate 0.000234757\n","2023-06-28T22:26:22.647930: step 3417, loss 0.141377, acc 0.9375, learning_rate 0.000234624\n","2023-06-28T22:26:22.831079: step 3418, loss 0.159313, acc 0.9375, learning_rate 0.000234492\n","2023-06-28T22:26:22.998322: step 3419, loss 0.0163022, acc 1, learning_rate 0.000234359\n","2023-06-28T22:26:23.175600: step 3420, loss 0.0911541, acc 0.96875, learning_rate 0.000234227\n","2023-06-28T22:26:23.345246: step 3421, loss 0.124296, acc 0.90625, learning_rate 0.000234095\n","2023-06-28T22:26:23.522518: step 3422, loss 0.169934, acc 0.9375, learning_rate 0.000233963\n","2023-06-28T22:26:23.683122: step 3423, loss 0.0991293, acc 0.9375, learning_rate 0.000233831\n","2023-06-28T22:26:23.851075: step 3424, loss 0.203385, acc 0.90625, learning_rate 0.000233699\n","2023-06-28T22:26:24.022645: step 3425, loss 0.208568, acc 0.90625, learning_rate 0.000233568\n","2023-06-28T22:26:24.228600: step 3426, loss 0.0167391, acc 1, learning_rate 0.000233436\n","2023-06-28T22:26:24.408423: step 3427, loss 0.371428, acc 0.90625, learning_rate 0.000233305\n","2023-06-28T22:26:24.584711: step 3428, loss 0.0950485, acc 0.96875, learning_rate 0.000233173\n","2023-06-28T22:26:24.737600: step 3429, loss 0.0636499, acc 0.96875, learning_rate 0.000233042\n","2023-06-28T22:26:24.888548: step 3430, loss 0.0361878, acc 1, learning_rate 0.000232911\n","2023-06-28T22:26:25.057113: step 3431, loss 0.182644, acc 0.9375, learning_rate 0.00023278\n","2023-06-28T22:26:25.214792: step 3432, loss 0.0649558, acc 1, learning_rate 0.00023265\n","2023-06-28T22:26:25.376769: step 3433, loss 0.177318, acc 0.9375, learning_rate 0.000232519\n","2023-06-28T22:26:25.554981: step 3434, loss 0.0507746, acc 1, learning_rate 0.000232389\n","2023-06-28T22:26:25.707010: step 3435, loss 0.100543, acc 0.9375, learning_rate 0.000232258\n","2023-06-28T22:26:25.857087: step 3436, loss 0.28777, acc 0.90625, learning_rate 0.000232128\n","2023-06-28T22:26:26.008300: step 3437, loss 0.256675, acc 0.90625, learning_rate 0.000231998\n","2023-06-28T22:26:26.116553: step 3438, loss 0.222556, acc 0.875, learning_rate 0.000231868\n","2023-06-28T22:26:26.214301: step 3439, loss 0.0386761, acc 1, learning_rate 0.000231738\n","2023-06-28T22:26:26.308554: step 3440, loss 0.108766, acc 0.96875, learning_rate 0.000231608\n","2023-06-28T22:26:26.415097: step 3441, loss 0.0888105, acc 0.96875, learning_rate 0.000231479\n","2023-06-28T22:26:26.519745: step 3442, loss 0.179031, acc 0.9375, learning_rate 0.000231349\n","2023-06-28T22:26:26.613925: step 3443, loss 0.54478, acc 0.84375, learning_rate 0.00023122\n","2023-06-28T22:26:26.715937: step 3444, loss 0.253022, acc 0.9375, learning_rate 0.000231091\n","2023-06-28T22:26:26.813105: step 3445, loss 0.0723067, acc 1, learning_rate 0.000230962\n","2023-06-28T22:26:26.908424: step 3446, loss 0.222888, acc 0.84375, learning_rate 0.000230833\n","2023-06-28T22:26:27.009661: step 3447, loss 0.154633, acc 0.96875, learning_rate 0.000230704\n","2023-06-28T22:26:27.105463: step 3448, loss 0.142389, acc 0.96875, learning_rate 0.000230575\n","2023-06-28T22:26:27.199857: step 3449, loss 0.102808, acc 0.9375, learning_rate 0.000230447\n","2023-06-28T22:26:27.305278: step 3450, loss 0.0838361, acc 0.96875, learning_rate 0.000230318\n","2023-06-28T22:26:27.404030: step 3451, loss 0.193256, acc 0.90625, learning_rate 0.00023019\n","2023-06-28T22:26:27.517434: step 3452, loss 0.354713, acc 0.84375, learning_rate 0.000230062\n","2023-06-28T22:26:27.613199: step 3453, loss 0.114546, acc 0.9375, learning_rate 0.000229934\n","2023-06-28T22:26:27.707568: step 3454, loss 0.175188, acc 0.875, learning_rate 0.000229806\n","2023-06-28T22:26:27.807510: step 3455, loss 0.0330976, acc 1, learning_rate 0.000229678\n","2023-06-28T22:26:27.905736: step 3456, loss 0.0920362, acc 0.96875, learning_rate 0.00022955\n","2023-06-28T22:26:28.046144: step 3457, loss 0.220063, acc 0.875, learning_rate 0.000229423\n","2023-06-28T22:26:28.160309: step 3458, loss 0.0959817, acc 0.96875, learning_rate 0.000229295\n","2023-06-28T22:26:28.256762: step 3459, loss 0.0358002, acc 1, learning_rate 0.000229168\n","2023-06-28T22:26:28.353253: step 3460, loss 0.0863013, acc 0.96875, learning_rate 0.000229041\n","2023-06-28T22:26:28.465717: step 3461, loss 0.0698906, acc 1, learning_rate 0.000228914\n","2023-06-28T22:26:28.574901: step 3462, loss 0.0978542, acc 0.96875, learning_rate 0.000228787\n","2023-06-28T22:26:28.680343: step 3463, loss 0.0722601, acc 0.96875, learning_rate 0.00022866\n","2023-06-28T22:26:28.789237: step 3464, loss 0.0572547, acc 0.96875, learning_rate 0.000228533\n","2023-06-28T22:26:28.888228: step 3465, loss 0.131459, acc 0.90625, learning_rate 0.000228407\n","2023-06-28T22:26:28.982051: step 3466, loss 0.284603, acc 0.90625, learning_rate 0.00022828\n","2023-06-28T22:26:29.078484: step 3467, loss 0.0643866, acc 0.96875, learning_rate 0.000228154\n","2023-06-28T22:26:29.180278: step 3468, loss 0.0599447, acc 1, learning_rate 0.000228028\n","2023-06-28T22:26:29.272605: step 3469, loss 0.238822, acc 0.9375, learning_rate 0.000227902\n","2023-06-28T22:26:29.401903: step 3470, loss 0.341577, acc 0.90625, learning_rate 0.000227776\n","2023-06-28T22:26:29.510702: step 3471, loss 0.0412013, acc 1, learning_rate 0.00022765\n","2023-06-28T22:26:29.616711: step 3472, loss 0.0527839, acc 1, learning_rate 0.000227524\n","2023-06-28T22:26:29.715518: step 3473, loss 0.0860212, acc 0.96875, learning_rate 0.000227399\n","2023-06-28T22:26:29.811756: step 3474, loss 1.01543, acc 0.875, learning_rate 0.000227273\n","2023-06-28T22:26:29.905899: step 3475, loss 0.0325066, acc 1, learning_rate 0.000227148\n","2023-06-28T22:26:30.006885: step 3476, loss 0.26556, acc 0.875, learning_rate 0.000227023\n","2023-06-28T22:26:30.102128: step 3477, loss 0.169397, acc 0.9375, learning_rate 0.000226898\n","2023-06-28T22:26:30.203388: step 3478, loss 0.0870257, acc 0.9375, learning_rate 0.000226773\n","2023-06-28T22:26:30.304996: step 3479, loss 0.0469918, acc 1, learning_rate 0.000226648\n","2023-06-28T22:26:30.400257: step 3480, loss 0.246127, acc 0.875, learning_rate 0.000226523\n","2023-06-28T22:26:30.510247: step 3481, loss 0.196571, acc 0.9375, learning_rate 0.000226398\n","2023-06-28T22:26:30.608897: step 3482, loss 0.694381, acc 0.96875, learning_rate 0.000226274\n","2023-06-28T22:26:30.701587: step 3483, loss 0.235648, acc 0.96875, learning_rate 0.00022615\n","2023-06-28T22:26:30.810134: step 3484, loss 0.11526, acc 0.9375, learning_rate 0.000226025\n","2023-06-28T22:26:30.914138: step 3485, loss 0.446688, acc 0.90625, learning_rate 0.000225901\n","2023-06-28T22:26:31.013054: step 3486, loss 0.133226, acc 0.96875, learning_rate 0.000225777\n","2023-06-28T22:26:31.129862: step 3487, loss 0.0157839, acc 1, learning_rate 0.000225654\n","2023-06-28T22:26:31.231924: step 3488, loss 0.0525841, acc 0.96875, learning_rate 0.00022553\n","2023-06-28T22:26:31.330010: step 3489, loss 0.198793, acc 0.90625, learning_rate 0.000225406\n","2023-06-28T22:26:31.432058: step 3490, loss 0.11459, acc 0.96875, learning_rate 0.000225283\n","2023-06-28T22:26:31.530427: step 3491, loss 0.0479934, acc 1, learning_rate 0.000225159\n","2023-06-28T22:26:31.656108: step 3492, loss 0.0670625, acc 0.9375, learning_rate 0.000225036\n","2023-06-28T22:26:31.749814: step 3493, loss 0.0481783, acc 1, learning_rate 0.000224913\n","2023-06-28T22:26:31.847001: step 3494, loss 0.00646923, acc 1, learning_rate 0.00022479\n","2023-06-28T22:26:31.946602: step 3495, loss 0.344157, acc 0.875, learning_rate 0.000224667\n","2023-06-28T22:26:32.046723: step 3496, loss 0.0560874, acc 1, learning_rate 0.000224544\n","2023-06-28T22:26:32.154309: step 3497, loss 0.07854, acc 0.96875, learning_rate 0.000224422\n","2023-06-28T22:26:32.266209: step 3498, loss 0.0574603, acc 0.96875, learning_rate 0.000224299\n","2023-06-28T22:26:32.362777: step 3499, loss 0.17272, acc 0.96875, learning_rate 0.000224177\n","\n","Evaluation:\n","2023-06-28T22:26:33.545362: step 3500, loss 1.12121, acc 0.837746\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3500\n","\n","2023-06-28T22:26:33.858662: step 3500, loss 0.221421, acc 0.84375, learning_rate 0.000224055\n","2023-06-28T22:26:34.021317: step 3501, loss 0.310706, acc 0.9375, learning_rate 0.000223932\n","2023-06-28T22:26:34.193181: step 3502, loss 0.271568, acc 0.90625, learning_rate 0.00022381\n","2023-06-28T22:26:34.354897: step 3503, loss 0.290127, acc 0.875, learning_rate 0.000223688\n","2023-06-28T22:26:34.530606: step 3504, loss 0.0465775, acc 0.96875, learning_rate 0.000223567\n","2023-06-28T22:26:34.697567: step 3505, loss 0.188725, acc 0.90625, learning_rate 0.000223445\n","2023-06-28T22:26:34.898200: step 3506, loss 0.153467, acc 0.9375, learning_rate 0.000223323\n","2023-06-28T22:26:35.066146: step 3507, loss 0.0841813, acc 0.96875, learning_rate 0.000223202\n","2023-06-28T22:26:35.250736: step 3508, loss 0.0566716, acc 0.96875, learning_rate 0.000223081\n","2023-06-28T22:26:35.419976: step 3509, loss 0.190642, acc 0.96875, learning_rate 0.00022296\n","2023-06-28T22:26:35.596203: step 3510, loss 0.0628842, acc 0.96875, learning_rate 0.000222838\n","2023-06-28T22:26:35.764653: step 3511, loss 0.00975571, acc 1, learning_rate 0.000222717\n","2023-06-28T22:26:35.937261: step 3512, loss 0.198984, acc 0.9375, learning_rate 0.000222597\n","2023-06-28T22:26:36.122293: step 3513, loss 0.0549538, acc 0.96875, learning_rate 0.000222476\n","2023-06-28T22:26:36.325513: step 3514, loss 0.370034, acc 0.9375, learning_rate 0.000222355\n","2023-06-28T22:26:36.537437: step 3515, loss 0.162082, acc 0.9375, learning_rate 0.000222235\n","2023-06-28T22:26:36.748078: step 3516, loss 0.130635, acc 0.9375, learning_rate 0.000222114\n","2023-06-28T22:26:36.947079: step 3517, loss 0.143465, acc 0.90625, learning_rate 0.000221994\n","2023-06-28T22:26:37.122710: step 3518, loss 0.147083, acc 0.96875, learning_rate 0.000221874\n","2023-06-28T22:26:37.355043: step 3519, loss 0.131293, acc 0.96875, learning_rate 0.000221754\n","2023-06-28T22:26:37.532672: step 3520, loss 0.182809, acc 0.875, learning_rate 0.000221634\n","2023-06-28T22:26:37.756656: step 3521, loss 0.0975923, acc 0.96875, learning_rate 0.000221514\n","2023-06-28T22:26:37.937425: step 3522, loss 0.165874, acc 0.96875, learning_rate 0.000221395\n","2023-06-28T22:26:38.134227: step 3523, loss 0.0688851, acc 0.96875, learning_rate 0.000221275\n","2023-06-28T22:26:38.317706: step 3524, loss 0.0418956, acc 1, learning_rate 0.000221156\n","2023-06-28T22:26:38.510749: step 3525, loss 0.0391998, acc 0.96875, learning_rate 0.000221037\n","2023-06-28T22:26:38.720922: step 3526, loss 0.0724926, acc 0.96875, learning_rate 0.000220917\n","2023-06-28T22:26:38.907191: step 3527, loss 0.321821, acc 0.96875, learning_rate 0.000220798\n","2023-06-28T22:26:39.137600: step 3528, loss 0.209623, acc 0.875, learning_rate 0.000220679\n","2023-06-28T22:26:39.328554: step 3529, loss 0.49827, acc 0.90625, learning_rate 0.000220561\n","2023-06-28T22:26:39.594343: step 3530, loss 0.126151, acc 0.9375, learning_rate 0.000220442\n","2023-06-28T22:26:39.786543: step 3531, loss 0.292433, acc 0.90625, learning_rate 0.000220323\n","2023-06-28T22:26:39.972737: step 3532, loss 0.104177, acc 0.9375, learning_rate 0.000220205\n","2023-06-28T22:26:40.225649: step 3533, loss 0.0213238, acc 1, learning_rate 0.000220086\n","2023-06-28T22:26:40.410255: step 3534, loss 0.0268326, acc 1, learning_rate 0.000219968\n","2023-06-28T22:26:40.622789: step 3535, loss 0.132934, acc 0.9375, learning_rate 0.00021985\n","2023-06-28T22:26:40.863789: step 3536, loss 0.0749202, acc 0.9375, learning_rate 0.000219732\n","2023-06-28T22:26:41.066696: step 3537, loss 0.0414793, acc 1, learning_rate 0.000219614\n","2023-06-28T22:26:41.319392: step 3538, loss 0.080579, acc 0.96875, learning_rate 0.000219496\n","2023-06-28T22:26:41.529555: step 3539, loss 0.0726693, acc 0.96875, learning_rate 0.000219379\n","2023-06-28T22:26:41.733613: step 3540, loss 0.0470225, acc 0.96875, learning_rate 0.000219261\n","2023-06-28T22:26:41.925620: step 3541, loss 0.0368568, acc 1, learning_rate 0.000219144\n","2023-06-28T22:26:42.108369: step 3542, loss 0.0458245, acc 0.96875, learning_rate 0.000219026\n","2023-06-28T22:26:42.306536: step 3543, loss 0.0151439, acc 1, learning_rate 0.000218909\n","2023-06-28T22:26:42.489863: step 3544, loss 0.160851, acc 0.9375, learning_rate 0.000218792\n","2023-06-28T22:26:42.666587: step 3545, loss 0.178847, acc 0.9375, learning_rate 0.000218675\n","2023-06-28T22:26:42.887638: step 3546, loss 0.0039387, acc 1, learning_rate 0.000218558\n","2023-06-28T22:26:43.058013: step 3547, loss 0.159504, acc 0.96875, learning_rate 0.000218442\n","2023-06-28T22:26:43.237454: step 3548, loss 0.403694, acc 0.9375, learning_rate 0.000218325\n","2023-06-28T22:26:43.412940: step 3549, loss 0.130355, acc 0.9375, learning_rate 0.000218208\n","2023-06-28T22:26:43.584515: step 3550, loss 0.133253, acc 0.9375, learning_rate 0.000218092\n","2023-06-28T22:26:43.743249: step 3551, loss 0.0679769, acc 0.96875, learning_rate 0.000217976\n","2023-06-28T22:26:43.918287: step 3552, loss 0.527599, acc 0.9375, learning_rate 0.00021786\n","2023-06-28T22:26:44.079206: step 3553, loss 0.135384, acc 0.9375, learning_rate 0.000217744\n","2023-06-28T22:26:44.261631: step 3554, loss 0.108146, acc 0.96875, learning_rate 0.000217628\n","2023-06-28T22:26:44.442816: step 3555, loss 0.143975, acc 0.9375, learning_rate 0.000217512\n","2023-06-28T22:26:44.642073: step 3556, loss 0.136312, acc 0.9375, learning_rate 0.000217396\n","2023-06-28T22:26:44.812895: step 3557, loss 0.141191, acc 0.9375, learning_rate 0.00021728\n","2023-06-28T22:26:45.004801: step 3558, loss 0.301384, acc 0.90625, learning_rate 0.000217165\n","2023-06-28T22:26:45.165567: step 3559, loss 0.144257, acc 0.9375, learning_rate 0.00021705\n","2023-06-28T22:26:45.344015: step 3560, loss 0.230296, acc 0.9375, learning_rate 0.000216934\n","2023-06-28T22:26:45.517557: step 3561, loss 0.304114, acc 0.90625, learning_rate 0.000216819\n","2023-06-28T22:26:45.696824: step 3562, loss 0.0204472, acc 1, learning_rate 0.000216704\n","2023-06-28T22:26:45.863152: step 3563, loss 0.270287, acc 0.90625, learning_rate 0.000216589\n","2023-06-28T22:26:46.038080: step 3564, loss 0.685171, acc 0.875, learning_rate 0.000216474\n","2023-06-28T22:26:46.218616: step 3565, loss 0.0593099, acc 1, learning_rate 0.00021636\n","2023-06-28T22:26:46.384958: step 3566, loss 0.104818, acc 0.9375, learning_rate 0.000216245\n","2023-06-28T22:26:46.600172: step 3567, loss 0.134606, acc 0.9375, learning_rate 0.000216131\n","2023-06-28T22:26:46.768590: step 3568, loss 0.388393, acc 0.90625, learning_rate 0.000216016\n","2023-06-28T22:26:46.944487: step 3569, loss 0.18724, acc 0.9375, learning_rate 0.000215902\n","2023-06-28T22:26:47.117975: step 3570, loss 0.194748, acc 0.9375, learning_rate 0.000215788\n","2023-06-28T22:26:47.276181: step 3571, loss 0.0731838, acc 0.96875, learning_rate 0.000215674\n","2023-06-28T22:26:47.439756: step 3572, loss 0.0998578, acc 0.9375, learning_rate 0.00021556\n","2023-06-28T22:26:47.599502: step 3573, loss 0.0716237, acc 0.96875, learning_rate 0.000215446\n","2023-06-28T22:26:47.758193: step 3574, loss 0.41124, acc 0.90625, learning_rate 0.000215333\n","2023-06-28T22:26:47.922143: step 3575, loss 0.0694778, acc 0.96875, learning_rate 0.000215219\n","2023-06-28T22:26:48.093290: step 3576, loss 0.181748, acc 0.9375, learning_rate 0.000215106\n","2023-06-28T22:26:48.263022: step 3577, loss 0.252677, acc 0.90625, learning_rate 0.000214992\n","2023-06-28T22:26:48.425983: step 3578, loss 0.255214, acc 0.90625, learning_rate 0.000214879\n","2023-06-28T22:26:48.594945: step 3579, loss 0.506284, acc 0.875, learning_rate 0.000214766\n","2023-06-28T22:26:48.762545: step 3580, loss 0.290284, acc 0.875, learning_rate 0.000214653\n","2023-06-28T22:26:48.917008: step 3581, loss 0.0798416, acc 0.96875, learning_rate 0.00021454\n","2023-06-28T22:26:49.084575: step 3582, loss 0.452011, acc 0.90625, learning_rate 0.000214427\n","2023-06-28T22:26:49.242360: step 3583, loss 0.201972, acc 0.90625, learning_rate 0.000214315\n","2023-06-28T22:26:49.400375: step 3584, loss 0.115255, acc 0.96875, learning_rate 0.000214202\n","2023-06-28T22:26:49.574052: step 3585, loss 0.110132, acc 0.9375, learning_rate 0.00021409\n","2023-06-28T22:26:49.731872: step 3586, loss 0.0689851, acc 0.9375, learning_rate 0.000213977\n","2023-06-28T22:26:49.893853: step 3587, loss 0.0838047, acc 0.96875, learning_rate 0.000213865\n","2023-06-28T22:26:50.057106: step 3588, loss 0.0298116, acc 1, learning_rate 0.000213753\n","2023-06-28T22:26:50.230973: step 3589, loss 0.0563615, acc 0.96875, learning_rate 0.000213641\n","2023-06-28T22:26:50.398621: step 3590, loss 0.238425, acc 0.9375, learning_rate 0.000213529\n","2023-06-28T22:26:50.562940: step 3591, loss 0.114569, acc 0.9375, learning_rate 0.000213417\n","2023-06-28T22:26:50.723341: step 3592, loss 0.174526, acc 0.90625, learning_rate 0.000213306\n","2023-06-28T22:26:50.893420: step 3593, loss 0.0953486, acc 0.96875, learning_rate 0.000213194\n","2023-06-28T22:26:51.067903: step 3594, loss 0.216792, acc 0.9375, learning_rate 0.000213083\n","2023-06-28T22:26:51.221099: step 3595, loss 0.0397226, acc 0.96875, learning_rate 0.000212971\n","2023-06-28T22:26:51.380163: step 3596, loss 0.0226332, acc 1, learning_rate 0.00021286\n","2023-06-28T22:26:51.507123: step 3597, loss 0.0690673, acc 1, learning_rate 0.000212749\n","2023-06-28T22:26:51.603210: step 3598, loss 0.372895, acc 0.875, learning_rate 0.000212638\n","2023-06-28T22:26:51.708251: step 3599, loss 0.18677, acc 0.9375, learning_rate 0.000212527\n","\n","Evaluation:\n","2023-06-28T22:26:52.419857: step 3600, loss 1.14105, acc 0.830973\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3600\n","\n","2023-06-28T22:26:52.627389: step 3600, loss 0.0892339, acc 0.96875, learning_rate 0.000212416\n","2023-06-28T22:26:52.735692: step 3601, loss 0.0636006, acc 1, learning_rate 0.000212305\n","2023-06-28T22:26:52.845468: step 3602, loss 0.131903, acc 0.9375, learning_rate 0.000212195\n","2023-06-28T22:26:52.946035: step 3603, loss 0.0570832, acc 0.96875, learning_rate 0.000212084\n","2023-06-28T22:26:53.042937: step 3604, loss 0.0196175, acc 1, learning_rate 0.000211974\n","2023-06-28T22:26:53.144601: step 3605, loss 0.0369944, acc 1, learning_rate 0.000211864\n","2023-06-28T22:26:53.241271: step 3606, loss 0.110789, acc 0.9375, learning_rate 0.000211754\n","2023-06-28T22:26:53.342792: step 3607, loss 0.433296, acc 0.90625, learning_rate 0.000211644\n","2023-06-28T22:26:53.440987: step 3608, loss 0.142439, acc 0.9375, learning_rate 0.000211534\n","2023-06-28T22:26:53.543131: step 3609, loss 0.0839246, acc 0.96875, learning_rate 0.000211424\n","2023-06-28T22:26:53.637076: step 3610, loss 0.0470216, acc 1, learning_rate 0.000211314\n","2023-06-28T22:26:53.741581: step 3611, loss 0.102106, acc 0.96875, learning_rate 0.000211204\n","2023-06-28T22:26:53.858047: step 3612, loss 0.164462, acc 0.90625, learning_rate 0.000211095\n","2023-06-28T22:26:53.957341: step 3613, loss 0.102084, acc 0.96875, learning_rate 0.000210986\n","2023-06-28T22:26:54.053656: step 3614, loss 0.022182, acc 1, learning_rate 0.000210876\n","2023-06-28T22:26:54.170839: step 3615, loss 0.0976527, acc 0.96875, learning_rate 0.000210767\n","2023-06-28T22:26:54.282729: step 3616, loss 0.216648, acc 0.90625, learning_rate 0.000210658\n","2023-06-28T22:26:54.401338: step 3617, loss 0.0575311, acc 1, learning_rate 0.000210549\n","2023-06-28T22:26:54.509865: step 3618, loss 0.323153, acc 0.96875, learning_rate 0.00021044\n","2023-06-28T22:26:54.607365: step 3619, loss 0.240546, acc 0.90625, learning_rate 0.000210331\n","2023-06-28T22:26:54.711173: step 3620, loss 0.536479, acc 0.9375, learning_rate 0.000210223\n","2023-06-28T22:26:54.827342: step 3621, loss 0.801054, acc 0.9375, learning_rate 0.000210114\n","2023-06-28T22:26:54.922920: step 3622, loss 0.262643, acc 0.90625, learning_rate 0.000210006\n","2023-06-28T22:26:55.029915: step 3623, loss 0.135913, acc 0.96875, learning_rate 0.000209898\n","2023-06-28T22:26:55.134300: step 3624, loss 0.0952406, acc 0.96875, learning_rate 0.000209789\n","2023-06-28T22:26:55.236289: step 3625, loss 0.445, acc 0.875, learning_rate 0.000209681\n","2023-06-28T22:26:55.340571: step 3626, loss 0.163969, acc 0.9375, learning_rate 0.000209573\n","2023-06-28T22:26:55.439604: step 3627, loss 0.102174, acc 0.96875, learning_rate 0.000209465\n","2023-06-28T22:26:55.540890: step 3628, loss 0.50397, acc 0.90625, learning_rate 0.000209358\n","2023-06-28T22:26:55.633683: step 3629, loss 0.0298301, acc 0.96875, learning_rate 0.00020925\n","2023-06-28T22:26:55.730087: step 3630, loss 0.149797, acc 0.96875, learning_rate 0.000209142\n","2023-06-28T22:26:55.844857: step 3631, loss 0.321113, acc 0.8125, learning_rate 0.000209035\n","2023-06-28T22:26:55.942347: step 3632, loss 0.0679542, acc 1, learning_rate 0.000208927\n","2023-06-28T22:26:56.046826: step 3633, loss 0.181591, acc 0.90625, learning_rate 0.00020882\n","2023-06-28T22:26:56.148981: step 3634, loss 0.074151, acc 0.96875, learning_rate 0.000208713\n","2023-06-28T22:26:56.248784: step 3635, loss 0.0607372, acc 1, learning_rate 0.000208606\n","2023-06-28T22:26:56.347225: step 3636, loss 0.0638388, acc 0.96875, learning_rate 0.000208499\n","2023-06-28T22:26:56.440573: step 3637, loss 0.0142619, acc 1, learning_rate 0.000208392\n","2023-06-28T22:26:56.533397: step 3638, loss 0.0900831, acc 0.96875, learning_rate 0.000208285\n","2023-06-28T22:26:56.632508: step 3639, loss 0.00569046, acc 1, learning_rate 0.000208179\n","2023-06-28T22:26:56.729301: step 3640, loss 0.255654, acc 0.90625, learning_rate 0.000208072\n","2023-06-28T22:26:56.822879: step 3641, loss 0.147675, acc 0.9375, learning_rate 0.000207966\n","2023-06-28T22:26:56.938244: step 3642, loss 0.171333, acc 0.96875, learning_rate 0.00020786\n","2023-06-28T22:26:57.037026: step 3643, loss 0.159464, acc 0.90625, learning_rate 0.000207753\n","2023-06-28T22:26:57.163518: step 3644, loss 0.279718, acc 0.9375, learning_rate 0.000207647\n","2023-06-28T22:26:57.264481: step 3645, loss 0.317416, acc 0.9375, learning_rate 0.000207541\n","2023-06-28T22:26:57.362938: step 3646, loss 0.12772, acc 0.9375, learning_rate 0.000207435\n","2023-06-28T22:26:57.467454: step 3647, loss 0.0526189, acc 0.96875, learning_rate 0.00020733\n","2023-06-28T22:26:57.566862: step 3648, loss 0.0739674, acc 0.9375, learning_rate 0.000207224\n","2023-06-28T22:26:57.664097: step 3649, loss 0.182866, acc 0.96875, learning_rate 0.000207118\n","2023-06-28T22:26:57.772691: step 3650, loss 0.895801, acc 0.9375, learning_rate 0.000207013\n","2023-06-28T22:26:57.872880: step 3651, loss 0.385477, acc 0.90625, learning_rate 0.000206908\n","2023-06-28T22:26:57.962444: step 3652, loss 0.261052, acc 0.90625, learning_rate 0.000206802\n","2023-06-28T22:26:58.070567: step 3653, loss 0.0567624, acc 1, learning_rate 0.000206697\n","2023-06-28T22:26:58.172038: step 3654, loss 0.100761, acc 1, learning_rate 0.000206592\n","2023-06-28T22:26:58.279795: step 3655, loss 0.396678, acc 0.9375, learning_rate 0.000206487\n","2023-06-28T22:26:58.380694: step 3656, loss 0.486956, acc 0.9375, learning_rate 0.000206382\n","2023-06-28T22:26:58.474205: step 3657, loss 0.334924, acc 0.9375, learning_rate 0.000206277\n","2023-06-28T22:26:58.579443: step 3658, loss 0.421071, acc 0.90625, learning_rate 0.000206173\n","2023-06-28T22:26:58.678043: step 3659, loss 0.0696646, acc 0.96875, learning_rate 0.000206068\n","2023-06-28T22:26:58.787860: step 3660, loss 0.717316, acc 0.84375, learning_rate 0.000205964\n","2023-06-28T22:26:58.891591: step 3661, loss 0.107316, acc 0.96875, learning_rate 0.00020586\n","2023-06-28T22:26:58.955646: step 3662, loss 0.000671875, acc 1, learning_rate 0.000205755\n","2023-06-28T22:26:59.060034: step 3663, loss 0.160035, acc 0.90625, learning_rate 0.000205651\n","2023-06-28T22:26:59.162611: step 3664, loss 0.078273, acc 0.96875, learning_rate 0.000205547\n","2023-06-28T22:26:59.258489: step 3665, loss 0.17713, acc 0.90625, learning_rate 0.000205443\n","2023-06-28T22:26:59.366744: step 3666, loss 0.162442, acc 0.90625, learning_rate 0.000205339\n","2023-06-28T22:26:59.495673: step 3667, loss 0.322675, acc 0.84375, learning_rate 0.000205236\n","2023-06-28T22:26:59.589884: step 3668, loss 0.115918, acc 0.96875, learning_rate 0.000205132\n","2023-06-28T22:26:59.694701: step 3669, loss 0.108418, acc 0.9375, learning_rate 0.000205028\n","2023-06-28T22:26:59.791427: step 3670, loss 0.219482, acc 0.90625, learning_rate 0.000204925\n","2023-06-28T22:26:59.896014: step 3671, loss 0.0600281, acc 1, learning_rate 0.000204822\n","2023-06-28T22:27:00.004484: step 3672, loss 0.108232, acc 0.9375, learning_rate 0.000204719\n","2023-06-28T22:27:00.100889: step 3673, loss 0.309606, acc 0.875, learning_rate 0.000204615\n","2023-06-28T22:27:00.204865: step 3674, loss 0.267652, acc 0.9375, learning_rate 0.000204512\n","2023-06-28T22:27:00.317030: step 3675, loss 0.0730221, acc 0.96875, learning_rate 0.00020441\n","2023-06-28T22:27:00.432601: step 3676, loss 0.0232023, acc 1, learning_rate 0.000204307\n","2023-06-28T22:27:00.536666: step 3677, loss 0.141252, acc 0.9375, learning_rate 0.000204204\n","2023-06-28T22:27:00.637641: step 3678, loss 0.0279515, acc 1, learning_rate 0.000204101\n","2023-06-28T22:27:00.746096: step 3679, loss 0.109147, acc 0.96875, learning_rate 0.000203999\n","2023-06-28T22:27:00.843252: step 3680, loss 0.105247, acc 0.9375, learning_rate 0.000203896\n","2023-06-28T22:27:00.949247: step 3681, loss 0.265175, acc 0.875, learning_rate 0.000203794\n","2023-06-28T22:27:01.058455: step 3682, loss 0.0904004, acc 1, learning_rate 0.000203692\n","2023-06-28T22:27:01.159104: step 3683, loss 0.276655, acc 0.875, learning_rate 0.00020359\n","2023-06-28T22:27:01.258981: step 3684, loss 0.0551641, acc 1, learning_rate 0.000203488\n","2023-06-28T22:27:01.361625: step 3685, loss 0.120773, acc 0.9375, learning_rate 0.000203386\n","2023-06-28T22:27:01.459102: step 3686, loss 0.1289, acc 0.90625, learning_rate 0.000203284\n","2023-06-28T22:27:01.595246: step 3687, loss 0.163821, acc 0.9375, learning_rate 0.000203182\n","2023-06-28T22:27:01.745348: step 3688, loss 0.0863729, acc 0.9375, learning_rate 0.000203081\n","2023-06-28T22:27:01.914063: step 3689, loss 0.0797042, acc 0.96875, learning_rate 0.000202979\n","2023-06-28T22:27:02.088066: step 3690, loss 0.00132079, acc 1, learning_rate 0.000202878\n","2023-06-28T22:27:02.264430: step 3691, loss 0.0233716, acc 1, learning_rate 0.000202777\n","2023-06-28T22:27:02.447464: step 3692, loss 0.115483, acc 0.9375, learning_rate 0.000202675\n","2023-06-28T22:27:02.624517: step 3693, loss 0.147967, acc 0.96875, learning_rate 0.000202574\n","2023-06-28T22:27:02.798203: step 3694, loss 0.12084, acc 0.96875, learning_rate 0.000202473\n","2023-06-28T22:27:02.970833: step 3695, loss 0.115915, acc 0.9375, learning_rate 0.000202372\n","2023-06-28T22:27:03.153878: step 3696, loss 0.0664576, acc 1, learning_rate 0.000202272\n","2023-06-28T22:27:03.320645: step 3697, loss 0.456258, acc 0.90625, learning_rate 0.000202171\n","2023-06-28T22:27:03.482842: step 3698, loss 0.0481758, acc 1, learning_rate 0.00020207\n","2023-06-28T22:27:03.650527: step 3699, loss 0.0436646, acc 0.96875, learning_rate 0.00020197\n","\n","Evaluation:\n","2023-06-28T22:27:04.879983: step 3700, loss 1.201, acc 0.832512\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3700\n","\n","2023-06-28T22:27:05.224836: step 3700, loss 0.0768793, acc 0.9375, learning_rate 0.000201869\n","2023-06-28T22:27:05.385208: step 3701, loss 0.0861816, acc 0.96875, learning_rate 0.000201769\n","2023-06-28T22:27:05.570658: step 3702, loss 0.30356, acc 0.90625, learning_rate 0.000201669\n","2023-06-28T22:27:05.756883: step 3703, loss 0.183435, acc 0.9375, learning_rate 0.000201569\n","2023-06-28T22:27:05.929738: step 3704, loss 0.146142, acc 0.9375, learning_rate 0.000201469\n","2023-06-28T22:27:06.099220: step 3705, loss 0.168983, acc 0.90625, learning_rate 0.000201369\n","2023-06-28T22:27:06.256614: step 3706, loss 0.273379, acc 0.96875, learning_rate 0.000201269\n","2023-06-28T22:27:06.414880: step 3707, loss 0.0601147, acc 1, learning_rate 0.000201169\n","2023-06-28T22:27:06.589965: step 3708, loss 0.168019, acc 0.96875, learning_rate 0.00020107\n","2023-06-28T22:27:06.755070: step 3709, loss 0.407241, acc 0.875, learning_rate 0.00020097\n","2023-06-28T22:27:06.921886: step 3710, loss 0.0831029, acc 0.96875, learning_rate 0.000200871\n","2023-06-28T22:27:07.086445: step 3711, loss 0.0609528, acc 0.96875, learning_rate 0.000200772\n","2023-06-28T22:27:07.234853: step 3712, loss 0.0455329, acc 1, learning_rate 0.000200672\n","2023-06-28T22:27:07.399088: step 3713, loss 0.181798, acc 0.9375, learning_rate 0.000200573\n","2023-06-28T22:27:07.566330: step 3714, loss 0.140797, acc 0.9375, learning_rate 0.000200474\n","2023-06-28T22:27:07.732233: step 3715, loss 0.204152, acc 0.9375, learning_rate 0.000200375\n","2023-06-28T22:27:07.876564: step 3716, loss 0.141549, acc 0.9375, learning_rate 0.000200276\n","2023-06-28T22:27:08.029299: step 3717, loss 0.0533282, acc 1, learning_rate 0.000200178\n","2023-06-28T22:27:08.198497: step 3718, loss 0.0644204, acc 1, learning_rate 0.000200079\n","2023-06-28T22:27:08.356790: step 3719, loss 0.0547803, acc 1, learning_rate 0.00019998\n","2023-06-28T22:27:08.515052: step 3720, loss 0.623967, acc 0.875, learning_rate 0.000199882\n","2023-06-28T22:27:08.686623: step 3721, loss 0.449168, acc 0.96875, learning_rate 0.000199784\n","2023-06-28T22:27:08.855570: step 3722, loss 0.058454, acc 1, learning_rate 0.000199685\n","2023-06-28T22:27:09.027528: step 3723, loss 0.196294, acc 0.96875, learning_rate 0.000199587\n","2023-06-28T22:27:09.193734: step 3724, loss 0.0883789, acc 0.96875, learning_rate 0.000199489\n","2023-06-28T22:27:09.374511: step 3725, loss 0.134535, acc 0.90625, learning_rate 0.000199391\n","2023-06-28T22:27:09.552418: step 3726, loss 0.213789, acc 0.9375, learning_rate 0.000199293\n","2023-06-28T22:27:09.730658: step 3727, loss 0.320217, acc 0.9375, learning_rate 0.000199196\n","2023-06-28T22:27:09.890981: step 3728, loss 0.0356551, acc 1, learning_rate 0.000199098\n","2023-06-28T22:27:10.052271: step 3729, loss 0.0796688, acc 0.9375, learning_rate 0.000199\n","2023-06-28T22:27:10.225022: step 3730, loss 0.0070747, acc 1, learning_rate 0.000198903\n","2023-06-28T22:27:10.419893: step 3731, loss 0.231771, acc 0.875, learning_rate 0.000198805\n","2023-06-28T22:27:10.587905: step 3732, loss 0.0235153, acc 1, learning_rate 0.000198708\n","2023-06-28T22:27:10.757618: step 3733, loss 0.0210709, acc 1, learning_rate 0.000198611\n","2023-06-28T22:27:10.903806: step 3734, loss 0.112437, acc 0.9375, learning_rate 0.000198514\n","2023-06-28T22:27:11.077317: step 3735, loss 0.121563, acc 0.9375, learning_rate 0.000198417\n","2023-06-28T22:27:11.241376: step 3736, loss 0.0354025, acc 1, learning_rate 0.00019832\n","2023-06-28T22:27:11.401195: step 3737, loss 0.00268504, acc 1, learning_rate 0.000198223\n","2023-06-28T22:27:11.573872: step 3738, loss 0.0259034, acc 1, learning_rate 0.000198126\n","2023-06-28T22:27:11.728015: step 3739, loss 0.050064, acc 1, learning_rate 0.00019803\n","2023-06-28T22:27:11.875784: step 3740, loss 0.0192962, acc 1, learning_rate 0.000197933\n","2023-06-28T22:27:12.046772: step 3741, loss 0.130434, acc 0.96875, learning_rate 0.000197837\n","2023-06-28T22:27:12.207856: step 3742, loss 0.0865704, acc 0.96875, learning_rate 0.000197741\n","2023-06-28T22:27:12.381606: step 3743, loss 0.0940161, acc 0.9375, learning_rate 0.000197644\n","2023-06-28T22:27:12.512453: step 3744, loss 0.255227, acc 0.96875, learning_rate 0.000197548\n","2023-06-28T22:27:12.607091: step 3745, loss 0.235394, acc 0.90625, learning_rate 0.000197452\n","2023-06-28T22:27:12.713714: step 3746, loss 0.429207, acc 0.875, learning_rate 0.000197356\n","2023-06-28T22:27:12.815489: step 3747, loss 0.10881, acc 0.9375, learning_rate 0.00019726\n","2023-06-28T22:27:12.908911: step 3748, loss 0.101364, acc 0.9375, learning_rate 0.000197164\n","2023-06-28T22:27:13.019319: step 3749, loss 0.0420533, acc 0.96875, learning_rate 0.000197069\n","2023-06-28T22:27:13.117475: step 3750, loss 0.345945, acc 0.96875, learning_rate 0.000196973\n","2023-06-28T22:27:13.215600: step 3751, loss 0.0379591, acc 1, learning_rate 0.000196878\n","2023-06-28T22:27:13.317078: step 3752, loss 0.0960776, acc 0.96875, learning_rate 0.000196782\n","2023-06-28T22:27:13.412387: step 3753, loss 0.120152, acc 0.9375, learning_rate 0.000196687\n","2023-06-28T22:27:13.528705: step 3754, loss 0.111581, acc 0.96875, learning_rate 0.000196592\n","2023-06-28T22:27:13.627759: step 3755, loss 0.0677725, acc 0.96875, learning_rate 0.000196497\n","2023-06-28T22:27:13.729103: step 3756, loss 0.42509, acc 0.96875, learning_rate 0.000196402\n","2023-06-28T22:27:13.835006: step 3757, loss 0.0246829, acc 1, learning_rate 0.000196307\n","2023-06-28T22:27:13.929880: step 3758, loss 0.251167, acc 0.875, learning_rate 0.000196212\n","2023-06-28T22:27:14.048991: step 3759, loss 0.0561413, acc 0.96875, learning_rate 0.000196117\n","2023-06-28T22:27:14.143056: step 3760, loss 0.169302, acc 0.96875, learning_rate 0.000196023\n","2023-06-28T22:27:14.237981: step 3761, loss 0.402777, acc 0.875, learning_rate 0.000195928\n","2023-06-28T22:27:14.332014: step 3762, loss 0.0588384, acc 0.96875, learning_rate 0.000195834\n","2023-06-28T22:27:14.457056: step 3763, loss 0.101418, acc 0.96875, learning_rate 0.000195739\n","2023-06-28T22:27:14.572840: step 3764, loss 0.221566, acc 0.9375, learning_rate 0.000195645\n","2023-06-28T22:27:14.674775: step 3765, loss 0.0889307, acc 0.96875, learning_rate 0.000195551\n","2023-06-28T22:27:14.788211: step 3766, loss 0.0579661, acc 1, learning_rate 0.000195457\n","2023-06-28T22:27:14.895750: step 3767, loss 0.0610657, acc 0.96875, learning_rate 0.000195363\n","2023-06-28T22:27:14.994127: step 3768, loss 0.0492117, acc 0.96875, learning_rate 0.000195269\n","2023-06-28T22:27:15.091975: step 3769, loss 0.0926957, acc 0.96875, learning_rate 0.000195175\n","2023-06-28T22:27:15.197776: step 3770, loss 0.0250401, acc 1, learning_rate 0.000195081\n","2023-06-28T22:27:15.291970: step 3771, loss 0.183578, acc 0.9375, learning_rate 0.000194988\n","2023-06-28T22:27:15.387624: step 3772, loss 0.197745, acc 0.90625, learning_rate 0.000194894\n","2023-06-28T22:27:15.486611: step 3773, loss 0.128077, acc 0.96875, learning_rate 0.000194801\n","2023-06-28T22:27:15.590330: step 3774, loss 0.143858, acc 0.9375, learning_rate 0.000194707\n","2023-06-28T22:27:15.686965: step 3775, loss 0.0487846, acc 0.96875, learning_rate 0.000194614\n","2023-06-28T22:27:15.792532: step 3776, loss 0.304341, acc 0.8125, learning_rate 0.000194521\n","2023-06-28T22:27:15.888552: step 3777, loss 0.107375, acc 0.9375, learning_rate 0.000194428\n","2023-06-28T22:27:15.988312: step 3778, loss 0.194135, acc 0.96875, learning_rate 0.000194335\n","2023-06-28T22:27:16.080651: step 3779, loss 0.0924185, acc 0.9375, learning_rate 0.000194242\n","2023-06-28T22:27:16.174850: step 3780, loss 0.417389, acc 0.96875, learning_rate 0.000194149\n","2023-06-28T22:27:16.266165: step 3781, loss 0.285694, acc 0.875, learning_rate 0.000194056\n","2023-06-28T22:27:16.372806: step 3782, loss 0.149457, acc 0.9375, learning_rate 0.000193964\n","2023-06-28T22:27:16.476055: step 3783, loss 0.0962304, acc 0.9375, learning_rate 0.000193871\n","2023-06-28T22:27:16.578867: step 3784, loss 0.146563, acc 0.9375, learning_rate 0.000193779\n","2023-06-28T22:27:16.688778: step 3785, loss 0.0544955, acc 1, learning_rate 0.000193687\n","2023-06-28T22:27:16.793614: step 3786, loss 0.0162186, acc 1, learning_rate 0.000193594\n","2023-06-28T22:27:16.893650: step 3787, loss 0.192761, acc 0.9375, learning_rate 0.000193502\n","2023-06-28T22:27:16.997189: step 3788, loss 0.025551, acc 1, learning_rate 0.00019341\n","2023-06-28T22:27:17.095186: step 3789, loss 0.182675, acc 0.9375, learning_rate 0.000193318\n","2023-06-28T22:27:17.189788: step 3790, loss 0.0859814, acc 0.96875, learning_rate 0.000193226\n","2023-06-28T22:27:17.289479: step 3791, loss 0.222186, acc 0.96875, learning_rate 0.000193134\n","2023-06-28T22:27:17.384801: step 3792, loss 0.205295, acc 0.875, learning_rate 0.000193043\n","2023-06-28T22:27:17.490506: step 3793, loss 0.0521553, acc 1, learning_rate 0.000192951\n","2023-06-28T22:27:17.617497: step 3794, loss 0.12788, acc 0.9375, learning_rate 0.00019286\n","2023-06-28T22:27:17.711302: step 3795, loss 0.153523, acc 0.96875, learning_rate 0.000192768\n","2023-06-28T22:27:17.812340: step 3796, loss 0.36126, acc 0.875, learning_rate 0.000192677\n","2023-06-28T22:27:17.909081: step 3797, loss 0.102449, acc 0.96875, learning_rate 0.000192586\n","2023-06-28T22:27:18.008463: step 3798, loss 0.182081, acc 0.9375, learning_rate 0.000192494\n","2023-06-28T22:27:18.115153: step 3799, loss 0.035599, acc 0.96875, learning_rate 0.000192403\n","\n","Evaluation:\n","2023-06-28T22:27:18.822255: step 3800, loss 1.36183, acc 0.832204\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3800\n","\n","2023-06-28T22:27:19.007816: step 3800, loss 0.0452234, acc 1, learning_rate 0.000192312\n","2023-06-28T22:27:19.112985: step 3801, loss 0.125142, acc 0.9375, learning_rate 0.000192221\n","2023-06-28T22:27:19.218685: step 3802, loss 0.376562, acc 0.90625, learning_rate 0.000192131\n","2023-06-28T22:27:19.319040: step 3803, loss 0.0575675, acc 1, learning_rate 0.00019204\n","2023-06-28T22:27:19.423485: step 3804, loss 0.036412, acc 1, learning_rate 0.000191949\n","2023-06-28T22:27:19.527560: step 3805, loss 0.123824, acc 0.96875, learning_rate 0.000191859\n","2023-06-28T22:27:19.638727: step 3806, loss 0.215897, acc 0.90625, learning_rate 0.000191768\n","2023-06-28T22:27:19.755572: step 3807, loss 0.120543, acc 0.96875, learning_rate 0.000191678\n","2023-06-28T22:27:19.851059: step 3808, loss 0.0369356, acc 1, learning_rate 0.000191588\n","2023-06-28T22:27:19.945868: step 3809, loss 0.0185226, acc 1, learning_rate 0.000191497\n","2023-06-28T22:27:20.057422: step 3810, loss 0.0569843, acc 1, learning_rate 0.000191407\n","2023-06-28T22:27:20.159599: step 3811, loss 0.292371, acc 0.875, learning_rate 0.000191317\n","2023-06-28T22:27:20.273985: step 3812, loss 0.0524334, acc 1, learning_rate 0.000191227\n","2023-06-28T22:27:20.369055: step 3813, loss 0.584539, acc 0.90625, learning_rate 0.000191138\n","2023-06-28T22:27:20.464145: step 3814, loss 0.018626, acc 1, learning_rate 0.000191048\n","2023-06-28T22:27:20.566240: step 3815, loss 0.0551711, acc 0.96875, learning_rate 0.000190958\n","2023-06-28T22:27:20.664095: step 3816, loss 0.128462, acc 0.96875, learning_rate 0.000190869\n","2023-06-28T22:27:20.766537: step 3817, loss 0.2284, acc 0.90625, learning_rate 0.000190779\n","2023-06-28T22:27:20.873318: step 3818, loss 0.108466, acc 0.9375, learning_rate 0.00019069\n","2023-06-28T22:27:20.967341: step 3819, loss 0.0296271, acc 0.96875, learning_rate 0.000190601\n","2023-06-28T22:27:21.065173: step 3820, loss 0.131697, acc 0.9375, learning_rate 0.000190511\n","2023-06-28T22:27:21.168763: step 3821, loss 0.151022, acc 0.9375, learning_rate 0.000190422\n","2023-06-28T22:27:21.263049: step 3822, loss 0.146513, acc 0.9375, learning_rate 0.000190333\n","2023-06-28T22:27:21.361771: step 3823, loss 0.294457, acc 0.875, learning_rate 0.000190244\n","2023-06-28T22:27:21.467914: step 3824, loss 0.00346517, acc 1, learning_rate 0.000190155\n","2023-06-28T22:27:21.563721: step 3825, loss 0.113795, acc 0.9375, learning_rate 0.000190067\n","2023-06-28T22:27:21.660892: step 3826, loss 0.0700046, acc 0.96875, learning_rate 0.000189978\n","2023-06-28T22:27:21.782254: step 3827, loss 0.119117, acc 0.9375, learning_rate 0.000189889\n","2023-06-28T22:27:21.884117: step 3828, loss 0.131279, acc 0.90625, learning_rate 0.000189801\n","2023-06-28T22:27:21.985294: step 3829, loss 0.0219512, acc 1, learning_rate 0.000189712\n","2023-06-28T22:27:22.090628: step 3830, loss 0.0570726, acc 1, learning_rate 0.000189624\n","2023-06-28T22:27:22.192088: step 3831, loss 0.140556, acc 0.96875, learning_rate 0.000189536\n","2023-06-28T22:27:22.285484: step 3832, loss 0.0526163, acc 1, learning_rate 0.000189448\n","2023-06-28T22:27:22.393017: step 3833, loss 0.173178, acc 0.90625, learning_rate 0.00018936\n","2023-06-28T22:27:22.513767: step 3834, loss 0.175963, acc 0.9375, learning_rate 0.000189272\n","2023-06-28T22:27:22.660952: step 3835, loss 0.221819, acc 0.875, learning_rate 0.000189184\n","2023-06-28T22:27:22.832014: step 3836, loss 0.0575474, acc 1, learning_rate 0.000189096\n","2023-06-28T22:27:23.019887: step 3837, loss 0.284821, acc 0.8125, learning_rate 0.000189008\n","2023-06-28T22:27:23.228361: step 3838, loss 0.057574, acc 1, learning_rate 0.00018892\n","2023-06-28T22:27:23.400107: step 3839, loss 0.0481981, acc 0.96875, learning_rate 0.000188833\n","2023-06-28T22:27:23.570756: step 3840, loss 0.153217, acc 0.9375, learning_rate 0.000188745\n","2023-06-28T22:27:23.742080: step 3841, loss 0.0478305, acc 1, learning_rate 0.000188658\n","2023-06-28T22:27:23.919821: step 3842, loss 0.0553712, acc 1, learning_rate 0.000188571\n","2023-06-28T22:27:24.097471: step 3843, loss 0.0719333, acc 0.96875, learning_rate 0.000188484\n","2023-06-28T22:27:24.272303: step 3844, loss 0.103527, acc 1, learning_rate 0.000188396\n","2023-06-28T22:27:24.447460: step 3845, loss 0.0944897, acc 0.96875, learning_rate 0.000188309\n","2023-06-28T22:27:24.626246: step 3846, loss 0.137126, acc 0.96875, learning_rate 0.000188222\n","2023-06-28T22:27:24.794194: step 3847, loss 0.204801, acc 0.96875, learning_rate 0.000188136\n","2023-06-28T22:27:24.970864: step 3848, loss 0.456015, acc 0.9375, learning_rate 0.000188049\n","2023-06-28T22:27:25.144218: step 3849, loss 0.115608, acc 0.9375, learning_rate 0.000187962\n","2023-06-28T22:27:25.313050: step 3850, loss 0.0432243, acc 1, learning_rate 0.000187875\n","2023-06-28T22:27:25.473099: step 3851, loss 0.106444, acc 0.9375, learning_rate 0.000187789\n","2023-06-28T22:27:25.646362: step 3852, loss 0.0842012, acc 0.9375, learning_rate 0.000187702\n","2023-06-28T22:27:25.819964: step 3853, loss 0.196386, acc 0.90625, learning_rate 0.000187616\n","2023-06-28T22:27:25.996976: step 3854, loss 0.17131, acc 0.96875, learning_rate 0.00018753\n","2023-06-28T22:27:26.184595: step 3855, loss 0.214865, acc 0.90625, learning_rate 0.000187444\n","2023-06-28T22:27:26.354492: step 3856, loss 0.0527691, acc 0.96875, learning_rate 0.000187358\n","2023-06-28T22:27:26.524590: step 3857, loss 0.132239, acc 0.9375, learning_rate 0.000187272\n","2023-06-28T22:27:26.699918: step 3858, loss 0.0944786, acc 0.9375, learning_rate 0.000187186\n","2023-06-28T22:27:26.882048: step 3859, loss 0.13526, acc 0.96875, learning_rate 0.0001871\n","2023-06-28T22:27:27.055742: step 3860, loss 0.181322, acc 0.9375, learning_rate 0.000187014\n","2023-06-28T22:27:27.233511: step 3861, loss 0.612315, acc 0.9375, learning_rate 0.000186928\n","2023-06-28T22:27:27.405888: step 3862, loss 0.0866305, acc 1, learning_rate 0.000186843\n","2023-06-28T22:27:27.579751: step 3863, loss 0.506746, acc 0.90625, learning_rate 0.000186757\n","2023-06-28T22:27:27.761932: step 3864, loss 0.229548, acc 0.875, learning_rate 0.000186672\n","2023-06-28T22:27:27.920051: step 3865, loss 0.402066, acc 0.96875, learning_rate 0.000186586\n","2023-06-28T22:27:28.075022: step 3866, loss 0.0821341, acc 0.96875, learning_rate 0.000186501\n","2023-06-28T22:27:28.251857: step 3867, loss 0.0522887, acc 1, learning_rate 0.000186416\n","2023-06-28T22:27:28.409183: step 3868, loss 0.0920989, acc 0.9375, learning_rate 0.000186331\n","2023-06-28T22:27:28.570754: step 3869, loss 0.154406, acc 0.9375, learning_rate 0.000186246\n","2023-06-28T22:27:28.745044: step 3870, loss 0.169882, acc 0.90625, learning_rate 0.000186161\n","2023-06-28T22:27:28.911480: step 3871, loss 0.0139502, acc 1, learning_rate 0.000186076\n","2023-06-28T22:27:29.090347: step 3872, loss 0.294996, acc 0.90625, learning_rate 0.000185991\n","2023-06-28T22:27:29.262711: step 3873, loss 0.196928, acc 0.90625, learning_rate 0.000185907\n","2023-06-28T22:27:29.425394: step 3874, loss 0.205851, acc 0.9375, learning_rate 0.000185822\n","2023-06-28T22:27:29.575279: step 3875, loss 0.0713897, acc 0.96875, learning_rate 0.000185738\n","2023-06-28T22:27:29.758174: step 3876, loss 0.108481, acc 0.9375, learning_rate 0.000185653\n","2023-06-28T22:27:29.913830: step 3877, loss 0.542049, acc 0.9375, learning_rate 0.000185569\n","2023-06-28T22:27:30.084374: step 3878, loss 0.123573, acc 0.96875, learning_rate 0.000185485\n","2023-06-28T22:27:30.241654: step 3879, loss 0.136008, acc 0.875, learning_rate 0.0001854\n","2023-06-28T22:27:30.395333: step 3880, loss 0.678523, acc 0.90625, learning_rate 0.000185316\n","2023-06-28T22:27:30.554585: step 3881, loss 0.304626, acc 0.9375, learning_rate 0.000185232\n","2023-06-28T22:27:30.714527: step 3882, loss 0.031497, acc 1, learning_rate 0.000185148\n","2023-06-28T22:27:30.890699: step 3883, loss 0.429944, acc 0.90625, learning_rate 0.000185065\n","2023-06-28T22:27:31.051560: step 3884, loss 0.173902, acc 0.9375, learning_rate 0.000184981\n","2023-06-28T22:27:31.219487: step 3885, loss 0.130896, acc 0.90625, learning_rate 0.000184897\n","2023-06-28T22:27:31.376457: step 3886, loss 0.125841, acc 0.9375, learning_rate 0.000184814\n","2023-06-28T22:27:31.538287: step 3887, loss 0.0893844, acc 1, learning_rate 0.00018473\n","2023-06-28T22:27:31.712019: step 3888, loss 0.159898, acc 0.9375, learning_rate 0.000184647\n","2023-06-28T22:27:31.881305: step 3889, loss 0.0601822, acc 0.96875, learning_rate 0.000184563\n","2023-06-28T22:27:32.049169: step 3890, loss 0.24586, acc 0.9375, learning_rate 0.00018448\n","2023-06-28T22:27:32.221875: step 3891, loss 0.201832, acc 0.96875, learning_rate 0.000184397\n","2023-06-28T22:27:32.389200: step 3892, loss 0.261491, acc 0.90625, learning_rate 0.000184314\n","2023-06-28T22:27:32.550303: step 3893, loss 0.136118, acc 0.9375, learning_rate 0.000184231\n","2023-06-28T22:27:32.712755: step 3894, loss 0.215526, acc 0.9375, learning_rate 0.000184148\n","2023-06-28T22:27:32.894542: step 3895, loss 0.048252, acc 1, learning_rate 0.000184065\n","2023-06-28T22:27:33.052325: step 3896, loss 0.0249828, acc 1, learning_rate 0.000183982\n","2023-06-28T22:27:33.227623: step 3897, loss 0.09379, acc 0.9375, learning_rate 0.000183899\n","2023-06-28T22:27:33.393237: step 3898, loss 0.0449702, acc 1, learning_rate 0.000183817\n","2023-06-28T22:27:33.488455: step 3899, loss 0.28802, acc 0.875, learning_rate 0.000183734\n","\n","Evaluation:\n","2023-06-28T22:27:34.180038: step 3900, loss 1.21642, acc 0.833128\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-3900\n","\n","2023-06-28T22:27:34.388163: step 3900, loss 0.0811548, acc 0.96875, learning_rate 0.000183652\n","2023-06-28T22:27:34.496686: step 3901, loss 0.00397307, acc 1, learning_rate 0.000183569\n","2023-06-28T22:27:34.589855: step 3902, loss 0.0815098, acc 0.96875, learning_rate 0.000183487\n","2023-06-28T22:27:34.692274: step 3903, loss 0.191219, acc 0.96875, learning_rate 0.000183405\n","2023-06-28T22:27:34.801361: step 3904, loss 0.123246, acc 0.9375, learning_rate 0.000183323\n","2023-06-28T22:27:34.904689: step 3905, loss 0.239563, acc 0.875, learning_rate 0.000183241\n","2023-06-28T22:27:35.017060: step 3906, loss 0.064675, acc 0.96875, learning_rate 0.000183159\n","2023-06-28T22:27:35.120430: step 3907, loss 0.132541, acc 0.90625, learning_rate 0.000183077\n","2023-06-28T22:27:35.218115: step 3908, loss 0.178272, acc 0.90625, learning_rate 0.000182995\n","2023-06-28T22:27:35.358582: step 3909, loss 0.238664, acc 0.90625, learning_rate 0.000182913\n","2023-06-28T22:27:35.454857: step 3910, loss 0.247895, acc 0.875, learning_rate 0.000182832\n","2023-06-28T22:27:35.556405: step 3911, loss 0.177448, acc 0.96875, learning_rate 0.00018275\n","2023-06-28T22:27:35.664959: step 3912, loss 0.07926, acc 0.96875, learning_rate 0.000182669\n","2023-06-28T22:27:35.774431: step 3913, loss 0.165254, acc 0.96875, learning_rate 0.000182587\n","2023-06-28T22:27:35.867796: step 3914, loss 0.0208821, acc 1, learning_rate 0.000182506\n","2023-06-28T22:27:35.971616: step 3915, loss 0.00660515, acc 1, learning_rate 0.000182425\n","2023-06-28T22:27:36.066217: step 3916, loss 0.151985, acc 0.9375, learning_rate 0.000182344\n","2023-06-28T22:27:36.191484: step 3917, loss 0.0227321, acc 1, learning_rate 0.000182263\n","2023-06-28T22:27:36.289823: step 3918, loss 0.0997869, acc 0.96875, learning_rate 0.000182182\n","2023-06-28T22:27:36.394172: step 3919, loss 0.0428293, acc 1, learning_rate 0.000182101\n","2023-06-28T22:27:36.505114: step 3920, loss 0.134479, acc 0.9375, learning_rate 0.00018202\n","2023-06-28T22:27:36.601191: step 3921, loss 0.161344, acc 0.9375, learning_rate 0.000181939\n","2023-06-28T22:27:36.698482: step 3922, loss 0.120569, acc 0.96875, learning_rate 0.000181858\n","2023-06-28T22:27:36.800966: step 3923, loss 0.049166, acc 1, learning_rate 0.000181778\n","2023-06-28T22:27:36.898149: step 3924, loss 0.737551, acc 0.9375, learning_rate 0.000181697\n","2023-06-28T22:27:36.993692: step 3925, loss 0.13212, acc 0.9375, learning_rate 0.000181617\n","2023-06-28T22:27:37.095477: step 3926, loss 0.0951393, acc 0.96875, learning_rate 0.000181536\n","2023-06-28T22:27:37.199827: step 3927, loss 0.0518253, acc 1, learning_rate 0.000181456\n","2023-06-28T22:27:37.294853: step 3928, loss 0.0321961, acc 1, learning_rate 0.000181376\n","2023-06-28T22:27:37.409631: step 3929, loss 0.802471, acc 0.90625, learning_rate 0.000181296\n","2023-06-28T22:27:37.503287: step 3930, loss 0.165874, acc 0.9375, learning_rate 0.000181216\n","2023-06-28T22:27:37.592363: step 3931, loss 0.0742303, acc 1, learning_rate 0.000181136\n","2023-06-28T22:27:37.697023: step 3932, loss 0.0850562, acc 0.9375, learning_rate 0.000181056\n","2023-06-28T22:27:37.790747: step 3933, loss 0.0630068, acc 1, learning_rate 0.000180976\n","2023-06-28T22:27:37.896144: step 3934, loss 0.0220471, acc 1, learning_rate 0.000180896\n","2023-06-28T22:27:37.999418: step 3935, loss 0.187527, acc 0.9375, learning_rate 0.000180817\n","2023-06-28T22:27:38.100359: step 3936, loss 0.115583, acc 0.9375, learning_rate 0.000180737\n","2023-06-28T22:27:38.206930: step 3937, loss 0.163879, acc 0.9375, learning_rate 0.000180658\n","2023-06-28T22:27:38.301397: step 3938, loss 0.276283, acc 0.875, learning_rate 0.000180578\n","2023-06-28T22:27:38.406270: step 3939, loss 0.467, acc 0.9375, learning_rate 0.000180499\n","2023-06-28T22:27:38.508618: step 3940, loss 0.40255, acc 0.875, learning_rate 0.00018042\n","2023-06-28T22:27:38.606911: step 3941, loss 0.0869049, acc 0.9375, learning_rate 0.00018034\n","2023-06-28T22:27:38.711725: step 3942, loss 0.0465029, acc 1, learning_rate 0.000180261\n","2023-06-28T22:27:38.813304: step 3943, loss 0.192438, acc 0.9375, learning_rate 0.000180182\n","2023-06-28T22:27:38.918131: step 3944, loss 0.288175, acc 0.84375, learning_rate 0.000180103\n","2023-06-28T22:27:39.024944: step 3945, loss 0.0709208, acc 1, learning_rate 0.000180024\n","2023-06-28T22:27:39.123158: step 3946, loss 0.548166, acc 0.9375, learning_rate 0.000179946\n","2023-06-28T22:27:39.222513: step 3947, loss 0.125156, acc 0.9375, learning_rate 0.000179867\n","2023-06-28T22:27:39.356973: step 3948, loss 0.0482254, acc 1, learning_rate 0.000179788\n","2023-06-28T22:27:39.488599: step 3949, loss 0.0603042, acc 0.96875, learning_rate 0.00017971\n","2023-06-28T22:27:39.588719: step 3950, loss 0.115846, acc 0.96875, learning_rate 0.000179631\n","2023-06-28T22:27:39.684950: step 3951, loss 0.10473, acc 0.9375, learning_rate 0.000179553\n","2023-06-28T22:27:39.781892: step 3952, loss 0.436221, acc 0.9375, learning_rate 0.000179474\n","2023-06-28T22:27:39.884175: step 3953, loss 0.0122411, acc 1, learning_rate 0.000179396\n","2023-06-28T22:27:39.987778: step 3954, loss 0.180072, acc 0.9375, learning_rate 0.000179318\n","2023-06-28T22:27:40.096814: step 3955, loss 0.16384, acc 0.90625, learning_rate 0.00017924\n","2023-06-28T22:27:40.201351: step 3956, loss 0.249407, acc 0.9375, learning_rate 0.000179162\n","2023-06-28T22:27:40.297460: step 3957, loss 0.0858414, acc 0.9375, learning_rate 0.000179084\n","2023-06-28T22:27:40.406257: step 3958, loss 0.0434866, acc 0.96875, learning_rate 0.000179006\n","2023-06-28T22:27:40.535978: step 3959, loss 0.176862, acc 0.90625, learning_rate 0.000178928\n","2023-06-28T22:27:40.642208: step 3960, loss 0.118676, acc 0.96875, learning_rate 0.000178851\n","2023-06-28T22:27:40.745061: step 3961, loss 0.600915, acc 0.9375, learning_rate 0.000178773\n","2023-06-28T22:27:40.846966: step 3962, loss 0.165972, acc 0.9375, learning_rate 0.000178695\n","2023-06-28T22:27:40.960584: step 3963, loss 0.510658, acc 0.9375, learning_rate 0.000178618\n","2023-06-28T22:27:41.062230: step 3964, loss 0.19247, acc 0.90625, learning_rate 0.00017854\n","2023-06-28T22:27:41.164326: step 3965, loss 0.00858571, acc 1, learning_rate 0.000178463\n","2023-06-28T22:27:41.271425: step 3966, loss 0.00153475, acc 1, learning_rate 0.000178386\n","2023-06-28T22:27:41.374092: step 3967, loss 0.467937, acc 0.9375, learning_rate 0.000178309\n","2023-06-28T22:27:41.491195: step 3968, loss 0.232082, acc 0.90625, learning_rate 0.000178232\n","2023-06-28T22:27:41.596195: step 3969, loss 0.22652, acc 0.875, learning_rate 0.000178155\n","2023-06-28T22:27:41.697504: step 3970, loss 0.280357, acc 0.875, learning_rate 0.000178078\n","2023-06-28T22:27:41.799587: step 3971, loss 0.360052, acc 0.9375, learning_rate 0.000178001\n","2023-06-28T22:27:41.896735: step 3972, loss 0.113081, acc 0.96875, learning_rate 0.000177924\n","2023-06-28T22:27:41.999483: step 3973, loss 0.114471, acc 0.96875, learning_rate 0.000177847\n","2023-06-28T22:27:42.108892: step 3974, loss 0.0469824, acc 1, learning_rate 0.000177771\n","2023-06-28T22:27:42.208183: step 3975, loss 0.0123643, acc 1, learning_rate 0.000177694\n","2023-06-28T22:27:42.310166: step 3976, loss 0.0458781, acc 1, learning_rate 0.000177617\n","2023-06-28T22:27:42.407283: step 3977, loss 0.0284319, acc 1, learning_rate 0.000177541\n","2023-06-28T22:27:42.514351: step 3978, loss 0.0181161, acc 1, learning_rate 0.000177465\n","2023-06-28T22:27:42.647378: step 3979, loss 0.037636, acc 0.96875, learning_rate 0.000177388\n","2023-06-28T22:27:42.753936: step 3980, loss 0.0455895, acc 1, learning_rate 0.000177312\n","2023-06-28T22:27:42.850712: step 3981, loss 0.314247, acc 0.9375, learning_rate 0.000177236\n","2023-06-28T22:27:42.961880: step 3982, loss 0.193957, acc 0.90625, learning_rate 0.00017716\n","2023-06-28T22:27:43.055037: step 3983, loss 0.199767, acc 0.9375, learning_rate 0.000177084\n","2023-06-28T22:27:43.154849: step 3984, loss 0.342926, acc 0.9375, learning_rate 0.000177008\n","2023-06-28T22:27:43.259528: step 3985, loss 0.133178, acc 0.9375, learning_rate 0.000176932\n","2023-06-28T22:27:43.357742: step 3986, loss 0.10497, acc 0.96875, learning_rate 0.000176857\n","2023-06-28T22:27:43.502242: step 3987, loss 0.0393336, acc 1, learning_rate 0.000176781\n","2023-06-28T22:27:43.650555: step 3988, loss 0.114212, acc 0.96875, learning_rate 0.000176705\n","2023-06-28T22:27:43.805561: step 3989, loss 0.0320743, acc 1, learning_rate 0.00017663\n","2023-06-28T22:27:43.979378: step 3990, loss 0.234585, acc 0.9375, learning_rate 0.000176554\n","2023-06-28T22:27:44.170686: step 3991, loss 0.132433, acc 0.9375, learning_rate 0.000176479\n","2023-06-28T22:27:44.343165: step 3992, loss 0.0990125, acc 0.96875, learning_rate 0.000176404\n","2023-06-28T22:27:44.518153: step 3993, loss 0.545442, acc 0.9375, learning_rate 0.000176328\n","2023-06-28T22:27:44.695642: step 3994, loss 0.0327469, acc 1, learning_rate 0.000176253\n","2023-06-28T22:27:44.881011: step 3995, loss 0.0453505, acc 0.96875, learning_rate 0.000176178\n","2023-06-28T22:27:45.051428: step 3996, loss 0.0929016, acc 0.96875, learning_rate 0.000176103\n","2023-06-28T22:27:45.227925: step 3997, loss 0.593878, acc 0.9375, learning_rate 0.000176028\n","2023-06-28T22:27:45.384445: step 3998, loss 0.157587, acc 0.9375, learning_rate 0.000175953\n","2023-06-28T22:27:45.561462: step 3999, loss 0.323641, acc 0.9375, learning_rate 0.000175879\n","\n","Evaluation:\n","2023-06-28T22:27:46.915966: step 4000, loss 1.2899, acc 0.831589\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/lightweighted-cnn/train/runs/1687990696/checkpoints/model-4000\n","\n","2023-06-28T22:27:47.227395: step 4000, loss 0.281982, acc 0.875, learning_rate 0.000175804\n","2023-06-28T22:27:47.398338: step 4001, loss 0.0686289, acc 0.96875, learning_rate 0.000175729\n","2023-06-28T22:27:47.568390: step 4002, loss 0.0447041, acc 0.96875, learning_rate 0.000175655\n","2023-06-28T22:27:47.721155: step 4003, loss 0.20419, acc 0.90625, learning_rate 0.00017558\n","2023-06-28T22:27:47.875999: step 4004, loss 0.27206, acc 0.90625, learning_rate 0.000175506\n","2023-06-28T22:27:48.038312: step 4005, loss 0.15242, acc 0.9375, learning_rate 0.000175431\n","2023-06-28T22:27:48.194288: step 4006, loss 0.0725282, acc 0.96875, learning_rate 0.000175357\n","2023-06-28T22:27:48.338807: step 4007, loss 0.311288, acc 0.9375, learning_rate 0.000175283\n","2023-06-28T22:27:48.514959: step 4008, loss 0.141612, acc 0.96875, learning_rate 0.000175209\n","2023-06-28T22:27:48.667410: step 4009, loss 0.33287, acc 0.9375, learning_rate 0.000175135\n","2023-06-28T22:27:48.832178: step 4010, loss 0.0300908, acc 1, learning_rate 0.000175061\n","2023-06-28T22:27:48.994720: step 4011, loss 0.0882239, acc 0.96875, learning_rate 0.000174987\n","2023-06-28T22:27:49.140740: step 4012, loss 0.0880761, acc 0.96875, learning_rate 0.000174913\n","2023-06-28T22:27:49.318899: step 4013, loss 0.181091, acc 0.9375, learning_rate 0.000174839\n","2023-06-28T22:27:49.474229: step 4014, loss 0.0927713, acc 0.96875, learning_rate 0.000174765\n","2023-06-28T22:27:49.646859: step 4015, loss 0.101754, acc 0.9375, learning_rate 0.000174692\n","2023-06-28T22:27:49.812032: step 4016, loss 0.0192041, acc 1, learning_rate 0.000174618\n","2023-06-28T22:27:49.958021: step 4017, loss 0.086044, acc 0.96875, learning_rate 0.000174545\n","2023-06-28T22:27:50.099178: step 4018, loss 0.00987437, acc 1, learning_rate 0.000174471\n","2023-06-28T22:27:50.281365: step 4019, loss 0.0925036, acc 0.96875, learning_rate 0.000174398\n","2023-06-28T22:27:50.426431: step 4020, loss 0.113689, acc 0.9375, learning_rate 0.000174325\n","2023-06-28T22:27:50.584941: step 4021, loss 0.100929, acc 0.96875, learning_rate 0.000174252\n","2023-06-28T22:27:50.752838: step 4022, loss 0.296903, acc 0.90625, learning_rate 0.000174179\n","2023-06-28T22:27:50.924372: step 4023, loss 0.264199, acc 0.96875, learning_rate 0.000174106\n","2023-06-28T22:27:51.086507: step 4024, loss 0.0211703, acc 1, learning_rate 0.000174033\n","2023-06-28T22:27:51.255503: step 4025, loss 0.704062, acc 0.96875, learning_rate 0.00017396\n","2023-06-28T22:27:51.412551: step 4026, loss 0.212968, acc 0.9375, learning_rate 0.000173887\n","2023-06-28T22:27:51.585079: step 4027, loss 0.3805, acc 0.9375, learning_rate 0.000173814\n","2023-06-28T22:27:51.741932: step 4028, loss 0.0274861, acc 1, learning_rate 0.000173741\n","2023-06-28T22:27:51.915714: step 4029, loss 0.0764667, acc 0.96875, learning_rate 0.000173669\n","2023-06-28T22:27:52.082724: step 4030, loss 0.233445, acc 0.9375, learning_rate 0.000173596\n","2023-06-28T22:27:52.250801: step 4031, loss 0.0885474, acc 0.96875, learning_rate 0.000173524\n","2023-06-28T22:27:52.413434: step 4032, loss 0.263407, acc 0.84375, learning_rate 0.000173451\n","2023-06-28T22:27:52.572806: step 4033, loss 0.0425043, acc 1, learning_rate 0.000173379\n","2023-06-28T22:27:52.708345: step 4034, loss 0.0256788, acc 0.96875, learning_rate 0.000173307\n","2023-06-28T22:27:52.865360: step 4035, loss 0.311631, acc 0.9375, learning_rate 0.000173235\n","2023-06-28T22:27:53.014819: step 4036, loss 0.0552972, acc 0.96875, learning_rate 0.000173163\n","2023-06-28T22:27:53.187730: step 4037, loss 0.126248, acc 0.96875, learning_rate 0.00017309\n","2023-06-28T22:27:53.343751: step 4038, loss 0.135087, acc 1, learning_rate 0.000173019\n","2023-06-28T22:27:53.511475: step 4039, loss 0.126008, acc 0.96875, learning_rate 0.000172947\n","2023-06-28T22:27:53.677633: step 4040, loss 0.0160199, acc 1, learning_rate 0.000172875\n","2023-06-28T22:27:53.842310: step 4041, loss 0.169299, acc 0.96875, learning_rate 0.000172803\n","2023-06-28T22:27:54.008305: step 4042, loss 0.438752, acc 0.90625, learning_rate 0.000172731\n","2023-06-28T22:27:54.149078: step 4043, loss 0.218439, acc 0.875, learning_rate 0.00017266\n","2023-06-28T22:27:54.255861: step 4044, loss 0.0727784, acc 0.96875, learning_rate 0.000172588\n","2023-06-28T22:27:54.349978: step 4045, loss 0.236735, acc 0.875, learning_rate 0.000172517\n","2023-06-28T22:27:54.465416: step 4046, loss 0.137584, acc 0.9375, learning_rate 0.000172445\n","2023-06-28T22:27:54.570555: step 4047, loss 0.0823716, acc 0.9375, learning_rate 0.000172374\n","2023-06-28T22:27:54.664726: step 4048, loss 0.142052, acc 0.9375, learning_rate 0.000172303\n","2023-06-28T22:27:54.756912: step 4049, loss 0.0822066, acc 0.96875, learning_rate 0.000172232\n","2023-06-28T22:27:54.863320: step 4050, loss 0.205897, acc 0.9375, learning_rate 0.00017216\n","2023-06-28T22:27:54.965237: step 4051, loss 0.182607, acc 0.9375, learning_rate 0.000172089\n","2023-06-28T22:27:55.076890: step 4052, loss 0.132711, acc 0.96875, learning_rate 0.000172018\n","2023-06-28T22:27:55.177100: step 4053, loss 0.0915977, acc 0.96875, learning_rate 0.000171947\n","2023-06-28T22:27:55.278679: step 4054, loss 0.0911767, acc 0.96875, learning_rate 0.000171877\n","2023-06-28T22:27:55.382099: step 4055, loss 0.446216, acc 0.9375, learning_rate 0.000171806\n","2023-06-28T22:27:55.481667: step 4056, loss 0.431439, acc 0.875, learning_rate 0.000171735\n","2023-06-28T22:27:55.575300: step 4057, loss 0.183533, acc 0.875, learning_rate 0.000171664\n","2023-06-28T22:27:55.671723: step 4058, loss 0.0879611, acc 0.96875, learning_rate 0.000171594\n","2023-06-28T22:27:55.771057: step 4059, loss 0.130936, acc 0.9375, learning_rate 0.000171523\n","2023-06-28T22:27:55.871204: step 4060, loss 0.122823, acc 0.9375, learning_rate 0.000171453\n","2023-06-28T22:27:55.972488: step 4061, loss 0.00505316, acc 1, learning_rate 0.000171383\n","2023-06-28T22:27:56.098533: step 4062, loss 0.448001, acc 0.90625, learning_rate 0.000171312\n","2023-06-28T22:27:56.208060: step 4063, loss 0.111229, acc 0.96875, learning_rate 0.000171242\n","2023-06-28T22:27:56.306810: step 4064, loss 0.0717275, acc 1, learning_rate 0.000171172\n","2023-06-28T22:27:56.404288: step 4065, loss 0.190635, acc 0.875, learning_rate 0.000171102\n","2023-06-28T22:27:56.508968: step 4066, loss 0.134439, acc 0.96875, learning_rate 0.000171032\n","2023-06-28T22:27:56.608592: step 4067, loss 0.151428, acc 0.96875, learning_rate 0.000170962\n","2023-06-28T22:27:56.713117: step 4068, loss 0.0665238, acc 1, learning_rate 0.000170892\n","2023-06-28T22:27:56.776949: step 4069, loss 0.000765343, acc 1, learning_rate 0.000170822\n"]}],"source":["! python3.6 Nadamax_optmized_train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1687986493229,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"PFQNE40h4YB1","outputId":"fbb44f74-1bdb-4e0b-ff7d-f98767bd97e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/lightweighted-cnn/train\n"]}],"source":["cd output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1687986258355,"user":{"displayName":"Milad Mahaki","userId":"06106732680968882418"},"user_tz":-210},"id":"VikXys4o4aGt","outputId":"3b9f5027-c687-498b-d41f-f7ab2300f14c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# باز کردن فایل و خواندن داده‌ها\n","data = np.loadtxt('Optimized-Text-CNN-Nadamax.txt', delimiter=',')\n","\n","# استخراج ستون‌های مختلف\n","steps = data[:, 0]\n","# loss = data[:, 1]\n","acc = data[:, 2]\n","\n","# تعریف نمودار\n","fig, ax = plt.subplots()\n","\n","# رسم منحنی‌ها\n","# ax.plot(steps, loss, label='Loss')\n","ax.plot(steps, acc, label='Accuracy')\n","\n","\n","# تنظیمات محور y\n","ax.set_yticks(np.arange(0, 1.5, 0.1))\n","\n","# شطرنجی کردن پس زمینه نمودار\n","# ax.set_facecolor('white')\n","# hatch_pattern = '/\\\\\\\\'\n","# ax.patch.set_hatch(hatch_pattern*10)\n","\n","# تنظیمات نمودار\n","ax.set_xlabel('Steps')\n","ax.set_ylabel('Accuracy')\n","ax.legend()\n","\n","# نمایش نمودار\n","plt.show()\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}