{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNdD0hgqpwKxjwR28aQ9J88"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9E0LqR3wQon1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664210298,"user_tz":-210,"elapsed":25113,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"1d10b6d1-09c3-48df-bbb2-e3b3a8e35d90"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["cd gdrive/MyDrive/LightWeight/train"],"metadata":{"id":"k8ZCjucPQ5Al","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664210299,"user_tz":-210,"elapsed":13,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"f654db02-a776-4700-f8e7-3d94b72f40ab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/LightWeight/train\n"]}]},{"cell_type":"code","source":["!apt-get install python3.6"],"metadata":{"id":"1eJqVQ8cQ74b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664219129,"user_tz":-210,"elapsed":8837,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"5daf5497-9670-460f-c311-0620e1c27ebf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6-minimal\n","Suggested packages:\n","  python3.6-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.6-minimal libpython3.6-stdlib python3.6 python3.6-minimal\n","0 upgraded, 4 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 4,294 kB of archives.\n","After this operation, 22.1 MB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-minimal amd64 3.6.15-1+focal3 [569 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-minimal amd64 3.6.15-1+focal3 [1,718 kB]\n","Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 libpython3.6-stdlib amd64 3.6.15-1+focal3 [1,758 kB]\n","Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6 amd64 3.6.15-1+focal3 [248 kB]\n","Fetched 4,294 kB in 2s (1,906 kB/s)\n","Selecting previously unselected package libpython3.6-minimal:amd64.\n","(Reading database ... 122541 files and directories currently installed.)\n","Preparing to unpack .../libpython3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-minimal.\n","Preparing to unpack .../python3.6-minimal_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6-minimal (3.6.15-1+focal3) ...\n","Selecting previously unselected package libpython3.6-stdlib:amd64.\n","Preparing to unpack .../libpython3.6-stdlib_3.6.15-1+focal3_amd64.deb ...\n","Unpacking libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6.\n","Preparing to unpack .../python3.6_3.6.15-1+focal3_amd64.deb ...\n","Unpacking python3.6 (3.6.15-1+focal3) ...\n","Setting up libpython3.6-minimal:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6-minimal (3.6.15-1+focal3) ...\n","Setting up libpython3.6-stdlib:amd64 (3.6.15-1+focal3) ...\n","Setting up python3.6 (3.6.15-1+focal3) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n"]}]},{"cell_type":"code","source":["!sudo apt-get install python3.6-distutils"],"metadata":{"id":"U_cocIBoQ-Ql","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664223054,"user_tz":-210,"elapsed":3940,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"dbb57143-1757-4c9b-c53a-a382e2a1bbc4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  python3.6-lib2to3\n","The following NEW packages will be installed:\n","  python3.6-distutils python3.6-lib2to3\n","0 upgraded, 2 newly installed, 0 to remove and 38 not upgraded.\n","Need to get 308 kB of archives.\n","After this operation, 1,232 kB of additional disk space will be used.\n","Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-lib2to3 all 3.6.15-1+focal3 [122 kB]\n","Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 python3.6-distutils all 3.6.15-1+focal3 [187 kB]\n","Fetched 308 kB in 1s (394 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package python3.6-lib2to3.\n","(Reading database ... 123152 files and directories currently installed.)\n","Preparing to unpack .../python3.6-lib2to3_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Selecting previously unselected package python3.6-distutils.\n","Preparing to unpack .../python3.6-distutils_3.6.15-1+focal3_all.deb ...\n","Unpacking python3.6-distutils (3.6.15-1+focal3) ...\n","Setting up python3.6-lib2to3 (3.6.15-1+focal3) ...\n","Setting up python3.6-distutils (3.6.15-1+focal3) ...\n"]}]},{"cell_type":"code","source":["!curl https://bootstrap.pypa.io/pip/3.6/get-pip.py -o get-pip.py"],"metadata":{"id":"N4zQLMcYRB1z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664223690,"user_tz":-210,"elapsed":655,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"60106405-abee-4271-9d1e-4fd60d12b676"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2108k  100 2108k    0     0  5092k      0 --:--:-- --:--:-- --:--:-- 5080k\n"]}]},{"cell_type":"code","source":["!python3.6 get-pip.py"],"metadata":{"id":"1w2p27g1RGWB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664228330,"user_tz":-210,"elapsed":4648,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"a6ec4b06-41ec-42e9-a218-8f19afc18b12"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pip<22.0\n","  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n","     |████████████████████████████████| 1.7 MB 19.6 MB/s            \n","\u001b[?25hCollecting setuptools\n","  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n","     |████████████████████████████████| 952 kB 65.6 MB/s            \n","\u001b[?25hCollecting wheel\n","  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n","Installing collected packages: wheel, setuptools, pip\n","Successfully installed pip-21.3.1 setuptools-59.6.0 wheel-0.37.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip3.6 install tensorflow==1.14.0"],"metadata":{"id":"NUB3fMY5RIzT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664263677,"user_tz":-210,"elapsed":35358,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"e82273da-8b7d-4eca-ee83-532717f2df2d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==1.14.0\n","  Downloading tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2 MB)\n","     |████████████████████████████████| 109.2 MB 4.6 kB/s            \n","\u001b[?25hCollecting gast>=0.2.0\n","  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n","Collecting astor>=0.6.0\n","  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n","Collecting keras-preprocessing>=1.0.5\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","     |████████████████████████████████| 42 kB 380 kB/s             \n","\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n","  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n","     |████████████████████████████████| 3.1 MB 53.4 MB/s            \n","\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n","     |████████████████████████████████| 488 kB 55.0 MB/s            \n","\u001b[?25hCollecting wrapt>=1.11.1\n","  Downloading wrapt-1.15.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n","     |████████████████████████████████| 75 kB 4.1 MB/s             \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.37.1)\n","Collecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","     |████████████████████████████████| 50 kB 8.5 MB/s             \n","\u001b[?25hCollecting termcolor>=1.1.0\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting google-pasta>=0.1.6\n","  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","     |████████████████████████████████| 57 kB 6.4 MB/s             \n","\u001b[?25hCollecting absl-py>=0.7.0\n","  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n","     |████████████████████████████████| 126 kB 82.8 MB/s            \n","\u001b[?25hCollecting grpcio>=1.8.6\n","  Downloading grpcio-1.48.2-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","     |████████████████████████████████| 4.6 MB 66.9 MB/s            \n","\u001b[?25hCollecting numpy<2.0,>=1.14.5\n","  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n","     |████████████████████████████████| 14.8 MB 72.4 MB/s            \n","\u001b[?25hCollecting protobuf>=3.6.1\n","  Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","     |████████████████████████████████| 1.1 MB 70.8 MB/s            \n","\u001b[?25hCollecting six>=1.10.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting h5py\n","  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n","     |████████████████████████████████| 4.0 MB 71.0 MB/s            \n","\u001b[?25hCollecting werkzeug>=0.11.15\n","  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n","     |████████████████████████████████| 289 kB 78.9 MB/s            \n","\u001b[?25hCollecting markdown>=2.6.8\n","  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n","     |████████████████████████████████| 97 kB 8.1 MB/s             \n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (59.6.0)\n","Collecting importlib-metadata>=4.4\n","  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n","Collecting dataclasses\n","  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n","Collecting cached-property\n","  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n","Collecting typing-extensions>=3.6.4\n","  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n","Building wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=fd538a0fec5299e243ea0d58dcf0516e79f8a0ff24a7969322f6b466fb928340\n","  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n","Successfully built termcolor\n","Installing collected packages: zipp, typing-extensions, six, numpy, importlib-metadata, dataclasses, cached-property, werkzeug, protobuf, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, keras-preprocessing, keras-applications, google-pasta, gast, astor, tensorflow\n","Successfully installed absl-py-1.4.0 astor-0.8.1 cached-property-1.5.2 dataclasses-0.8 gast-0.5.4 google-pasta-0.2.0 grpcio-1.48.2 h5py-3.1.0 importlib-metadata-4.8.3 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.3.7 numpy-1.19.5 protobuf-3.19.6 six-1.16.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0 typing-extensions-4.1.1 werkzeug-2.0.3 wrapt-1.15.0 zipp-3.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","source":["!pip3.6 install scikit-learn"],"metadata":{"id":"M7eR9D5nRLe2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664276376,"user_tz":-210,"elapsed":12725,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"84952d94-13fc-4267-9418-87a89b3c777b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-learn\n","  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n","     |████████████████████████████████| 22.2 MB 1.2 MB/s             \n","\u001b[?25hCollecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n","Collecting joblib>=0.11\n","  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n","     |████████████████████████████████| 309 kB 65.0 MB/s            \n","\u001b[?25hCollecting scipy>=0.19.1\n","  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n","     |████████████████████████████████| 25.9 MB 54.3 MB/s            \n","\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n","Successfully installed joblib-1.1.1 scikit-learn-0.24.2 scipy-1.5.4 threadpoolctl-3.1.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"metadata":{"id":"vmjwRTyAR736","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664279434,"user_tz":-210,"elapsed":3067,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"2a42a09f-d97e-402c-e825-9b3e9e979f88"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"code","source":["!pip3.6 install pyyaml"],"metadata":{"id":"SmgfRFIfR-ds","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686664280989,"user_tz":-210,"elapsed":1565,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"81abc20d-6689-4e4b-d601-f29b25a5fc1c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n","     |████████████████████████████████| 603 kB 15.9 MB/s            \n","\u001b[?25hInstalling collected packages: pyyaml\n","Successfully installed pyyaml-6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}]},{"cell_type":"code","source":["!python3.6 singleADAM_LW_train.py"],"metadata":{"id":"fw2TGC8fSEPM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686679524181,"user_tz":-210,"elapsed":2494669,"user":{"displayName":"abc abc","userId":"00306039706503801176"}},"outputId":"0cec7695-a2e4-4f17-c970-9e0dcd707bd4"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","2023-06-13T15:16:33.623276: step 2354, loss 1.51748, acc 0.5625, learning_rate 0.0013452\n","2023-06-13T15:16:35.952561: step 2355, loss 1.17408, acc 0.5625, learning_rate 0.0013446\n","2023-06-13T15:16:37.410196: step 2356, loss 1.36923, acc 0.5, learning_rate 0.00134399\n","2023-06-13T15:16:38.902361: step 2357, loss 1.37726, acc 0.4375, learning_rate 0.00134339\n","2023-06-13T15:16:40.339521: step 2358, loss 1.00387, acc 0.625, learning_rate 0.00134279\n","2023-06-13T15:16:41.796601: step 2359, loss 1.39576, acc 0.53125, learning_rate 0.00134219\n","2023-06-13T15:16:43.252089: step 2360, loss 1.1068, acc 0.71875, learning_rate 0.00134158\n","2023-06-13T15:16:44.691266: step 2361, loss 1.56687, acc 0.59375, learning_rate 0.00134098\n","2023-06-13T15:16:46.353529: step 2362, loss 1.2775, acc 0.59375, learning_rate 0.00134038\n","2023-06-13T15:16:48.902449: step 2363, loss 1.24744, acc 0.625, learning_rate 0.00133978\n","2023-06-13T15:16:51.305121: step 2364, loss 1.31522, acc 0.53125, learning_rate 0.00133918\n","2023-06-13T15:16:53.634880: step 2365, loss 0.920709, acc 0.6875, learning_rate 0.00133858\n","2023-06-13T15:16:55.958875: step 2366, loss 1.13574, acc 0.625, learning_rate 0.00133798\n","2023-06-13T15:16:57.490726: step 2367, loss 1.18255, acc 0.625, learning_rate 0.00133738\n","2023-06-13T15:16:58.939640: step 2368, loss 1.56193, acc 0.375, learning_rate 0.00133678\n","2023-06-13T15:17:00.380030: step 2369, loss 1.03151, acc 0.6875, learning_rate 0.00133618\n","2023-06-13T15:17:01.797183: step 2370, loss 1.28283, acc 0.53125, learning_rate 0.00133558\n","2023-06-13T15:17:03.221163: step 2371, loss 1.17785, acc 0.5625, learning_rate 0.00133498\n","2023-06-13T15:17:04.666307: step 2372, loss 1.66548, acc 0.53125, learning_rate 0.00133438\n","2023-06-13T15:17:06.146747: step 2373, loss 1.12321, acc 0.625, learning_rate 0.00133378\n","2023-06-13T15:17:08.485541: step 2374, loss 0.940585, acc 0.625, learning_rate 0.00133318\n","2023-06-13T15:17:10.924336: step 2375, loss 1.29507, acc 0.5, learning_rate 0.00133258\n","2023-06-13T15:17:13.287468: step 2376, loss 1.7213, acc 0.4375, learning_rate 0.00133199\n","2023-06-13T15:17:15.731321: step 2377, loss 1.27343, acc 0.53125, learning_rate 0.00133139\n","2023-06-13T15:17:17.604352: step 2378, loss 1.09159, acc 0.53125, learning_rate 0.00133079\n","2023-06-13T15:17:19.082329: step 2379, loss 1.60075, acc 0.5, learning_rate 0.00133019\n","2023-06-13T15:17:20.525641: step 2380, loss 1.46728, acc 0.53125, learning_rate 0.0013296\n","2023-06-13T15:17:22.015952: step 2381, loss 0.873315, acc 0.625, learning_rate 0.001329\n","2023-06-13T15:17:23.435678: step 2382, loss 1.79918, acc 0.375, learning_rate 0.00132841\n","2023-06-13T15:17:24.874390: step 2383, loss 1.32949, acc 0.5625, learning_rate 0.00132781\n","2023-06-13T15:17:26.311000: step 2384, loss 1.27547, acc 0.5625, learning_rate 0.00132722\n","2023-06-13T15:17:28.477045: step 2385, loss 0.996529, acc 0.65625, learning_rate 0.00132662\n","2023-06-13T15:17:30.999508: step 2386, loss 1.40872, acc 0.59375, learning_rate 0.00132603\n","2023-06-13T15:17:33.305470: step 2387, loss 1.0372, acc 0.625, learning_rate 0.00132543\n","2023-06-13T15:17:35.681144: step 2388, loss 1.41203, acc 0.5, learning_rate 0.00132484\n","2023-06-13T15:17:37.675701: step 2389, loss 1.32702, acc 0.5, learning_rate 0.00132424\n","2023-06-13T15:17:39.124362: step 2390, loss 1.02074, acc 0.6875, learning_rate 0.00132365\n","2023-06-13T15:17:40.554689: step 2391, loss 1.16385, acc 0.53125, learning_rate 0.00132306\n","2023-06-13T15:17:42.008029: step 2392, loss 1.48502, acc 0.53125, learning_rate 0.00132246\n","2023-06-13T15:17:43.441154: step 2393, loss 1.08784, acc 0.5625, learning_rate 0.00132187\n","2023-06-13T15:17:44.877017: step 2394, loss 1.0678, acc 0.53125, learning_rate 0.00132128\n","2023-06-13T15:17:46.290638: step 2395, loss 1.4601, acc 0.59375, learning_rate 0.00132069\n","2023-06-13T15:17:48.188680: step 2396, loss 1.36524, acc 0.5, learning_rate 0.00132009\n","2023-06-13T15:17:50.782472: step 2397, loss 1.43436, acc 0.4375, learning_rate 0.0013195\n","2023-06-13T15:17:53.287498: step 2398, loss 0.960793, acc 0.5625, learning_rate 0.00131891\n","2023-06-13T15:17:55.646035: step 2399, loss 1.38478, acc 0.5, learning_rate 0.00131832\n","\n","Evaluation:\n","2023-06-13T15:18:24.162894: step 2400, loss 1.86908, acc 0.377084\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2400\n","\n","2023-06-13T15:18:25.807636: step 2400, loss 1.37342, acc 0.46875, learning_rate 0.00131773\n","2023-06-13T15:18:27.234237: step 2401, loss 1.37582, acc 0.34375, learning_rate 0.00131714\n","2023-06-13T15:18:28.868782: step 2402, loss 1.52128, acc 0.4375, learning_rate 0.00131655\n","2023-06-13T15:18:31.428865: step 2403, loss 1.15973, acc 0.625, learning_rate 0.00131596\n","2023-06-13T15:18:33.867690: step 2404, loss 1.02494, acc 0.71875, learning_rate 0.00131537\n","2023-06-13T15:18:36.204172: step 2405, loss 0.949829, acc 0.5625, learning_rate 0.00131478\n","2023-06-13T15:18:38.658575: step 2406, loss 1.40569, acc 0.53125, learning_rate 0.00131419\n","2023-06-13T15:18:40.135234: step 2407, loss 1.23831, acc 0.53125, learning_rate 0.0013136\n","2023-06-13T15:18:41.574193: step 2408, loss 1.24459, acc 0.625, learning_rate 0.00131301\n","2023-06-13T15:18:43.021368: step 2409, loss 1.14876, acc 0.46875, learning_rate 0.00131243\n","2023-06-13T15:18:44.449777: step 2410, loss 0.928208, acc 0.75, learning_rate 0.00131184\n","2023-06-13T15:18:45.860084: step 2411, loss 1.448, acc 0.5, learning_rate 0.00131125\n","2023-06-13T15:18:47.279278: step 2412, loss 1.00403, acc 0.75, learning_rate 0.00131066\n","2023-06-13T15:18:48.850232: step 2413, loss 1.28553, acc 0.53125, learning_rate 0.00131008\n","2023-06-13T15:18:51.456004: step 2414, loss 1.2355, acc 0.65625, learning_rate 0.00130949\n","2023-06-13T15:18:53.912957: step 2415, loss 1.42014, acc 0.6875, learning_rate 0.0013089\n","2023-06-13T15:18:56.321923: step 2416, loss 1.01674, acc 0.65625, learning_rate 0.00130832\n","2023-06-13T15:18:58.658322: step 2417, loss 1.12612, acc 0.65625, learning_rate 0.00130773\n","2023-06-13T15:19:00.091436: step 2418, loss 1.05216, acc 0.65625, learning_rate 0.00130714\n","2023-06-13T15:19:01.573190: step 2419, loss 1.27749, acc 0.5, learning_rate 0.00130656\n","2023-06-13T15:19:03.025694: step 2420, loss 1.25978, acc 0.5, learning_rate 0.00130597\n","2023-06-13T15:19:04.450071: step 2421, loss 1.27109, acc 0.5625, learning_rate 0.00130539\n","2023-06-13T15:19:05.888677: step 2422, loss 1.32197, acc 0.59375, learning_rate 0.0013048\n","2023-06-13T15:19:07.332303: step 2423, loss 1.27824, acc 0.53125, learning_rate 0.00130422\n","2023-06-13T15:19:08.948313: step 2424, loss 1.21981, acc 0.5, learning_rate 0.00130364\n","2023-06-13T15:19:11.543823: step 2425, loss 1.13271, acc 0.6875, learning_rate 0.00130305\n","2023-06-13T15:19:13.941748: step 2426, loss 1.36579, acc 0.46875, learning_rate 0.00130247\n","2023-06-13T15:19:16.165996: step 2427, loss 1.22947, acc 0.5, learning_rate 0.00130189\n","2023-06-13T15:19:18.550428: step 2428, loss 1.2027, acc 0.625, learning_rate 0.0013013\n","2023-06-13T15:19:20.162082: step 2429, loss 1.12171, acc 0.59375, learning_rate 0.00130072\n","2023-06-13T15:19:21.632342: step 2430, loss 1.04801, acc 0.625, learning_rate 0.00130014\n","2023-06-13T15:19:23.061782: step 2431, loss 1.42239, acc 0.5625, learning_rate 0.00129956\n","2023-06-13T15:19:24.537920: step 2432, loss 0.880231, acc 0.75, learning_rate 0.00129898\n","2023-06-13T15:19:25.960420: step 2433, loss 1.35029, acc 0.5, learning_rate 0.00129839\n","2023-06-13T15:19:27.452838: step 2434, loss 1.66059, acc 0.59375, learning_rate 0.00129781\n","2023-06-13T15:19:28.880991: step 2435, loss 1.06231, acc 0.71875, learning_rate 0.00129723\n","2023-06-13T15:19:31.295470: step 2436, loss 1.26421, acc 0.53125, learning_rate 0.00129665\n","2023-06-13T15:19:33.730213: step 2437, loss 1.20676, acc 0.59375, learning_rate 0.00129607\n","2023-06-13T15:19:36.116502: step 2438, loss 1.08143, acc 0.53125, learning_rate 0.00129549\n","2023-06-13T15:19:38.546058: step 2439, loss 1.20948, acc 0.59375, learning_rate 0.00129491\n","2023-06-13T15:19:40.281283: step 2440, loss 1.13249, acc 0.625, learning_rate 0.00129433\n","2023-06-13T15:19:41.724896: step 2441, loss 1.49215, acc 0.5625, learning_rate 0.00129375\n","2023-06-13T15:19:43.186512: step 2442, loss 1.25582, acc 0.53125, learning_rate 0.00129317\n","2023-06-13T15:19:44.591810: step 2443, loss 1.47129, acc 0.46875, learning_rate 0.0012926\n","2023-06-13T15:19:45.998519: step 2444, loss 1.35529, acc 0.40625, learning_rate 0.00129202\n","2023-06-13T15:19:47.431421: step 2445, loss 1.31351, acc 0.65625, learning_rate 0.00129144\n","2023-06-13T15:19:48.851202: step 2446, loss 1.17451, acc 0.5625, learning_rate 0.00129086\n","2023-06-13T15:19:51.002055: step 2447, loss 1.33844, acc 0.53125, learning_rate 0.00129028\n","2023-06-13T15:19:53.544992: step 2448, loss 1.48169, acc 0.5, learning_rate 0.00128971\n","2023-06-13T15:19:55.880021: step 2449, loss 1.50181, acc 0.5, learning_rate 0.00128913\n","2023-06-13T15:19:58.218075: step 2450, loss 1.65697, acc 0.5625, learning_rate 0.00128855\n","2023-06-13T15:20:00.172207: step 2451, loss 0.849072, acc 0.71875, learning_rate 0.00128798\n","2023-06-13T15:20:01.642044: step 2452, loss 0.860007, acc 0.65625, learning_rate 0.0012874\n","2023-06-13T15:20:03.103692: step 2453, loss 1.09468, acc 0.625, learning_rate 0.00128683\n","2023-06-13T15:20:04.541329: step 2454, loss 1.00631, acc 0.65625, learning_rate 0.00128625\n","2023-06-13T15:20:05.977705: step 2455, loss 1.30352, acc 0.65625, learning_rate 0.00128568\n","2023-06-13T15:20:07.431967: step 2456, loss 1.30226, acc 0.53125, learning_rate 0.0012851\n","2023-06-13T15:20:08.904168: step 2457, loss 1.14806, acc 0.5625, learning_rate 0.00128453\n","2023-06-13T15:20:10.969276: step 2458, loss 1.36608, acc 0.375, learning_rate 0.00128395\n","2023-06-13T15:20:13.534032: step 2459, loss 1.02951, acc 0.5, learning_rate 0.00128338\n","2023-06-13T15:20:15.819582: step 2460, loss 1.69853, acc 0.4375, learning_rate 0.0012828\n","2023-06-13T15:20:18.174959: step 2461, loss 1.57734, acc 0.4375, learning_rate 0.00128223\n","2023-06-13T15:20:20.376060: step 2462, loss 1.51004, acc 0.5625, learning_rate 0.00128166\n","2023-06-13T15:20:21.816253: step 2463, loss 1.27642, acc 0.59375, learning_rate 0.00128108\n","2023-06-13T15:20:23.251985: step 2464, loss 1.36387, acc 0.625, learning_rate 0.00128051\n","2023-06-13T15:20:24.728354: step 2465, loss 0.913203, acc 0.71875, learning_rate 0.00127994\n","2023-06-13T15:20:26.150146: step 2466, loss 1.15692, acc 0.6875, learning_rate 0.00127937\n","2023-06-13T15:20:27.612805: step 2467, loss 1.03401, acc 0.59375, learning_rate 0.0012788\n","2023-06-13T15:20:29.063511: step 2468, loss 1.23225, acc 0.5625, learning_rate 0.00127822\n","2023-06-13T15:20:30.776250: step 2469, loss 1.36624, acc 0.46875, learning_rate 0.00127765\n","2023-06-13T15:20:33.276209: step 2470, loss 0.883005, acc 0.625, learning_rate 0.00127708\n","2023-06-13T15:20:35.582493: step 2471, loss 1.4836, acc 0.46875, learning_rate 0.00127651\n","2023-06-13T15:20:37.907693: step 2472, loss 1.30715, acc 0.59375, learning_rate 0.00127594\n","2023-06-13T15:20:40.193605: step 2473, loss 0.778516, acc 0.875, learning_rate 0.00127537\n","2023-06-13T15:20:41.600018: step 2474, loss 1.29244, acc 0.5, learning_rate 0.0012748\n","2023-06-13T15:20:43.041395: step 2475, loss 0.947534, acc 0.75, learning_rate 0.00127423\n","2023-06-13T15:20:44.473727: step 2476, loss 1.26758, acc 0.65625, learning_rate 0.00127366\n","2023-06-13T15:20:45.936890: step 2477, loss 1.08882, acc 0.65625, learning_rate 0.00127309\n","2023-06-13T15:20:47.387472: step 2478, loss 1.02849, acc 0.625, learning_rate 0.00127252\n","2023-06-13T15:20:49.801247: step 2479, loss 0.857625, acc 0.75, learning_rate 0.00127195\n","2023-06-13T15:20:52.523651: step 2480, loss 0.85252, acc 0.78125, learning_rate 0.00127139\n","2023-06-13T15:20:55.304363: step 2481, loss 1.03814, acc 0.5625, learning_rate 0.00127082\n","2023-06-13T15:20:58.054198: step 2482, loss 1.26391, acc 0.625, learning_rate 0.00127025\n","2023-06-13T15:21:00.496419: step 2483, loss 1.306, acc 0.59375, learning_rate 0.00126968\n","2023-06-13T15:21:02.983916: step 2484, loss 1.01093, acc 0.6875, learning_rate 0.00126912\n","2023-06-13T15:21:05.208202: step 2485, loss 0.950921, acc 0.5625, learning_rate 0.00126855\n","2023-06-13T15:21:06.894167: step 2486, loss 0.781675, acc 0.75, learning_rate 0.00126798\n","2023-06-13T15:21:08.335593: step 2487, loss 0.904769, acc 0.625, learning_rate 0.00126742\n","2023-06-13T15:21:09.807478: step 2488, loss 1.12658, acc 0.71875, learning_rate 0.00126685\n","2023-06-13T15:21:11.257044: step 2489, loss 0.826862, acc 0.75, learning_rate 0.00126628\n","2023-06-13T15:21:12.679299: step 2490, loss 0.903292, acc 0.6875, learning_rate 0.00126572\n","2023-06-13T15:21:14.116984: step 2491, loss 1.14658, acc 0.53125, learning_rate 0.00126515\n","2023-06-13T15:21:15.577715: step 2492, loss 0.718704, acc 0.78125, learning_rate 0.00126459\n","2023-06-13T15:21:17.765905: step 2493, loss 0.953626, acc 0.71875, learning_rate 0.00126402\n","2023-06-13T15:21:20.225397: step 2494, loss 1.01223, acc 0.71875, learning_rate 0.00126346\n","2023-06-13T15:21:22.651250: step 2495, loss 0.957344, acc 0.6875, learning_rate 0.0012629\n","2023-06-13T15:21:24.942889: step 2496, loss 1.09147, acc 0.59375, learning_rate 0.00126233\n","2023-06-13T15:21:26.919228: step 2497, loss 0.972469, acc 0.71875, learning_rate 0.00126177\n","2023-06-13T15:21:28.360880: step 2498, loss 1.07271, acc 0.625, learning_rate 0.0012612\n","2023-06-13T15:21:29.782552: step 2499, loss 0.925256, acc 0.6875, learning_rate 0.00126064\n","\n","Evaluation:\n","2023-06-13T15:21:58.010941: step 2500, loss 1.88172, acc 0.385875\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2500\n","\n","2023-06-13T15:22:00.808962: step 2500, loss 0.7955, acc 0.71875, learning_rate 0.00126008\n","2023-06-13T15:22:03.174621: step 2501, loss 1.00999, acc 0.78125, learning_rate 0.00125952\n","2023-06-13T15:22:05.553445: step 2502, loss 0.969384, acc 0.6875, learning_rate 0.00125895\n","2023-06-13T15:22:07.738565: step 2503, loss 1.07533, acc 0.59375, learning_rate 0.00125839\n","2023-06-13T15:22:09.189684: step 2504, loss 0.860183, acc 0.65625, learning_rate 0.00125783\n","2023-06-13T15:22:10.713812: step 2505, loss 0.60675, acc 0.8125, learning_rate 0.00125727\n","2023-06-13T15:22:12.203751: step 2506, loss 0.930807, acc 0.625, learning_rate 0.00125671\n","2023-06-13T15:22:13.635586: step 2507, loss 1.0156, acc 0.65625, learning_rate 0.00125615\n","2023-06-13T15:22:15.082100: step 2508, loss 0.913074, acc 0.75, learning_rate 0.00125559\n","2023-06-13T15:22:16.543329: step 2509, loss 1.12507, acc 0.625, learning_rate 0.00125503\n","2023-06-13T15:22:18.451883: step 2510, loss 0.597519, acc 0.8125, learning_rate 0.00125447\n","2023-06-13T15:22:21.003522: step 2511, loss 0.948846, acc 0.65625, learning_rate 0.00125391\n","2023-06-13T15:22:23.306654: step 2512, loss 1.05082, acc 0.53125, learning_rate 0.00125335\n","2023-06-13T15:22:25.700374: step 2513, loss 0.622321, acc 0.78125, learning_rate 0.00125279\n","2023-06-13T15:22:27.915536: step 2514, loss 1.13623, acc 0.59375, learning_rate 0.00125223\n","2023-06-13T15:22:29.353248: step 2515, loss 1.1247, acc 0.625, learning_rate 0.00125167\n","2023-06-13T15:22:30.793866: step 2516, loss 1.06307, acc 0.625, learning_rate 0.00125111\n","2023-06-13T15:22:32.246634: step 2517, loss 0.935976, acc 0.6875, learning_rate 0.00125055\n","2023-06-13T15:22:33.673634: step 2518, loss 0.623351, acc 0.75, learning_rate 0.00125\n","2023-06-13T15:22:35.107182: step 2519, loss 0.730571, acc 0.71875, learning_rate 0.00124944\n","2023-06-13T15:22:36.524103: step 2520, loss 0.993927, acc 0.75, learning_rate 0.00124888\n","2023-06-13T15:22:38.249728: step 2521, loss 0.965073, acc 0.78125, learning_rate 0.00124832\n","2023-06-13T15:22:40.894309: step 2522, loss 0.819949, acc 0.75, learning_rate 0.00124777\n","2023-06-13T15:22:43.363047: step 2523, loss 0.968499, acc 0.5, learning_rate 0.00124721\n","2023-06-13T15:22:45.704052: step 2524, loss 0.86229, acc 0.65625, learning_rate 0.00124665\n","2023-06-13T15:22:47.899632: step 2525, loss 0.98338, acc 0.625, learning_rate 0.0012461\n","2023-06-13T15:22:49.338691: step 2526, loss 0.715508, acc 0.8125, learning_rate 0.00124554\n","2023-06-13T15:22:50.794583: step 2527, loss 1.07056, acc 0.65625, learning_rate 0.00124499\n","2023-06-13T15:22:52.252960: step 2528, loss 1.16835, acc 0.53125, learning_rate 0.00124443\n","2023-06-13T15:22:53.678198: step 2529, loss 0.904903, acc 0.53125, learning_rate 0.00124388\n","2023-06-13T15:22:55.114296: step 2530, loss 0.973592, acc 0.71875, learning_rate 0.00124332\n","2023-06-13T15:22:56.532521: step 2531, loss 0.995167, acc 0.625, learning_rate 0.00124277\n","2023-06-13T15:22:58.185908: step 2532, loss 1.21825, acc 0.5625, learning_rate 0.00124221\n","2023-06-13T15:23:00.841873: step 2533, loss 0.759761, acc 0.8125, learning_rate 0.00124166\n","2023-06-13T15:23:03.190540: step 2534, loss 0.826571, acc 0.71875, learning_rate 0.00124111\n","2023-06-13T15:23:05.571301: step 2535, loss 0.570066, acc 0.78125, learning_rate 0.00124055\n","2023-06-13T15:23:07.872009: step 2536, loss 1.13577, acc 0.59375, learning_rate 0.00124\n","2023-06-13T15:23:09.367456: step 2537, loss 0.921241, acc 0.6875, learning_rate 0.00123945\n","2023-06-13T15:23:10.816267: step 2538, loss 1.05866, acc 0.65625, learning_rate 0.00123889\n","2023-06-13T15:23:12.299233: step 2539, loss 1.17013, acc 0.625, learning_rate 0.00123834\n","2023-06-13T15:23:13.756751: step 2540, loss 0.930657, acc 0.625, learning_rate 0.00123779\n","2023-06-13T15:23:15.210975: step 2541, loss 0.719057, acc 0.78125, learning_rate 0.00123724\n","2023-06-13T15:23:16.675705: step 2542, loss 0.852638, acc 0.65625, learning_rate 0.00123669\n","2023-06-13T15:23:18.150134: step 2543, loss 0.733498, acc 0.75, learning_rate 0.00123614\n","2023-06-13T15:23:20.705085: step 2544, loss 1.01688, acc 0.65625, learning_rate 0.00123559\n","2023-06-13T15:23:23.210786: step 2545, loss 0.90966, acc 0.6875, learning_rate 0.00123503\n","2023-06-13T15:23:25.577570: step 2546, loss 1.08446, acc 0.625, learning_rate 0.00123448\n","2023-06-13T15:23:27.980730: step 2547, loss 0.698365, acc 0.8125, learning_rate 0.00123393\n","2023-06-13T15:23:29.528079: step 2548, loss 0.769832, acc 0.75, learning_rate 0.00123338\n","2023-06-13T15:23:30.962864: step 2549, loss 1.48724, acc 0.59375, learning_rate 0.00123283\n","2023-06-13T15:23:32.461182: step 2550, loss 1.07038, acc 0.625, learning_rate 0.00123229\n","2023-06-13T15:23:33.940193: step 2551, loss 0.900887, acc 0.59375, learning_rate 0.00123174\n","2023-06-13T15:23:35.377387: step 2552, loss 0.803419, acc 0.78125, learning_rate 0.00123119\n","2023-06-13T15:23:36.833145: step 2553, loss 0.841491, acc 0.65625, learning_rate 0.00123064\n","2023-06-13T15:23:38.307253: step 2554, loss 0.945145, acc 0.65625, learning_rate 0.00123009\n","2023-06-13T15:23:40.807582: step 2555, loss 1.01196, acc 0.65625, learning_rate 0.00122954\n","2023-06-13T15:23:43.291865: step 2556, loss 0.837193, acc 0.6875, learning_rate 0.001229\n","2023-06-13T15:23:45.627693: step 2557, loss 0.614497, acc 0.78125, learning_rate 0.00122845\n","2023-06-13T15:23:48.002314: step 2558, loss 1.10182, acc 0.59375, learning_rate 0.0012279\n","2023-06-13T15:23:49.579486: step 2559, loss 0.953264, acc 0.625, learning_rate 0.00122735\n","2023-06-13T15:23:50.996629: step 2560, loss 0.819844, acc 0.6875, learning_rate 0.00122681\n","2023-06-13T15:23:52.430268: step 2561, loss 1.0389, acc 0.75, learning_rate 0.00122626\n","2023-06-13T15:23:53.861625: step 2562, loss 1.01714, acc 0.59375, learning_rate 0.00122571\n","2023-06-13T15:23:55.313332: step 2563, loss 0.785553, acc 0.59375, learning_rate 0.00122517\n","2023-06-13T15:23:56.750817: step 2564, loss 1.02926, acc 0.59375, learning_rate 0.00122462\n","2023-06-13T15:23:58.189449: step 2565, loss 1.0623, acc 0.5625, learning_rate 0.00122408\n","2023-06-13T15:24:00.626556: step 2566, loss 1.44086, acc 0.59375, learning_rate 0.00122353\n","2023-06-13T15:24:03.096709: step 2567, loss 0.794071, acc 0.75, learning_rate 0.00122299\n","2023-06-13T15:24:05.439667: step 2568, loss 0.950442, acc 0.71875, learning_rate 0.00122244\n","2023-06-13T15:24:07.819188: step 2569, loss 0.833343, acc 0.75, learning_rate 0.0012219\n","2023-06-13T15:24:09.500333: step 2570, loss 0.67604, acc 0.75, learning_rate 0.00122136\n","2023-06-13T15:24:10.957355: step 2571, loss 0.864199, acc 0.75, learning_rate 0.00122081\n","2023-06-13T15:24:12.391978: step 2572, loss 1.10344, acc 0.625, learning_rate 0.00122027\n","2023-06-13T15:24:13.840201: step 2573, loss 1.18072, acc 0.5625, learning_rate 0.00121973\n","2023-06-13T15:24:15.267212: step 2574, loss 1.10032, acc 0.65625, learning_rate 0.00121918\n","2023-06-13T15:24:16.726141: step 2575, loss 1.2575, acc 0.6875, learning_rate 0.00121864\n","2023-06-13T15:24:18.159857: step 2576, loss 0.965033, acc 0.75, learning_rate 0.0012181\n","2023-06-13T15:24:20.413761: step 2577, loss 0.642897, acc 0.75, learning_rate 0.00121756\n","2023-06-13T15:24:22.931641: step 2578, loss 0.754469, acc 0.625, learning_rate 0.00121701\n","2023-06-13T15:24:25.375852: step 2579, loss 0.872585, acc 0.6875, learning_rate 0.00121647\n","2023-06-13T15:24:27.735599: step 2580, loss 0.977292, acc 0.6875, learning_rate 0.00121593\n","2023-06-13T15:24:29.457069: step 2581, loss 1.14063, acc 0.59375, learning_rate 0.00121539\n","2023-06-13T15:24:30.925843: step 2582, loss 1.22503, acc 0.59375, learning_rate 0.00121485\n","2023-06-13T15:24:32.362447: step 2583, loss 0.89503, acc 0.71875, learning_rate 0.00121431\n","2023-06-13T15:24:33.801302: step 2584, loss 1.06916, acc 0.59375, learning_rate 0.00121377\n","2023-06-13T15:24:35.215906: step 2585, loss 1.08148, acc 0.625, learning_rate 0.00121323\n","2023-06-13T15:24:36.700473: step 2586, loss 1.06552, acc 0.5625, learning_rate 0.00121269\n","2023-06-13T15:24:38.158392: step 2587, loss 0.857588, acc 0.75, learning_rate 0.00121215\n","2023-06-13T15:24:40.410977: step 2588, loss 1.22002, acc 0.59375, learning_rate 0.00121161\n","2023-06-13T15:24:42.886095: step 2589, loss 0.903513, acc 0.6875, learning_rate 0.00121107\n","2023-06-13T15:24:45.285472: step 2590, loss 0.959216, acc 0.625, learning_rate 0.00121053\n","2023-06-13T15:24:47.633595: step 2591, loss 1.21298, acc 0.59375, learning_rate 0.00120999\n","2023-06-13T15:24:49.504904: step 2592, loss 0.979299, acc 0.5625, learning_rate 0.00120945\n","2023-06-13T15:24:50.936947: step 2593, loss 1.13007, acc 0.5625, learning_rate 0.00120892\n","2023-06-13T15:24:52.377135: step 2594, loss 0.992241, acc 0.625, learning_rate 0.00120838\n","2023-06-13T15:24:53.816583: step 2595, loss 0.78729, acc 0.78125, learning_rate 0.00120784\n","2023-06-13T15:24:55.243721: step 2596, loss 1.28623, acc 0.53125, learning_rate 0.0012073\n","2023-06-13T15:24:56.704735: step 2597, loss 0.673024, acc 0.78125, learning_rate 0.00120677\n","2023-06-13T15:24:58.150205: step 2598, loss 1.06225, acc 0.5625, learning_rate 0.00120623\n","2023-06-13T15:25:00.238055: step 2599, loss 0.949566, acc 0.6875, learning_rate 0.00120569\n","\n","Evaluation:\n","2023-06-13T15:25:32.150243: step 2600, loss 2.01062, acc 0.381934\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2600\n","\n","2023-06-13T15:25:33.789244: step 2600, loss 0.882613, acc 0.75, learning_rate 0.00120516\n","2023-06-13T15:25:35.252991: step 2601, loss 0.806293, acc 0.6875, learning_rate 0.00120462\n","2023-06-13T15:25:36.705065: step 2602, loss 0.914201, acc 0.625, learning_rate 0.00120409\n","2023-06-13T15:25:38.159303: step 2603, loss 0.99519, acc 0.6875, learning_rate 0.00120355\n","2023-06-13T15:25:39.644743: step 2604, loss 0.723294, acc 0.65625, learning_rate 0.00120302\n","2023-06-13T15:25:41.148960: step 2605, loss 1.05542, acc 0.65625, learning_rate 0.00120248\n","2023-06-13T15:25:43.730650: step 2606, loss 0.988141, acc 0.59375, learning_rate 0.00120195\n","2023-06-13T15:25:46.117682: step 2607, loss 0.850177, acc 0.6875, learning_rate 0.00120141\n","2023-06-13T15:25:48.395602: step 2608, loss 1.36579, acc 0.59375, learning_rate 0.00120088\n","2023-06-13T15:25:50.767754: step 2609, loss 1.13423, acc 0.65625, learning_rate 0.00120034\n","2023-06-13T15:25:52.411882: step 2610, loss 0.645468, acc 0.8125, learning_rate 0.00119981\n","2023-06-13T15:25:53.862463: step 2611, loss 1.1654, acc 0.5625, learning_rate 0.00119928\n","2023-06-13T15:25:55.297298: step 2612, loss 0.776887, acc 0.71875, learning_rate 0.00119874\n","2023-06-13T15:25:56.721439: step 2613, loss 0.844539, acc 0.78125, learning_rate 0.00119821\n","2023-06-13T15:25:58.145530: step 2614, loss 0.875257, acc 0.75, learning_rate 0.00119768\n","2023-06-13T15:25:59.569574: step 2615, loss 0.796695, acc 0.71875, learning_rate 0.00119715\n","2023-06-13T15:26:01.001488: step 2616, loss 1.09284, acc 0.59375, learning_rate 0.00119662\n","2023-06-13T15:26:03.450782: step 2617, loss 0.982673, acc 0.71875, learning_rate 0.00119608\n","2023-06-13T15:26:05.869497: step 2618, loss 1.25701, acc 0.625, learning_rate 0.00119555\n","2023-06-13T15:26:08.215348: step 2619, loss 1.08358, acc 0.5625, learning_rate 0.00119502\n","2023-06-13T15:26:10.522027: step 2620, loss 1.03466, acc 0.65625, learning_rate 0.00119449\n","2023-06-13T15:26:13.207702: step 2621, loss 0.899979, acc 0.59375, learning_rate 0.00119396\n","2023-06-13T15:26:15.760364: step 2622, loss 1.2707, acc 0.53125, learning_rate 0.00119343\n","2023-06-13T15:26:18.134038: step 2623, loss 0.953559, acc 0.65625, learning_rate 0.0011929\n","2023-06-13T15:26:20.415447: step 2624, loss 0.831542, acc 0.625, learning_rate 0.00119237\n","2023-06-13T15:26:22.378383: step 2625, loss 1.08634, acc 0.6875, learning_rate 0.00119184\n","2023-06-13T15:26:24.654807: step 2626, loss 0.547949, acc 0.84375, learning_rate 0.00119131\n","2023-06-13T15:26:27.143874: step 2627, loss 0.919227, acc 0.65625, learning_rate 0.00119078\n","2023-06-13T15:26:29.522376: step 2628, loss 0.774293, acc 0.78125, learning_rate 0.00119025\n","2023-06-13T15:26:31.873555: step 2629, loss 0.961006, acc 0.65625, learning_rate 0.00118972\n","2023-06-13T15:26:33.735330: step 2630, loss 0.876988, acc 0.71875, learning_rate 0.00118919\n","2023-06-13T15:26:35.169426: step 2631, loss 1.1665, acc 0.625, learning_rate 0.00118867\n","2023-06-13T15:26:36.603136: step 2632, loss 0.751136, acc 0.78125, learning_rate 0.00118814\n","2023-06-13T15:26:38.060278: step 2633, loss 1.00141, acc 0.6875, learning_rate 0.00118761\n","2023-06-13T15:26:39.541262: step 2634, loss 0.883377, acc 0.71875, learning_rate 0.00118708\n","2023-06-13T15:26:40.976671: step 2635, loss 1.05858, acc 0.59375, learning_rate 0.00118656\n","2023-06-13T15:26:42.453018: step 2636, loss 1.35926, acc 0.53125, learning_rate 0.00118603\n","2023-06-13T15:26:44.616415: step 2637, loss 1.25766, acc 0.625, learning_rate 0.0011855\n","2023-06-13T15:26:47.175126: step 2638, loss 0.804921, acc 0.625, learning_rate 0.00118498\n","2023-06-13T15:26:49.353912: step 2639, loss 1.13829, acc 0.625, learning_rate 0.00118445\n","2023-06-13T15:26:51.631040: step 2640, loss 1.01389, acc 0.65625, learning_rate 0.00118392\n","2023-06-13T15:26:53.741236: step 2641, loss 0.807599, acc 0.6875, learning_rate 0.0011834\n","2023-06-13T15:26:55.165081: step 2642, loss 0.844027, acc 0.65625, learning_rate 0.00118287\n","2023-06-13T15:26:56.614563: step 2643, loss 0.881431, acc 0.65625, learning_rate 0.00118235\n","2023-06-13T15:26:58.056690: step 2644, loss 0.75012, acc 0.78125, learning_rate 0.00118182\n","2023-06-13T15:26:59.459672: step 2645, loss 1.00303, acc 0.625, learning_rate 0.0011813\n","2023-06-13T15:27:00.895956: step 2646, loss 1.12505, acc 0.6875, learning_rate 0.00118078\n","2023-06-13T15:27:02.329608: step 2647, loss 1.1039, acc 0.5625, learning_rate 0.00118025\n","2023-06-13T15:27:04.068972: step 2648, loss 1.25639, acc 0.5, learning_rate 0.00117973\n","2023-06-13T15:27:06.638074: step 2649, loss 0.815785, acc 0.6875, learning_rate 0.0011792\n","2023-06-13T15:27:09.002053: step 2650, loss 0.878548, acc 0.65625, learning_rate 0.00117868\n","2023-06-13T15:27:11.413710: step 2651, loss 0.835121, acc 0.65625, learning_rate 0.00117816\n","2023-06-13T15:27:13.680986: step 2652, loss 0.916683, acc 0.6875, learning_rate 0.00117763\n","2023-06-13T15:27:15.242373: step 2653, loss 1.03681, acc 0.5625, learning_rate 0.00117711\n","2023-06-13T15:27:16.699995: step 2654, loss 0.715089, acc 0.75, learning_rate 0.00117659\n","2023-06-13T15:27:18.141755: step 2655, loss 0.8548, acc 0.78125, learning_rate 0.00117607\n","2023-06-13T15:27:19.572008: step 2656, loss 0.760761, acc 0.78125, learning_rate 0.00117555\n","2023-06-13T15:27:21.006307: step 2657, loss 1.03457, acc 0.71875, learning_rate 0.00117502\n","2023-06-13T15:27:22.475004: step 2658, loss 0.792907, acc 0.8125, learning_rate 0.0011745\n","2023-06-13T15:27:24.163359: step 2659, loss 0.752314, acc 0.78125, learning_rate 0.00117398\n","2023-06-13T15:27:26.750456: step 2660, loss 1.33912, acc 0.5625, learning_rate 0.00117346\n","2023-06-13T15:27:29.131522: step 2661, loss 1.16743, acc 0.625, learning_rate 0.00117294\n","2023-06-13T15:27:31.544283: step 2662, loss 0.789144, acc 0.75, learning_rate 0.00117242\n","2023-06-13T15:27:33.890719: step 2663, loss 0.871253, acc 0.59375, learning_rate 0.0011719\n","2023-06-13T15:27:35.348390: step 2664, loss 1.02744, acc 0.59375, learning_rate 0.00117138\n","2023-06-13T15:27:36.777479: step 2665, loss 1.00718, acc 0.5625, learning_rate 0.00117086\n","2023-06-13T15:27:38.227346: step 2666, loss 0.59336, acc 0.84375, learning_rate 0.00117034\n","2023-06-13T15:27:39.651510: step 2667, loss 1.28428, acc 0.59375, learning_rate 0.00116982\n","2023-06-13T15:27:41.129760: step 2668, loss 0.836418, acc 0.6875, learning_rate 0.0011693\n","2023-06-13T15:27:42.565735: step 2669, loss 0.644506, acc 0.875, learning_rate 0.00116879\n","2023-06-13T15:27:44.318841: step 2670, loss 1.21725, acc 0.5625, learning_rate 0.00116827\n","2023-06-13T15:27:46.978300: step 2671, loss 1.29165, acc 0.5625, learning_rate 0.00116775\n","2023-06-13T15:27:49.327634: step 2672, loss 1.01111, acc 0.6875, learning_rate 0.00116723\n","2023-06-13T15:27:51.743283: step 2673, loss 0.809586, acc 0.6875, learning_rate 0.00116671\n","2023-06-13T15:27:53.901040: step 2674, loss 0.885512, acc 0.625, learning_rate 0.0011662\n","2023-06-13T15:27:55.307167: step 2675, loss 0.995047, acc 0.71875, learning_rate 0.00116568\n","2023-06-13T15:27:56.765211: step 2676, loss 1.16999, acc 0.59375, learning_rate 0.00116516\n","2023-06-13T15:27:58.227204: step 2677, loss 0.971925, acc 0.6875, learning_rate 0.00116465\n","2023-06-13T15:27:59.632986: step 2678, loss 0.897246, acc 0.75, learning_rate 0.00116413\n","2023-06-13T15:28:01.081352: step 2679, loss 0.939556, acc 0.6875, learning_rate 0.00116362\n","2023-06-13T15:28:02.506011: step 2680, loss 0.869031, acc 0.65625, learning_rate 0.0011631\n","2023-06-13T15:28:04.249248: step 2681, loss 0.956961, acc 0.71875, learning_rate 0.00116258\n","2023-06-13T15:28:06.756464: step 2682, loss 1.0991, acc 0.65625, learning_rate 0.00116207\n","2023-06-13T15:28:09.113730: step 2683, loss 0.986283, acc 0.65625, learning_rate 0.00116155\n","2023-06-13T15:28:11.552287: step 2684, loss 1.02797, acc 0.65625, learning_rate 0.00116104\n","2023-06-13T15:28:13.872548: step 2685, loss 0.967903, acc 0.65625, learning_rate 0.00116052\n","2023-06-13T15:28:15.315685: step 2686, loss 0.913795, acc 0.71875, learning_rate 0.00116001\n","2023-06-13T15:28:16.767781: step 2687, loss 0.953955, acc 0.6875, learning_rate 0.0011595\n","2023-06-13T15:28:18.237469: step 2688, loss 0.815495, acc 0.78125, learning_rate 0.00115898\n","2023-06-13T15:28:19.641611: step 2689, loss 1.04618, acc 0.625, learning_rate 0.00115847\n","2023-06-13T15:28:21.054060: step 2690, loss 0.82695, acc 0.625, learning_rate 0.00115796\n","2023-06-13T15:28:22.496051: step 2691, loss 0.976791, acc 0.8125, learning_rate 0.00115744\n","2023-06-13T15:28:23.984005: step 2692, loss 0.828731, acc 0.6875, learning_rate 0.00115693\n","2023-06-13T15:28:26.444994: step 2693, loss 0.725304, acc 0.75, learning_rate 0.00115642\n","2023-06-13T15:28:28.831269: step 2694, loss 1.11751, acc 0.625, learning_rate 0.00115591\n","2023-06-13T15:28:31.227497: step 2695, loss 1.34882, acc 0.53125, learning_rate 0.00115539\n","2023-06-13T15:28:33.640503: step 2696, loss 1.0827, acc 0.59375, learning_rate 0.00115488\n","2023-06-13T15:28:35.271797: step 2697, loss 0.881244, acc 0.59375, learning_rate 0.00115437\n","2023-06-13T15:28:36.724683: step 2698, loss 1.20083, acc 0.625, learning_rate 0.00115386\n","2023-06-13T15:28:38.211237: step 2699, loss 0.674526, acc 0.78125, learning_rate 0.00115335\n","\n","Evaluation:\n","2023-06-13T15:29:06.619705: step 2700, loss 2.05327, acc 0.38451\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2700\n","\n","2023-06-13T15:29:09.406594: step 2700, loss 0.724147, acc 0.75, learning_rate 0.00115284\n","2023-06-13T15:29:11.723832: step 2701, loss 1.42287, acc 0.53125, learning_rate 0.00115233\n","2023-06-13T15:29:14.098725: step 2702, loss 0.866265, acc 0.625, learning_rate 0.00115182\n","2023-06-13T15:29:15.950364: step 2703, loss 0.786928, acc 0.71875, learning_rate 0.00115131\n","2023-06-13T15:29:17.455702: step 2704, loss 1.07825, acc 0.53125, learning_rate 0.0011508\n","2023-06-13T15:29:18.928915: step 2705, loss 0.916492, acc 0.84375, learning_rate 0.00115029\n","2023-06-13T15:29:20.356188: step 2706, loss 1.03131, acc 0.6875, learning_rate 0.00114978\n","2023-06-13T15:29:21.804526: step 2707, loss 0.885235, acc 0.71875, learning_rate 0.00114927\n","2023-06-13T15:29:23.224595: step 2708, loss 0.80154, acc 0.6875, learning_rate 0.00114876\n","2023-06-13T15:29:24.676888: step 2709, loss 1.30327, acc 0.59375, learning_rate 0.00114825\n","2023-06-13T15:29:26.722473: step 2710, loss 1.08267, acc 0.625, learning_rate 0.00114774\n","2023-06-13T15:29:29.212337: step 2711, loss 0.853719, acc 0.71875, learning_rate 0.00114724\n","2023-06-13T15:29:31.540692: step 2712, loss 1.2782, acc 0.65625, learning_rate 0.00114673\n","2023-06-13T15:29:33.946779: step 2713, loss 0.909285, acc 0.65625, learning_rate 0.00114622\n","2023-06-13T15:29:36.007299: step 2714, loss 0.888449, acc 0.59375, learning_rate 0.00114571\n","2023-06-13T15:29:37.457807: step 2715, loss 0.826436, acc 0.71875, learning_rate 0.00114521\n","2023-06-13T15:29:38.902870: step 2716, loss 0.994452, acc 0.75, learning_rate 0.0011447\n","2023-06-13T15:29:40.324865: step 2717, loss 0.981503, acc 0.5625, learning_rate 0.00114419\n","2023-06-13T15:29:41.770386: step 2718, loss 0.989926, acc 0.71875, learning_rate 0.00114369\n","2023-06-13T15:29:43.279370: step 2719, loss 0.706172, acc 0.78125, learning_rate 0.00114318\n","2023-06-13T15:29:44.731244: step 2720, loss 1.67309, acc 0.5, learning_rate 0.00114267\n","2023-06-13T15:29:46.677730: step 2721, loss 1.01971, acc 0.65625, learning_rate 0.00114217\n","2023-06-13T15:29:49.211563: step 2722, loss 0.718716, acc 0.75, learning_rate 0.00114166\n","2023-06-13T15:29:51.535147: step 2723, loss 1.02991, acc 0.625, learning_rate 0.00114116\n","2023-06-13T15:29:53.871082: step 2724, loss 1.10868, acc 0.625, learning_rate 0.00114065\n","2023-06-13T15:29:56.037382: step 2725, loss 1.23248, acc 0.625, learning_rate 0.00114015\n","2023-06-13T15:29:57.529461: step 2726, loss 0.785341, acc 0.71875, learning_rate 0.00113964\n","2023-06-13T15:29:58.937129: step 2727, loss 0.9412, acc 0.6875, learning_rate 0.00113914\n","2023-06-13T15:30:00.397052: step 2728, loss 1.60448, acc 0.5625, learning_rate 0.00113864\n","2023-06-13T15:30:01.824885: step 2729, loss 0.83483, acc 0.65625, learning_rate 0.00113813\n","2023-06-13T15:30:03.231971: step 2730, loss 0.645208, acc 0.75, learning_rate 0.00113763\n","2023-06-13T15:30:04.692457: step 2731, loss 0.764782, acc 0.78125, learning_rate 0.00113713\n","2023-06-13T15:30:06.433993: step 2732, loss 0.673669, acc 0.78125, learning_rate 0.00113662\n","2023-06-13T15:30:08.962511: step 2733, loss 1.03457, acc 0.6875, learning_rate 0.00113612\n","2023-06-13T15:30:11.345752: step 2734, loss 0.917063, acc 0.75, learning_rate 0.00113562\n","2023-06-13T15:30:13.630148: step 2735, loss 1.08473, acc 0.625, learning_rate 0.00113512\n","2023-06-13T15:30:15.915679: step 2736, loss 0.876254, acc 0.71875, learning_rate 0.00113461\n","2023-06-13T15:30:17.546878: step 2737, loss 0.843545, acc 0.65625, learning_rate 0.00113411\n","2023-06-13T15:30:18.988258: step 2738, loss 1.13935, acc 0.625, learning_rate 0.00113361\n","2023-06-13T15:30:20.425281: step 2739, loss 0.839848, acc 0.71875, learning_rate 0.00113311\n","2023-06-13T15:30:21.854261: step 2740, loss 0.681202, acc 0.875, learning_rate 0.00113261\n","2023-06-13T15:30:23.300517: step 2741, loss 1.20261, acc 0.625, learning_rate 0.00113211\n","2023-06-13T15:30:24.741513: step 2742, loss 1.09698, acc 0.71875, learning_rate 0.00113161\n","2023-06-13T15:30:26.230057: step 2743, loss 0.806223, acc 0.71875, learning_rate 0.00113111\n","2023-06-13T15:30:28.654385: step 2744, loss 0.869932, acc 0.6875, learning_rate 0.00113061\n","2023-06-13T15:30:31.142757: step 2745, loss 0.727859, acc 0.75, learning_rate 0.00113011\n","2023-06-13T15:30:33.508776: step 2746, loss 0.700438, acc 0.8125, learning_rate 0.00112961\n","2023-06-13T15:30:35.844501: step 2747, loss 1.05004, acc 0.5625, learning_rate 0.00112911\n","2023-06-13T15:30:37.642433: step 2748, loss 0.790539, acc 0.75, learning_rate 0.00112861\n","2023-06-13T15:30:39.095595: step 2749, loss 0.691902, acc 0.78125, learning_rate 0.00112811\n","2023-06-13T15:30:40.531060: step 2750, loss 0.570071, acc 0.875, learning_rate 0.00112761\n","2023-06-13T15:30:41.986491: step 2751, loss 0.772958, acc 0.71875, learning_rate 0.00112712\n","2023-06-13T15:30:43.418142: step 2752, loss 0.928704, acc 0.6875, learning_rate 0.00112662\n","2023-06-13T15:30:44.870277: step 2753, loss 0.649252, acc 0.78125, learning_rate 0.00112612\n","2023-06-13T15:30:46.290329: step 2754, loss 1.20384, acc 0.625, learning_rate 0.00112562\n","2023-06-13T15:30:48.374385: step 2755, loss 0.806009, acc 0.84375, learning_rate 0.00112512\n","2023-06-13T15:30:50.916241: step 2756, loss 1.1847, acc 0.71875, learning_rate 0.00112463\n","2023-06-13T15:30:53.250645: step 2757, loss 1.29398, acc 0.625, learning_rate 0.00112413\n","2023-06-13T15:30:55.598162: step 2758, loss 0.874585, acc 0.6875, learning_rate 0.00112363\n","2023-06-13T15:30:57.655447: step 2759, loss 0.698575, acc 0.875, learning_rate 0.00112314\n","2023-06-13T15:30:59.086713: step 2760, loss 0.742745, acc 0.78125, learning_rate 0.00112264\n","2023-06-13T15:31:00.523562: step 2761, loss 0.670401, acc 0.71875, learning_rate 0.00112215\n","2023-06-13T15:31:01.984306: step 2762, loss 0.83474, acc 0.6875, learning_rate 0.00112165\n","2023-06-13T15:31:03.439727: step 2763, loss 0.753221, acc 0.8125, learning_rate 0.00112115\n","2023-06-13T15:31:04.895414: step 2764, loss 0.910162, acc 0.65625, learning_rate 0.00112066\n","2023-06-13T15:31:06.323621: step 2765, loss 1.09974, acc 0.625, learning_rate 0.00112016\n","2023-06-13T15:31:08.227283: step 2766, loss 0.902302, acc 0.6875, learning_rate 0.00111967\n","2023-06-13T15:31:10.830292: step 2767, loss 0.727636, acc 0.75, learning_rate 0.00111918\n","2023-06-13T15:31:13.174403: step 2768, loss 1.1101, acc 0.5625, learning_rate 0.00111868\n","2023-06-13T15:31:15.479697: step 2769, loss 0.866879, acc 0.75, learning_rate 0.00111819\n","2023-06-13T15:31:17.881779: step 2770, loss 1.03342, acc 0.6875, learning_rate 0.00111769\n","2023-06-13T15:31:19.349676: step 2771, loss 1.08715, acc 0.78125, learning_rate 0.0011172\n","2023-06-13T15:31:20.798129: step 2772, loss 1.13591, acc 0.625, learning_rate 0.00111671\n","2023-06-13T15:31:22.246965: step 2773, loss 0.846181, acc 0.6875, learning_rate 0.00111621\n","2023-06-13T15:31:23.694975: step 2774, loss 1.06628, acc 0.625, learning_rate 0.00111572\n","2023-06-13T15:31:25.133153: step 2775, loss 0.866119, acc 0.65625, learning_rate 0.00111523\n","2023-06-13T15:31:26.599759: step 2776, loss 0.719073, acc 0.8125, learning_rate 0.00111474\n","2023-06-13T15:31:28.298599: step 2777, loss 0.98776, acc 0.75, learning_rate 0.00111424\n","2023-06-13T15:31:30.918596: step 2778, loss 0.88315, acc 0.75, learning_rate 0.00111375\n","2023-06-13T15:31:33.323048: step 2779, loss 0.874797, acc 0.65625, learning_rate 0.00111326\n","2023-06-13T15:31:35.808811: step 2780, loss 0.855976, acc 0.6875, learning_rate 0.00111277\n","2023-06-13T15:31:38.622682: step 2781, loss 0.788064, acc 0.71875, learning_rate 0.00111228\n","2023-06-13T15:31:41.433300: step 2782, loss 1.00962, acc 0.71875, learning_rate 0.00111179\n","2023-06-13T15:31:43.762751: step 2783, loss 1.27853, acc 0.59375, learning_rate 0.0011113\n","2023-06-13T15:31:46.126593: step 2784, loss 1.02267, acc 0.59375, learning_rate 0.00111081\n","2023-06-13T15:31:48.427904: step 2785, loss 0.859696, acc 0.65625, learning_rate 0.00111032\n","2023-06-13T15:31:49.921634: step 2786, loss 1.32569, acc 0.4375, learning_rate 0.00110983\n","2023-06-13T15:31:51.374105: step 2787, loss 0.743154, acc 0.75, learning_rate 0.00110934\n","2023-06-13T15:31:53.479866: step 2788, loss 1.04815, acc 0.65625, learning_rate 0.00110885\n","2023-06-13T15:31:56.058088: step 2789, loss 0.975569, acc 0.625, learning_rate 0.00110836\n","2023-06-13T15:31:58.368060: step 2790, loss 0.782275, acc 0.75, learning_rate 0.00110787\n","2023-06-13T15:32:00.809379: step 2791, loss 1.11671, acc 0.5625, learning_rate 0.00110738\n","2023-06-13T15:32:02.935552: step 2792, loss 0.90051, acc 0.625, learning_rate 0.00110689\n","2023-06-13T15:32:04.399181: step 2793, loss 1.00531, acc 0.71875, learning_rate 0.0011064\n","2023-06-13T15:32:05.847907: step 2794, loss 0.845299, acc 0.78125, learning_rate 0.00110592\n","2023-06-13T15:32:07.325868: step 2795, loss 1.23994, acc 0.625, learning_rate 0.00110543\n","2023-06-13T15:32:08.814927: step 2796, loss 0.965041, acc 0.65625, learning_rate 0.00110494\n","2023-06-13T15:32:10.277166: step 2797, loss 0.910606, acc 0.71875, learning_rate 0.00110445\n","2023-06-13T15:32:11.726385: step 2798, loss 1.45953, acc 0.53125, learning_rate 0.00110397\n","2023-06-13T15:32:13.695769: step 2799, loss 1.08279, acc 0.6875, learning_rate 0.00110348\n","\n","Evaluation:\n","2023-06-13T15:32:46.380503: step 2800, loss 2.10467, acc 0.384056\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2800\n","\n","2023-06-13T15:32:48.030836: step 2800, loss 0.763889, acc 0.78125, learning_rate 0.00110299\n","2023-06-13T15:32:49.510087: step 2801, loss 0.896077, acc 0.75, learning_rate 0.00110251\n","2023-06-13T15:32:50.956589: step 2802, loss 0.630483, acc 0.75, learning_rate 0.00110202\n","2023-06-13T15:32:52.477940: step 2803, loss 0.738774, acc 0.75, learning_rate 0.00110154\n","2023-06-13T15:32:53.939697: step 2804, loss 0.953878, acc 0.75, learning_rate 0.00110105\n","2023-06-13T15:32:55.883501: step 2805, loss 0.742075, acc 0.75, learning_rate 0.00110056\n","2023-06-13T15:32:58.494705: step 2806, loss 0.983975, acc 0.6875, learning_rate 0.00110008\n","2023-06-13T15:33:00.891198: step 2807, loss 1.33245, acc 0.625, learning_rate 0.00109959\n","2023-06-13T15:33:03.285951: step 2808, loss 0.873441, acc 0.6875, learning_rate 0.00109911\n","2023-06-13T15:33:05.541610: step 2809, loss 1.3042, acc 0.625, learning_rate 0.00109863\n","2023-06-13T15:33:07.008568: step 2810, loss 0.86361, acc 0.71875, learning_rate 0.00109814\n","2023-06-13T15:33:08.477115: step 2811, loss 1.16202, acc 0.65625, learning_rate 0.00109766\n","2023-06-13T15:33:09.933324: step 2812, loss 1.0831, acc 0.46875, learning_rate 0.00109717\n","2023-06-13T15:33:11.439650: step 2813, loss 0.788273, acc 0.71875, learning_rate 0.00109669\n","2023-06-13T15:33:12.883169: step 2814, loss 1.0248, acc 0.625, learning_rate 0.00109621\n","2023-06-13T15:33:14.357767: step 2815, loss 1.01308, acc 0.65625, learning_rate 0.00109572\n","2023-06-13T15:33:16.150429: step 2816, loss 0.525303, acc 0.84375, learning_rate 0.00109524\n","2023-06-13T15:33:18.686832: step 2817, loss 1.3028, acc 0.59375, learning_rate 0.00109476\n","2023-06-13T15:33:21.052554: step 2818, loss 0.83583, acc 0.6875, learning_rate 0.00109428\n","2023-06-13T15:33:23.534008: step 2819, loss 0.930431, acc 0.625, learning_rate 0.00109379\n","2023-06-13T15:33:25.863355: step 2820, loss 0.606211, acc 0.84375, learning_rate 0.00109331\n","2023-06-13T15:33:27.464173: step 2821, loss 1.38766, acc 0.5, learning_rate 0.00109283\n","2023-06-13T15:33:28.946513: step 2822, loss 0.90271, acc 0.6875, learning_rate 0.00109235\n","2023-06-13T15:33:30.406670: step 2823, loss 0.587452, acc 0.78125, learning_rate 0.00109187\n","2023-06-13T15:33:31.871936: step 2824, loss 1.32085, acc 0.53125, learning_rate 0.00109139\n","2023-06-13T15:33:33.343256: step 2825, loss 0.695368, acc 0.875, learning_rate 0.00109091\n","2023-06-13T15:33:34.833495: step 2826, loss 0.849555, acc 0.78125, learning_rate 0.00109043\n","2023-06-13T15:33:36.324428: step 2827, loss 0.953719, acc 0.65625, learning_rate 0.00108995\n","2023-06-13T15:33:38.916339: step 2828, loss 1.27359, acc 0.53125, learning_rate 0.00108947\n","2023-06-13T15:33:41.449191: step 2829, loss 0.592423, acc 0.75, learning_rate 0.00108899\n","2023-06-13T15:33:43.944149: step 2830, loss 0.942219, acc 0.65625, learning_rate 0.00108851\n","2023-06-13T15:33:46.338986: step 2831, loss 0.916554, acc 0.59375, learning_rate 0.00108803\n","2023-06-13T15:33:47.957090: step 2832, loss 1.09362, acc 0.625, learning_rate 0.00108755\n","2023-06-13T15:33:49.454914: step 2833, loss 1.11635, acc 0.625, learning_rate 0.00108707\n","2023-06-13T15:33:50.919402: step 2834, loss 0.701595, acc 0.71875, learning_rate 0.00108659\n","2023-06-13T15:33:52.399243: step 2835, loss 0.711057, acc 0.78125, learning_rate 0.00108611\n","2023-06-13T15:33:53.878406: step 2836, loss 0.86791, acc 0.75, learning_rate 0.00108563\n","2023-06-13T15:33:55.356419: step 2837, loss 0.709419, acc 0.71875, learning_rate 0.00108516\n","2023-06-13T15:33:56.898807: step 2838, loss 0.967766, acc 0.65625, learning_rate 0.00108468\n","2023-06-13T15:33:59.477588: step 2839, loss 1.00067, acc 0.65625, learning_rate 0.0010842\n","2023-06-13T15:34:01.971755: step 2840, loss 1.0998, acc 0.59375, learning_rate 0.00108372\n","2023-06-13T15:34:04.280985: step 2841, loss 1.0722, acc 0.65625, learning_rate 0.00108325\n","2023-06-13T15:34:06.624045: step 2842, loss 0.999072, acc 0.625, learning_rate 0.00108277\n","2023-06-13T15:34:08.349728: step 2843, loss 1.00332, acc 0.65625, learning_rate 0.00108229\n","2023-06-13T15:34:09.841052: step 2844, loss 0.899025, acc 0.78125, learning_rate 0.00108182\n","2023-06-13T15:34:11.297667: step 2845, loss 0.719328, acc 0.78125, learning_rate 0.00108134\n","2023-06-13T15:34:12.775992: step 2846, loss 1.16275, acc 0.5625, learning_rate 0.00108086\n","2023-06-13T15:34:14.281802: step 2847, loss 0.765204, acc 0.75, learning_rate 0.00108039\n","2023-06-13T15:34:15.741302: step 2848, loss 0.82767, acc 0.6875, learning_rate 0.00107991\n","2023-06-13T15:34:17.209805: step 2849, loss 1.27591, acc 0.5625, learning_rate 0.00107944\n","2023-06-13T15:34:19.668579: step 2850, loss 1.30683, acc 0.625, learning_rate 0.00107896\n","2023-06-13T15:34:22.199646: step 2851, loss 0.650186, acc 0.6875, learning_rate 0.00107849\n","2023-06-13T15:34:24.574712: step 2852, loss 1.09575, acc 0.625, learning_rate 0.00107801\n","2023-06-13T15:34:26.996452: step 2853, loss 1.50528, acc 0.5, learning_rate 0.00107754\n","2023-06-13T15:34:28.732213: step 2854, loss 1.15701, acc 0.59375, learning_rate 0.00107707\n","2023-06-13T15:34:30.217584: step 2855, loss 1.10573, acc 0.65625, learning_rate 0.00107659\n","2023-06-13T15:34:31.658688: step 2856, loss 1.11084, acc 0.65625, learning_rate 0.00107612\n","2023-06-13T15:34:33.092959: step 2857, loss 0.666719, acc 0.8125, learning_rate 0.00107565\n","2023-06-13T15:34:34.526059: step 2858, loss 1.05214, acc 0.65625, learning_rate 0.00107517\n","2023-06-13T15:34:36.057207: step 2859, loss 0.909307, acc 0.625, learning_rate 0.0010747\n","2023-06-13T15:34:37.543316: step 2860, loss 0.76306, acc 0.71875, learning_rate 0.00107423\n","2023-06-13T15:34:40.016298: step 2861, loss 0.918578, acc 0.6875, learning_rate 0.00107376\n","2023-06-13T15:34:42.475348: step 2862, loss 0.706197, acc 0.75, learning_rate 0.00107328\n","2023-06-13T15:34:44.835688: step 2863, loss 0.565262, acc 0.8125, learning_rate 0.00107281\n","2023-06-13T15:34:47.165255: step 2864, loss 0.789905, acc 0.71875, learning_rate 0.00107234\n","2023-06-13T15:34:49.062281: step 2865, loss 1.13133, acc 0.625, learning_rate 0.00107187\n","2023-06-13T15:34:50.523253: step 2866, loss 1.25949, acc 0.625, learning_rate 0.0010714\n","2023-06-13T15:34:52.047610: step 2867, loss 0.80185, acc 0.71875, learning_rate 0.00107093\n","2023-06-13T15:34:53.490295: step 2868, loss 0.798787, acc 0.78125, learning_rate 0.00107045\n","2023-06-13T15:34:54.967994: step 2869, loss 0.999952, acc 0.75, learning_rate 0.00106998\n","2023-06-13T15:34:56.427031: step 2870, loss 0.635439, acc 0.75, learning_rate 0.00106951\n","2023-06-13T15:34:57.886816: step 2871, loss 0.825332, acc 0.71875, learning_rate 0.00106904\n","2023-06-13T15:35:00.076720: step 2872, loss 0.772149, acc 0.75, learning_rate 0.00106857\n","2023-06-13T15:35:02.519580: step 2873, loss 0.902036, acc 0.71875, learning_rate 0.0010681\n","2023-06-13T15:35:04.974063: step 2874, loss 0.80038, acc 0.75, learning_rate 0.00106763\n","2023-06-13T15:35:07.368762: step 2875, loss 1.09159, acc 0.6875, learning_rate 0.00106717\n","2023-06-13T15:35:09.267750: step 2876, loss 0.933699, acc 0.625, learning_rate 0.0010667\n","2023-06-13T15:35:10.740640: step 2877, loss 0.581313, acc 0.8125, learning_rate 0.00106623\n","2023-06-13T15:35:12.223142: step 2878, loss 0.857136, acc 0.71875, learning_rate 0.00106576\n","2023-06-13T15:35:13.720173: step 2879, loss 0.740558, acc 0.71875, learning_rate 0.00106529\n","2023-06-13T15:35:15.183037: step 2880, loss 1.09552, acc 0.625, learning_rate 0.00106482\n","2023-06-13T15:35:16.623088: step 2881, loss 0.512678, acc 0.8125, learning_rate 0.00106436\n","2023-06-13T15:35:18.087629: step 2882, loss 0.793925, acc 0.6875, learning_rate 0.00106389\n","2023-06-13T15:35:20.388285: step 2883, loss 0.610102, acc 0.75, learning_rate 0.00106342\n","2023-06-13T15:35:22.904182: step 2884, loss 0.947293, acc 0.6875, learning_rate 0.00106295\n","2023-06-13T15:35:25.161539: step 2885, loss 1.03474, acc 0.65625, learning_rate 0.00106249\n","2023-06-13T15:35:27.633293: step 2886, loss 0.89159, acc 0.75, learning_rate 0.00106202\n","2023-06-13T15:35:29.566689: step 2887, loss 0.731508, acc 0.71875, learning_rate 0.00106155\n","2023-06-13T15:35:31.030433: step 2888, loss 0.89164, acc 0.59375, learning_rate 0.00106109\n","2023-06-13T15:35:32.486495: step 2889, loss 1.07356, acc 0.5625, learning_rate 0.00106062\n","2023-06-13T15:35:33.937766: step 2890, loss 0.613147, acc 0.78125, learning_rate 0.00106016\n","2023-06-13T15:35:35.392303: step 2891, loss 1.12296, acc 0.65625, learning_rate 0.00105969\n","2023-06-13T15:35:36.847362: step 2892, loss 0.839596, acc 0.71875, learning_rate 0.00105922\n","2023-06-13T15:35:38.326877: step 2893, loss 1.22115, acc 0.53125, learning_rate 0.00105876\n","2023-06-13T15:35:40.416703: step 2894, loss 1.27731, acc 0.5625, learning_rate 0.00105829\n","2023-06-13T15:35:43.011392: step 2895, loss 0.97406, acc 0.75, learning_rate 0.00105783\n","2023-06-13T15:35:45.497268: step 2896, loss 1.03011, acc 0.71875, learning_rate 0.00105737\n","2023-06-13T15:35:47.834980: step 2897, loss 0.914111, acc 0.71875, learning_rate 0.0010569\n","2023-06-13T15:35:49.831044: step 2898, loss 1.51487, acc 0.6875, learning_rate 0.00105644\n","2023-06-13T15:35:51.274171: step 2899, loss 1.08829, acc 0.59375, learning_rate 0.00105597\n","\n","Evaluation:\n","2023-06-13T15:36:19.525280: step 2900, loss 2.10411, acc 0.388603\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-2900\n","\n","2023-06-13T15:36:21.829524: step 2900, loss 0.772132, acc 0.75, learning_rate 0.00105551\n","2023-06-13T15:36:24.470022: step 2901, loss 0.964387, acc 0.65625, learning_rate 0.00105505\n","2023-06-13T15:36:26.994160: step 2902, loss 1.30172, acc 0.625, learning_rate 0.00105458\n","2023-06-13T15:36:29.440986: step 2903, loss 0.932479, acc 0.65625, learning_rate 0.00105412\n","2023-06-13T15:36:31.338556: step 2904, loss 0.697607, acc 0.6875, learning_rate 0.00105366\n","2023-06-13T15:36:32.798217: step 2905, loss 0.989893, acc 0.71875, learning_rate 0.0010532\n","2023-06-13T15:36:34.259244: step 2906, loss 0.65239, acc 0.8125, learning_rate 0.00105273\n","2023-06-13T15:36:35.758426: step 2907, loss 0.711207, acc 0.75, learning_rate 0.00105227\n","2023-06-13T15:36:37.246500: step 2908, loss 0.88492, acc 0.65625, learning_rate 0.00105181\n","2023-06-13T15:36:38.699906: step 2909, loss 0.713488, acc 0.78125, learning_rate 0.00105135\n","2023-06-13T15:36:40.153372: step 2910, loss 0.954553, acc 0.8125, learning_rate 0.00105089\n","2023-06-13T15:36:42.511655: step 2911, loss 1.44964, acc 0.46875, learning_rate 0.00105043\n","2023-06-13T15:36:45.092990: step 2912, loss 1.07721, acc 0.59375, learning_rate 0.00104996\n","2023-06-13T15:36:47.454435: step 2913, loss 0.897204, acc 0.75, learning_rate 0.0010495\n","2023-06-13T15:36:49.783564: step 2914, loss 0.729616, acc 0.6875, learning_rate 0.00104904\n","2023-06-13T15:36:51.634645: step 2915, loss 1.04288, acc 0.625, learning_rate 0.00104858\n","2023-06-13T15:36:53.102285: step 2916, loss 1.19763, acc 0.59375, learning_rate 0.00104812\n","2023-06-13T15:36:54.581637: step 2917, loss 0.952144, acc 0.625, learning_rate 0.00104766\n","2023-06-13T15:36:56.038035: step 2918, loss 0.541681, acc 0.84375, learning_rate 0.0010472\n","2023-06-13T15:36:57.492385: step 2919, loss 0.97197, acc 0.6875, learning_rate 0.00104675\n","2023-06-13T15:36:58.898438: step 2920, loss 0.635368, acc 0.875, learning_rate 0.00104629\n","2023-06-13T15:37:00.371061: step 2921, loss 1.04673, acc 0.71875, learning_rate 0.00104583\n","2023-06-13T15:37:02.582634: step 2922, loss 1.45637, acc 0.46875, learning_rate 0.00104537\n","2023-06-13T15:37:05.011194: step 2923, loss 1.1075, acc 0.625, learning_rate 0.00104491\n","2023-06-13T15:37:07.323358: step 2924, loss 1.40481, acc 0.5, learning_rate 0.00104445\n","2023-06-13T15:37:09.604771: step 2925, loss 0.896237, acc 0.71875, learning_rate 0.00104399\n","2023-06-13T15:37:11.648604: step 2926, loss 0.816118, acc 0.71875, learning_rate 0.00104354\n","2023-06-13T15:37:13.076611: step 2927, loss 0.975305, acc 0.6875, learning_rate 0.00104308\n","2023-06-13T15:37:14.505695: step 2928, loss 0.809921, acc 0.71875, learning_rate 0.00104262\n","2023-06-13T15:37:15.940158: step 2929, loss 0.796266, acc 0.6875, learning_rate 0.00104216\n","2023-06-13T15:37:17.358289: step 2930, loss 1.64054, acc 0.4375, learning_rate 0.00104171\n","2023-06-13T15:37:18.777242: step 2931, loss 1.52298, acc 0.5625, learning_rate 0.00104125\n","2023-06-13T15:37:20.205093: step 2932, loss 0.761769, acc 0.78125, learning_rate 0.00104079\n","2023-06-13T15:37:21.913055: step 2933, loss 0.784691, acc 0.71875, learning_rate 0.00104034\n","2023-06-13T15:37:24.563639: step 2934, loss 0.957362, acc 0.6875, learning_rate 0.00103988\n","2023-06-13T15:37:27.023690: step 2935, loss 0.692578, acc 0.78125, learning_rate 0.00103943\n","2023-06-13T15:37:29.697125: step 2936, loss 0.916211, acc 0.65625, learning_rate 0.00103897\n","2023-06-13T15:37:32.379867: step 2937, loss 0.826341, acc 0.75, learning_rate 0.00103852\n","2023-06-13T15:37:35.047616: step 2938, loss 1.41343, acc 0.5625, learning_rate 0.00103806\n","2023-06-13T15:37:37.261139: step 2939, loss 1.14705, acc 0.53125, learning_rate 0.00103761\n","2023-06-13T15:37:39.581264: step 2940, loss 0.93685, acc 0.6875, learning_rate 0.00103715\n","2023-06-13T15:37:41.586086: step 2941, loss 0.907598, acc 0.59375, learning_rate 0.0010367\n","2023-06-13T15:37:43.022086: step 2942, loss 0.795859, acc 0.75, learning_rate 0.00103624\n","2023-06-13T15:37:44.676368: step 2943, loss 1.13328, acc 0.59375, learning_rate 0.00103579\n","2023-06-13T15:37:47.282163: step 2944, loss 0.637079, acc 0.78125, learning_rate 0.00103534\n","2023-06-13T15:37:49.627658: step 2945, loss 0.865793, acc 0.625, learning_rate 0.00103488\n","2023-06-13T15:37:51.926700: step 2946, loss 1.38638, acc 0.5625, learning_rate 0.00103443\n","2023-06-13T15:37:54.206913: step 2947, loss 0.962058, acc 0.625, learning_rate 0.00103398\n","2023-06-13T15:37:55.743538: step 2948, loss 0.788232, acc 0.78125, learning_rate 0.00103352\n","2023-06-13T15:37:57.176771: step 2949, loss 0.889358, acc 0.65625, learning_rate 0.00103307\n","2023-06-13T15:37:58.647343: step 2950, loss 0.929367, acc 0.6875, learning_rate 0.00103262\n","2023-06-13T15:38:00.098774: step 2951, loss 0.700638, acc 0.65625, learning_rate 0.00103217\n","2023-06-13T15:38:01.580362: step 2952, loss 0.759549, acc 0.65625, learning_rate 0.00103171\n","2023-06-13T15:38:03.019455: step 2953, loss 1.00128, acc 0.8125, learning_rate 0.00103126\n","2023-06-13T15:38:04.459067: step 2954, loss 1.19541, acc 0.625, learning_rate 0.00103081\n","2023-06-13T15:38:06.928434: step 2955, loss 1.1005, acc 0.6875, learning_rate 0.00103036\n","2023-06-13T15:38:09.415779: step 2956, loss 0.822224, acc 0.6875, learning_rate 0.00102991\n","2023-06-13T15:38:11.813163: step 2957, loss 0.729307, acc 0.78125, learning_rate 0.00102946\n","2023-06-13T15:38:14.221043: step 2958, loss 0.926637, acc 0.71875, learning_rate 0.00102901\n","2023-06-13T15:38:15.654074: step 2959, loss 0.913644, acc 0.6875, learning_rate 0.00102856\n","2023-06-13T15:38:17.130289: step 2960, loss 1.03459, acc 0.71875, learning_rate 0.00102811\n","2023-06-13T15:38:18.553102: step 2961, loss 0.970573, acc 0.65625, learning_rate 0.00102766\n","2023-06-13T15:38:19.965215: step 2962, loss 0.935037, acc 0.625, learning_rate 0.00102721\n","2023-06-13T15:38:21.396015: step 2963, loss 0.94742, acc 0.625, learning_rate 0.00102676\n","2023-06-13T15:38:22.842334: step 2964, loss 0.825323, acc 0.6875, learning_rate 0.00102631\n","2023-06-13T15:38:24.457910: step 2965, loss 1.0642, acc 0.5625, learning_rate 0.00102586\n","2023-06-13T15:38:27.027856: step 2966, loss 1.45585, acc 0.46875, learning_rate 0.00102541\n","2023-06-13T15:38:29.558594: step 2967, loss 0.835765, acc 0.65625, learning_rate 0.00102496\n","2023-06-13T15:38:32.022580: step 2968, loss 0.854964, acc 0.59375, learning_rate 0.00102451\n","2023-06-13T15:38:34.252833: step 2969, loss 1.14662, acc 0.625, learning_rate 0.00102406\n","2023-06-13T15:38:35.688706: step 2970, loss 0.922835, acc 0.625, learning_rate 0.00102362\n","2023-06-13T15:38:37.155473: step 2971, loss 1.11234, acc 0.625, learning_rate 0.00102317\n","2023-06-13T15:38:38.603811: step 2972, loss 0.946112, acc 0.65625, learning_rate 0.00102272\n","2023-06-13T15:38:40.048719: step 2973, loss 0.927522, acc 0.6875, learning_rate 0.00102227\n","2023-06-13T15:38:41.482344: step 2974, loss 0.763416, acc 0.875, learning_rate 0.00102182\n","2023-06-13T15:38:42.899960: step 2975, loss 1.05262, acc 0.625, learning_rate 0.00102138\n","2023-06-13T15:38:44.499743: step 2976, loss 1.11043, acc 0.59375, learning_rate 0.00102093\n","2023-06-13T15:38:47.028055: step 2977, loss 1.47864, acc 0.625, learning_rate 0.00102048\n","2023-06-13T15:38:49.580670: step 2978, loss 0.854249, acc 0.6875, learning_rate 0.00102004\n","2023-06-13T15:38:51.942495: step 2979, loss 1.37155, acc 0.65625, learning_rate 0.00101959\n","2023-06-13T15:38:54.285497: step 2980, loss 1.01413, acc 0.6875, learning_rate 0.00101915\n","2023-06-13T15:38:55.804058: step 2981, loss 1.13561, acc 0.71875, learning_rate 0.0010187\n","2023-06-13T15:38:57.278808: step 2982, loss 1.04801, acc 0.6875, learning_rate 0.00101826\n","2023-06-13T15:38:58.775601: step 2983, loss 0.958454, acc 0.6875, learning_rate 0.00101781\n","2023-06-13T15:39:00.218277: step 2984, loss 0.861266, acc 0.71875, learning_rate 0.00101736\n","2023-06-13T15:39:01.698642: step 2985, loss 0.806406, acc 0.71875, learning_rate 0.00101692\n","2023-06-13T15:39:03.160121: step 2986, loss 0.928579, acc 0.59375, learning_rate 0.00101648\n","2023-06-13T15:39:04.757276: step 2987, loss 0.925836, acc 0.59375, learning_rate 0.00101603\n","2023-06-13T15:39:07.499404: step 2988, loss 0.95236, acc 0.6875, learning_rate 0.00101559\n","2023-06-13T15:39:09.888669: step 2989, loss 1.26067, acc 0.46875, learning_rate 0.00101514\n","2023-06-13T15:39:12.265050: step 2990, loss 1.15357, acc 0.625, learning_rate 0.0010147\n","2023-06-13T15:39:14.688405: step 2991, loss 0.931574, acc 0.65625, learning_rate 0.00101426\n","2023-06-13T15:39:16.178324: step 2992, loss 1.24331, acc 0.59375, learning_rate 0.00101381\n","2023-06-13T15:39:17.664914: step 2993, loss 0.813828, acc 0.71875, learning_rate 0.00101337\n","2023-06-13T15:39:19.120632: step 2994, loss 0.91877, acc 0.6875, learning_rate 0.00101293\n","2023-06-13T15:39:20.583661: step 2995, loss 1.13789, acc 0.59375, learning_rate 0.00101248\n","2023-06-13T15:39:22.059710: step 2996, loss 0.592679, acc 0.8125, learning_rate 0.00101204\n","2023-06-13T15:39:23.550375: step 2997, loss 1.19331, acc 0.5625, learning_rate 0.0010116\n","2023-06-13T15:39:25.284618: step 2998, loss 0.661236, acc 0.78125, learning_rate 0.00101116\n","2023-06-13T15:39:27.888912: step 2999, loss 1.06723, acc 0.59375, learning_rate 0.00101072\n","\n","Evaluation:\n","2023-06-13T15:39:59.123140: step 3000, loss 2.06273, acc 0.389815\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3000\n","\n","2023-06-13T15:40:00.759145: step 3000, loss 1.08887, acc 0.6875, learning_rate 0.00101027\n","2023-06-13T15:40:02.208497: step 3001, loss 0.867875, acc 0.625, learning_rate 0.00100983\n","2023-06-13T15:40:03.650762: step 3002, loss 1.10374, acc 0.65625, learning_rate 0.00100939\n","2023-06-13T15:40:05.067755: step 3003, loss 0.952004, acc 0.75, learning_rate 0.00100895\n","2023-06-13T15:40:06.684831: step 3004, loss 0.800271, acc 0.75, learning_rate 0.00100851\n","2023-06-13T15:40:09.451351: step 3005, loss 0.896472, acc 0.6875, learning_rate 0.00100807\n","2023-06-13T15:40:11.859655: step 3006, loss 0.979202, acc 0.625, learning_rate 0.00100763\n","2023-06-13T15:40:14.313502: step 3007, loss 0.700799, acc 0.875, learning_rate 0.00100719\n","2023-06-13T15:40:16.617394: step 3008, loss 0.963456, acc 0.625, learning_rate 0.00100675\n","2023-06-13T15:40:18.088532: step 3009, loss 0.961619, acc 0.6875, learning_rate 0.00100631\n","2023-06-13T15:40:19.535907: step 3010, loss 1.01622, acc 0.625, learning_rate 0.00100587\n","2023-06-13T15:40:20.964386: step 3011, loss 0.690524, acc 0.8125, learning_rate 0.00100543\n","2023-06-13T15:40:22.423739: step 3012, loss 0.781883, acc 0.75, learning_rate 0.00100499\n","2023-06-13T15:40:23.877563: step 3013, loss 1.06794, acc 0.65625, learning_rate 0.00100455\n","2023-06-13T15:40:25.334946: step 3014, loss 1.67647, acc 0.59375, learning_rate 0.00100411\n","2023-06-13T15:40:27.039642: step 3015, loss 0.772944, acc 0.8125, learning_rate 0.00100368\n","2023-06-13T15:40:29.704284: step 3016, loss 0.983501, acc 0.6875, learning_rate 0.00100324\n","2023-06-13T15:40:32.269635: step 3017, loss 0.714641, acc 0.78125, learning_rate 0.0010028\n","2023-06-13T15:40:34.698148: step 3018, loss 0.889722, acc 0.6875, learning_rate 0.00100236\n","2023-06-13T15:40:36.945010: step 3019, loss 0.772458, acc 0.75, learning_rate 0.00100192\n","2023-06-13T15:40:38.398024: step 3020, loss 1.11206, acc 0.59375, learning_rate 0.00100149\n","2023-06-13T15:40:39.899683: step 3021, loss 0.95843, acc 0.6875, learning_rate 0.00100105\n","2023-06-13T15:40:41.375107: step 3022, loss 1.05165, acc 0.59375, learning_rate 0.00100061\n","2023-06-13T15:40:42.830824: step 3023, loss 0.974922, acc 0.65625, learning_rate 0.00100018\n","2023-06-13T15:40:44.327846: step 3024, loss 1.3122, acc 0.5625, learning_rate 0.00099974\n","2023-06-13T15:40:45.802409: step 3025, loss 1.16706, acc 0.625, learning_rate 0.000999304\n","2023-06-13T15:40:47.750744: step 3026, loss 0.88129, acc 0.6875, learning_rate 0.000998867\n","2023-06-13T15:40:50.246418: step 3027, loss 0.913416, acc 0.71875, learning_rate 0.000998432\n","2023-06-13T15:40:52.698452: step 3028, loss 0.721849, acc 0.75, learning_rate 0.000997996\n","2023-06-13T15:40:55.087678: step 3029, loss 0.932902, acc 0.65625, learning_rate 0.000997561\n","2023-06-13T15:40:57.333445: step 3030, loss 0.829879, acc 0.6875, learning_rate 0.000997125\n","2023-06-13T15:40:58.815003: step 3031, loss 1.07424, acc 0.71875, learning_rate 0.00099669\n","2023-06-13T15:41:00.302688: step 3032, loss 1.08627, acc 0.6875, learning_rate 0.000996256\n","2023-06-13T15:41:01.796724: step 3033, loss 0.849224, acc 0.71875, learning_rate 0.000995821\n","2023-06-13T15:41:03.272068: step 3034, loss 0.87904, acc 0.625, learning_rate 0.000995387\n","2023-06-13T15:41:04.772387: step 3035, loss 1.17629, acc 0.5625, learning_rate 0.000994952\n","2023-06-13T15:41:06.213190: step 3036, loss 0.938713, acc 0.78125, learning_rate 0.000994519\n","2023-06-13T15:41:08.169683: step 3037, loss 0.819279, acc 0.6875, learning_rate 0.000994085\n","2023-06-13T15:41:10.814750: step 3038, loss 0.858276, acc 0.84375, learning_rate 0.000993651\n","2023-06-13T15:41:13.157092: step 3039, loss 0.857677, acc 0.59375, learning_rate 0.000993218\n","2023-06-13T15:41:15.451892: step 3040, loss 0.616966, acc 0.71875, learning_rate 0.000992785\n","2023-06-13T15:41:17.774901: step 3041, loss 0.715542, acc 0.78125, learning_rate 0.000992352\n","2023-06-13T15:41:19.321451: step 3042, loss 0.522233, acc 0.8125, learning_rate 0.000991919\n","2023-06-13T15:41:20.752213: step 3043, loss 1.06266, acc 0.6875, learning_rate 0.000991487\n","2023-06-13T15:41:22.229671: step 3044, loss 0.785922, acc 0.8125, learning_rate 0.000991055\n","2023-06-13T15:41:23.633512: step 3045, loss 1.29842, acc 0.5, learning_rate 0.000990623\n","2023-06-13T15:41:25.064823: step 3046, loss 0.917322, acc 0.71875, learning_rate 0.000990191\n","2023-06-13T15:41:26.517850: step 3047, loss 0.963702, acc 0.6875, learning_rate 0.000989759\n","2023-06-13T15:41:28.050939: step 3048, loss 0.82016, acc 0.78125, learning_rate 0.000989328\n","2023-06-13T15:41:30.710628: step 3049, loss 0.736647, acc 0.78125, learning_rate 0.000988896\n","2023-06-13T15:41:33.247922: step 3050, loss 0.873326, acc 0.625, learning_rate 0.000988465\n","2023-06-13T15:41:35.808591: step 3051, loss 1.0929, acc 0.5625, learning_rate 0.000988035\n","2023-06-13T15:41:38.109586: step 3052, loss 0.843645, acc 0.6875, learning_rate 0.000987604\n","2023-06-13T15:41:39.562477: step 3053, loss 0.956074, acc 0.6875, learning_rate 0.000987174\n","2023-06-13T15:41:41.044232: step 3054, loss 0.932719, acc 0.6875, learning_rate 0.000986743\n","2023-06-13T15:41:42.495696: step 3055, loss 0.965444, acc 0.71875, learning_rate 0.000986313\n","2023-06-13T15:41:43.970388: step 3056, loss 1.22463, acc 0.5625, learning_rate 0.000985884\n","2023-06-13T15:41:45.407586: step 3057, loss 0.721539, acc 0.78125, learning_rate 0.000985454\n","2023-06-13T15:41:46.875522: step 3058, loss 0.813256, acc 0.75, learning_rate 0.000985025\n","2023-06-13T15:41:48.589723: step 3059, loss 1.0904, acc 0.59375, learning_rate 0.000984596\n","2023-06-13T15:41:51.270202: step 3060, loss 1.23989, acc 0.5, learning_rate 0.000984167\n","2023-06-13T15:41:53.681563: step 3061, loss 1.09355, acc 0.59375, learning_rate 0.000983738\n","2023-06-13T15:41:56.052011: step 3062, loss 0.926883, acc 0.78125, learning_rate 0.00098331\n","2023-06-13T15:41:58.496696: step 3063, loss 1.05134, acc 0.59375, learning_rate 0.000982881\n","2023-06-13T15:41:59.965626: step 3064, loss 0.967182, acc 0.71875, learning_rate 0.000982453\n","2023-06-13T15:42:01.397983: step 3065, loss 0.966525, acc 0.65625, learning_rate 0.000982025\n","2023-06-13T15:42:02.875536: step 3066, loss 0.739488, acc 0.75, learning_rate 0.000981598\n","2023-06-13T15:42:04.357900: step 3067, loss 0.933707, acc 0.71875, learning_rate 0.00098117\n","2023-06-13T15:42:05.832713: step 3068, loss 0.703468, acc 0.6875, learning_rate 0.000980743\n","2023-06-13T15:42:07.342188: step 3069, loss 1.05796, acc 0.75, learning_rate 0.000980316\n","2023-06-13T15:42:08.994875: step 3070, loss 1.38349, acc 0.46875, learning_rate 0.000979889\n","2023-06-13T15:42:11.663819: step 3071, loss 0.756516, acc 0.8125, learning_rate 0.000979462\n","2023-06-13T15:42:14.137025: step 3072, loss 0.758175, acc 0.8125, learning_rate 0.000979036\n","2023-06-13T15:42:16.455144: step 3073, loss 0.668304, acc 0.8125, learning_rate 0.00097861\n","2023-06-13T15:42:18.862288: step 3074, loss 1.41034, acc 0.5, learning_rate 0.000978184\n","2023-06-13T15:42:20.308858: step 3075, loss 0.749983, acc 0.6875, learning_rate 0.000977758\n","2023-06-13T15:42:21.784243: step 3076, loss 0.720234, acc 0.75, learning_rate 0.000977332\n","2023-06-13T15:42:23.265564: step 3077, loss 0.422959, acc 0.84375, learning_rate 0.000976907\n","2023-06-13T15:42:24.707656: step 3078, loss 1.39633, acc 0.625, learning_rate 0.000976482\n","2023-06-13T15:42:26.156835: step 3079, loss 0.850268, acc 0.71875, learning_rate 0.000976057\n","2023-06-13T15:42:27.654001: step 3080, loss 0.590983, acc 0.78125, learning_rate 0.000975632\n","2023-06-13T15:42:29.433736: step 3081, loss 1.10347, acc 0.65625, learning_rate 0.000975207\n","2023-06-13T15:42:32.033930: step 3082, loss 1.15794, acc 0.4375, learning_rate 0.000974783\n","2023-06-13T15:42:34.528369: step 3083, loss 0.795241, acc 0.75, learning_rate 0.000974359\n","2023-06-13T15:42:37.019726: step 3084, loss 0.865792, acc 0.75, learning_rate 0.000973935\n","2023-06-13T15:42:39.269174: step 3085, loss 0.72005, acc 0.78125, learning_rate 0.000973511\n","2023-06-13T15:42:40.745600: step 3086, loss 0.927203, acc 0.65625, learning_rate 0.000973088\n","2023-06-13T15:42:42.208884: step 3087, loss 1.2427, acc 0.6875, learning_rate 0.000972664\n","2023-06-13T15:42:43.679000: step 3088, loss 1.14095, acc 0.6875, learning_rate 0.000972241\n","2023-06-13T15:42:45.126186: step 3089, loss 0.855586, acc 0.71875, learning_rate 0.000971818\n","2023-06-13T15:42:46.556416: step 3090, loss 0.994172, acc 0.75, learning_rate 0.000971395\n","2023-06-13T15:42:48.003400: step 3091, loss 0.696396, acc 0.78125, learning_rate 0.000970973\n","2023-06-13T15:42:49.830319: step 3092, loss 1.15794, acc 0.65625, learning_rate 0.000970551\n","2023-06-13T15:42:52.358240: step 3093, loss 0.787817, acc 0.59375, learning_rate 0.000970128\n","2023-06-13T15:42:54.741073: step 3094, loss 0.640632, acc 0.8125, learning_rate 0.000969707\n","2023-06-13T15:42:57.275250: step 3095, loss 1.48132, acc 0.5625, learning_rate 0.000969285\n","2023-06-13T15:42:59.468958: step 3096, loss 0.625606, acc 0.78125, learning_rate 0.000968863\n","2023-06-13T15:43:00.977989: step 3097, loss 0.777701, acc 0.71875, learning_rate 0.000968442\n","2023-06-13T15:43:02.431157: step 3098, loss 1.41562, acc 0.5625, learning_rate 0.000968021\n","2023-06-13T15:43:03.943126: step 3099, loss 0.904674, acc 0.625, learning_rate 0.0009676\n","\n","Evaluation:\n","2023-06-13T15:43:33.499406: step 3100, loss 2.09236, acc 0.390573\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3100\n","\n","2023-06-13T15:43:36.257842: step 3100, loss 1.02588, acc 0.59375, learning_rate 0.000967179\n","2023-06-13T15:43:38.594241: step 3101, loss 1.04965, acc 0.625, learning_rate 0.000966759\n","2023-06-13T15:43:41.074510: step 3102, loss 1.0872, acc 0.6875, learning_rate 0.000966339\n","2023-06-13T15:43:42.744910: step 3103, loss 0.860776, acc 0.6875, learning_rate 0.000965919\n","2023-06-13T15:43:44.235664: step 3104, loss 0.847687, acc 0.75, learning_rate 0.000965499\n","2023-06-13T15:43:45.682137: step 3105, loss 0.59776, acc 0.8125, learning_rate 0.000965079\n","2023-06-13T15:43:47.161289: step 3106, loss 1.35138, acc 0.65625, learning_rate 0.00096466\n","2023-06-13T15:43:48.608765: step 3107, loss 1.17121, acc 0.6875, learning_rate 0.00096424\n","2023-06-13T15:43:50.538365: step 3108, loss 1.8504, acc 0.375, learning_rate 0.000963821\n","2023-06-13T15:43:53.171835: step 3109, loss 0.869228, acc 0.5625, learning_rate 0.000963402\n","2023-06-13T15:43:55.940393: step 3110, loss 1.0383, acc 0.5625, learning_rate 0.000962984\n","2023-06-13T15:43:58.714857: step 3111, loss 1.04536, acc 0.65625, learning_rate 0.000962565\n","2023-06-13T15:44:01.328837: step 3112, loss 0.836386, acc 0.6875, learning_rate 0.000962147\n","2023-06-13T15:44:03.965980: step 3113, loss 0.781606, acc 0.71875, learning_rate 0.000961729\n","2023-06-13T15:44:06.511732: step 3114, loss 1.29367, acc 0.59375, learning_rate 0.000961311\n","2023-06-13T15:44:08.629711: step 3115, loss 0.944787, acc 0.625, learning_rate 0.000960894\n","2023-06-13T15:44:10.106643: step 3116, loss 1.33534, acc 0.5625, learning_rate 0.000960476\n","2023-06-13T15:44:11.560127: step 3117, loss 0.726713, acc 0.78125, learning_rate 0.000960059\n","2023-06-13T15:44:13.004474: step 3118, loss 1.32614, acc 0.625, learning_rate 0.000959642\n","2023-06-13T15:44:14.475429: step 3119, loss 1.23147, acc 0.5625, learning_rate 0.000959225\n","2023-06-13T15:44:15.918643: step 3120, loss 1.02064, acc 0.65625, learning_rate 0.000958809\n","2023-06-13T15:44:17.367701: step 3121, loss 1.15272, acc 0.59375, learning_rate 0.000958392\n","2023-06-13T15:44:19.207228: step 3122, loss 0.685794, acc 0.75, learning_rate 0.000957976\n","2023-06-13T15:44:21.778209: step 3123, loss 1.11699, acc 0.59375, learning_rate 0.00095756\n","2023-06-13T15:44:24.244664: step 3124, loss 0.98754, acc 0.625, learning_rate 0.000957144\n","2023-06-13T15:44:26.645502: step 3125, loss 0.929186, acc 0.71875, learning_rate 0.000956728\n","2023-06-13T15:44:28.886663: step 3126, loss 1.21543, acc 0.59375, learning_rate 0.000956313\n","2023-06-13T15:44:30.344968: step 3127, loss 0.869577, acc 0.65625, learning_rate 0.000955898\n","2023-06-13T15:44:31.791258: step 3128, loss 0.716869, acc 0.78125, learning_rate 0.000955483\n","2023-06-13T15:44:33.249992: step 3129, loss 0.883843, acc 0.6875, learning_rate 0.000955068\n","2023-06-13T15:44:34.710573: step 3130, loss 0.936967, acc 0.6875, learning_rate 0.000954653\n","2023-06-13T15:44:36.138038: step 3131, loss 0.990482, acc 0.71875, learning_rate 0.000954239\n","2023-06-13T15:44:37.620766: step 3132, loss 1.29218, acc 0.5625, learning_rate 0.000953825\n","2023-06-13T15:44:39.438505: step 3133, loss 0.813689, acc 0.59375, learning_rate 0.000953411\n","2023-06-13T15:44:42.052699: step 3134, loss 0.657924, acc 0.75, learning_rate 0.000952997\n","2023-06-13T15:44:44.400608: step 3135, loss 1.01538, acc 0.625, learning_rate 0.000952583\n","2023-06-13T15:44:46.817419: step 3136, loss 0.819855, acc 0.6875, learning_rate 0.00095217\n","2023-06-13T15:44:49.193922: step 3137, loss 1.09321, acc 0.5, learning_rate 0.000951757\n","2023-06-13T15:44:50.664005: step 3138, loss 1.11442, acc 0.71875, learning_rate 0.000951344\n","2023-06-13T15:44:52.138433: step 3139, loss 0.687825, acc 0.75, learning_rate 0.000950931\n","2023-06-13T15:44:53.581193: step 3140, loss 0.863206, acc 0.65625, learning_rate 0.000950518\n","2023-06-13T15:44:55.020063: step 3141, loss 0.668284, acc 0.78125, learning_rate 0.000950106\n","2023-06-13T15:44:56.476502: step 3142, loss 1.02147, acc 0.65625, learning_rate 0.000949694\n","2023-06-13T15:44:57.947595: step 3143, loss 0.843464, acc 0.71875, learning_rate 0.000949282\n","2023-06-13T15:44:59.602875: step 3144, loss 1.1266, acc 0.59375, learning_rate 0.00094887\n","2023-06-13T15:45:02.281922: step 3145, loss 0.549562, acc 0.8125, learning_rate 0.000948458\n","2023-06-13T15:45:04.625043: step 3146, loss 0.893772, acc 0.65625, learning_rate 0.000948047\n","2023-06-13T15:45:07.166619: step 3147, loss 0.789532, acc 0.71875, learning_rate 0.000947636\n","2023-06-13T15:45:09.488650: step 3148, loss 1.11068, acc 0.65625, learning_rate 0.000947225\n","2023-06-13T15:45:10.927728: step 3149, loss 1.12149, acc 0.65625, learning_rate 0.000946814\n","2023-06-13T15:45:12.415383: step 3150, loss 0.852937, acc 0.75, learning_rate 0.000946403\n","2023-06-13T15:45:13.868884: step 3151, loss 0.891261, acc 0.71875, learning_rate 0.000945993\n","2023-06-13T15:45:15.326511: step 3152, loss 0.885458, acc 0.65625, learning_rate 0.000945583\n","2023-06-13T15:45:16.783733: step 3153, loss 0.905819, acc 0.6875, learning_rate 0.000945173\n","2023-06-13T15:45:18.256414: step 3154, loss 1.29531, acc 0.46875, learning_rate 0.000944763\n","2023-06-13T15:45:20.015358: step 3155, loss 0.848569, acc 0.59375, learning_rate 0.000944353\n","2023-06-13T15:45:22.570405: step 3156, loss 1.08826, acc 0.625, learning_rate 0.000943944\n","2023-06-13T15:45:24.973553: step 3157, loss 0.909945, acc 0.78125, learning_rate 0.000943535\n","2023-06-13T15:45:27.327285: step 3158, loss 0.949733, acc 0.6875, learning_rate 0.000943126\n","2023-06-13T15:45:29.745154: step 3159, loss 0.829754, acc 0.78125, learning_rate 0.000942717\n","2023-06-13T15:45:31.386272: step 3160, loss 1.27219, acc 0.53125, learning_rate 0.000942308\n","2023-06-13T15:45:32.860137: step 3161, loss 0.710715, acc 0.78125, learning_rate 0.0009419\n","2023-06-13T15:45:34.321719: step 3162, loss 1.16848, acc 0.71875, learning_rate 0.000941492\n","2023-06-13T15:45:35.777203: step 3163, loss 0.776037, acc 0.71875, learning_rate 0.000941084\n","2023-06-13T15:45:37.218065: step 3164, loss 1.63877, acc 0.5625, learning_rate 0.000940676\n","2023-06-13T15:45:38.697879: step 3165, loss 0.942478, acc 0.6875, learning_rate 0.000940268\n","2023-06-13T15:45:40.178709: step 3166, loss 1.03557, acc 0.6875, learning_rate 0.000939861\n","2023-06-13T15:45:42.704679: step 3167, loss 1.02887, acc 0.59375, learning_rate 0.000939454\n","2023-06-13T15:45:45.246248: step 3168, loss 1.21222, acc 0.59375, learning_rate 0.000939047\n","2023-06-13T15:45:47.485717: step 3169, loss 0.880766, acc 0.75, learning_rate 0.00093864\n","2023-06-13T15:45:49.947665: step 3170, loss 1.00728, acc 0.625, learning_rate 0.000938233\n","2023-06-13T15:45:51.790263: step 3171, loss 1.08293, acc 0.5625, learning_rate 0.000937827\n","2023-06-13T15:45:53.266562: step 3172, loss 1.08566, acc 0.625, learning_rate 0.00093742\n","2023-06-13T15:45:54.705484: step 3173, loss 0.96008, acc 0.65625, learning_rate 0.000937014\n","2023-06-13T15:45:56.170929: step 3174, loss 0.981655, acc 0.71875, learning_rate 0.000936609\n","2023-06-13T15:45:57.643519: step 3175, loss 0.460048, acc 0.75, learning_rate 0.000936203\n","2023-06-13T15:45:59.115916: step 3176, loss 0.696891, acc 0.75, learning_rate 0.000935797\n","2023-06-13T15:46:00.554143: step 3177, loss 1.22174, acc 0.5625, learning_rate 0.000935392\n","2023-06-13T15:46:02.857577: step 3178, loss 1.33688, acc 0.5, learning_rate 0.000934987\n","2023-06-13T15:46:05.490565: step 3179, loss 0.599529, acc 0.78125, learning_rate 0.000934582\n","2023-06-13T15:46:08.016782: step 3180, loss 1.02848, acc 0.6875, learning_rate 0.000934178\n","2023-06-13T15:46:10.374301: step 3181, loss 0.98464, acc 0.59375, learning_rate 0.000933773\n","2023-06-13T15:46:12.175878: step 3182, loss 1.03347, acc 0.59375, learning_rate 0.000933369\n","2023-06-13T15:46:13.618279: step 3183, loss 0.822805, acc 0.71875, learning_rate 0.000932965\n","2023-06-13T15:46:15.036204: step 3184, loss 0.816395, acc 0.71875, learning_rate 0.000932561\n","2023-06-13T15:46:16.488152: step 3185, loss 0.976161, acc 0.75, learning_rate 0.000932157\n","2023-06-13T15:46:17.948783: step 3186, loss 0.845121, acc 0.75, learning_rate 0.000931754\n","2023-06-13T15:46:19.390054: step 3187, loss 1.10958, acc 0.71875, learning_rate 0.00093135\n","2023-06-13T15:46:20.889145: step 3188, loss 0.931069, acc 0.6875, learning_rate 0.000930947\n","2023-06-13T15:46:23.029230: step 3189, loss 0.881515, acc 0.78125, learning_rate 0.000930544\n","2023-06-13T15:46:25.704081: step 3190, loss 0.945235, acc 0.6875, learning_rate 0.000930142\n","2023-06-13T15:46:28.126004: step 3191, loss 1.36775, acc 0.5625, learning_rate 0.000929739\n","2023-06-13T15:46:30.435246: step 3192, loss 0.623696, acc 0.78125, learning_rate 0.000929337\n","2023-06-13T15:46:32.472860: step 3193, loss 1.08802, acc 0.625, learning_rate 0.000928935\n","2023-06-13T15:46:33.954723: step 3194, loss 1.14614, acc 0.65625, learning_rate 0.000928533\n","2023-06-13T15:46:35.378634: step 3195, loss 0.783462, acc 0.59375, learning_rate 0.000928131\n","2023-06-13T15:46:36.815058: step 3196, loss 0.653063, acc 0.75, learning_rate 0.000927729\n","2023-06-13T15:46:38.321774: step 3197, loss 1.33157, acc 0.59375, learning_rate 0.000927328\n","2023-06-13T15:46:39.824405: step 3198, loss 0.718187, acc 0.8125, learning_rate 0.000926927\n","2023-06-13T15:46:41.396048: step 3199, loss 1.06632, acc 0.65625, learning_rate 0.000926526\n","\n","Evaluation:\n","2023-06-13T15:47:14.848346: step 3200, loss 2.07593, acc 0.390421\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3200\n","\n","2023-06-13T15:47:16.581707: step 3200, loss 0.611426, acc 0.78125, learning_rate 0.000926125\n","2023-06-13T15:47:18.077157: step 3201, loss 1.02171, acc 0.71875, learning_rate 0.000925725\n","2023-06-13T15:47:19.524305: step 3202, loss 1.2436, acc 0.6875, learning_rate 0.000925324\n","2023-06-13T15:47:20.997392: step 3203, loss 0.976586, acc 0.75, learning_rate 0.000924924\n","2023-06-13T15:47:22.448463: step 3204, loss 0.967962, acc 0.5625, learning_rate 0.000924524\n","2023-06-13T15:47:23.954764: step 3205, loss 1.10889, acc 0.5625, learning_rate 0.000924124\n","2023-06-13T15:47:25.708431: step 3206, loss 0.928783, acc 0.6875, learning_rate 0.000923725\n","2023-06-13T15:47:28.350047: step 3207, loss 0.916803, acc 0.59375, learning_rate 0.000923325\n","2023-06-13T15:47:30.918651: step 3208, loss 0.946553, acc 0.71875, learning_rate 0.000922926\n","2023-06-13T15:47:33.214047: step 3209, loss 0.826722, acc 0.71875, learning_rate 0.000922527\n","2023-06-13T15:47:35.531211: step 3210, loss 0.736076, acc 0.65625, learning_rate 0.000922128\n","2023-06-13T15:47:37.006991: step 3211, loss 0.622792, acc 0.71875, learning_rate 0.00092173\n","2023-06-13T15:47:38.512972: step 3212, loss 0.912845, acc 0.59375, learning_rate 0.000921331\n","2023-06-13T15:47:39.963644: step 3213, loss 1.05491, acc 0.65625, learning_rate 0.000920933\n","2023-06-13T15:47:41.416343: step 3214, loss 1.34284, acc 0.5, learning_rate 0.000920535\n","2023-06-13T15:47:42.886187: step 3215, loss 0.750395, acc 0.78125, learning_rate 0.000920137\n","2023-06-13T15:47:44.381326: step 3216, loss 0.903843, acc 0.71875, learning_rate 0.000919739\n","2023-06-13T15:47:46.058706: step 3217, loss 0.811987, acc 0.75, learning_rate 0.000919342\n","2023-06-13T15:47:48.660674: step 3218, loss 0.824531, acc 0.71875, learning_rate 0.000918945\n","2023-06-13T15:47:51.136425: step 3219, loss 1.08871, acc 0.59375, learning_rate 0.000918548\n","2023-06-13T15:47:53.449788: step 3220, loss 0.969326, acc 0.65625, learning_rate 0.000918151\n","2023-06-13T15:47:55.823977: step 3221, loss 0.949678, acc 0.75, learning_rate 0.000917754\n","2023-06-13T15:47:57.388147: step 3222, loss 0.823, acc 0.71875, learning_rate 0.000917357\n","2023-06-13T15:47:58.857225: step 3223, loss 1.26399, acc 0.5625, learning_rate 0.000916961\n","2023-06-13T15:48:00.304504: step 3224, loss 0.945424, acc 0.71875, learning_rate 0.000916565\n","2023-06-13T15:48:01.798772: step 3225, loss 0.734727, acc 0.78125, learning_rate 0.000916169\n","2023-06-13T15:48:03.287390: step 3226, loss 0.698232, acc 0.78125, learning_rate 0.000915773\n","2023-06-13T15:48:04.769168: step 3227, loss 1.27737, acc 0.59375, learning_rate 0.000915378\n","2023-06-13T15:48:06.381397: step 3228, loss 1.10973, acc 0.65625, learning_rate 0.000914982\n","2023-06-13T15:48:08.971537: step 3229, loss 0.974097, acc 0.71875, learning_rate 0.000914587\n","2023-06-13T15:48:11.466146: step 3230, loss 1.10899, acc 0.625, learning_rate 0.000914192\n","2023-06-13T15:48:13.898798: step 3231, loss 1.1143, acc 0.625, learning_rate 0.000913797\n","2023-06-13T15:48:16.290197: step 3232, loss 1.14896, acc 0.65625, learning_rate 0.000913403\n","2023-06-13T15:48:17.766629: step 3233, loss 1.02663, acc 0.59375, learning_rate 0.000913008\n","2023-06-13T15:48:19.228652: step 3234, loss 1.12311, acc 0.59375, learning_rate 0.000912614\n","2023-06-13T15:48:20.698628: step 3235, loss 0.858709, acc 0.6875, learning_rate 0.00091222\n","2023-06-13T15:48:22.162847: step 3236, loss 0.955522, acc 0.78125, learning_rate 0.000911826\n","2023-06-13T15:48:23.617512: step 3237, loss 0.818065, acc 0.78125, learning_rate 0.000911433\n","2023-06-13T15:48:25.066169: step 3238, loss 0.841048, acc 0.65625, learning_rate 0.000911039\n","2023-06-13T15:48:26.933548: step 3239, loss 0.989744, acc 0.59375, learning_rate 0.000910646\n","2023-06-13T15:48:29.523921: step 3240, loss 0.781654, acc 0.75, learning_rate 0.000910253\n","2023-06-13T15:48:31.938968: step 3241, loss 0.886438, acc 0.6875, learning_rate 0.00090986\n","2023-06-13T15:48:34.388447: step 3242, loss 0.663475, acc 0.75, learning_rate 0.000909467\n","2023-06-13T15:48:36.656800: step 3243, loss 0.974913, acc 0.71875, learning_rate 0.000909075\n","2023-06-13T15:48:38.140573: step 3244, loss 0.768135, acc 0.78125, learning_rate 0.000908683\n","2023-06-13T15:48:39.642181: step 3245, loss 0.766859, acc 0.71875, learning_rate 0.000908291\n","2023-06-13T15:48:41.100915: step 3246, loss 0.93783, acc 0.71875, learning_rate 0.000907899\n","2023-06-13T15:48:42.557529: step 3247, loss 0.929159, acc 0.75, learning_rate 0.000907507\n","2023-06-13T15:48:43.994764: step 3248, loss 0.869634, acc 0.71875, learning_rate 0.000907115\n","2023-06-13T15:48:45.478715: step 3249, loss 1.2459, acc 0.65625, learning_rate 0.000906724\n","2023-06-13T15:48:47.369535: step 3250, loss 0.850576, acc 0.625, learning_rate 0.000906333\n","2023-06-13T15:48:50.051007: step 3251, loss 0.804993, acc 0.78125, learning_rate 0.000905942\n","2023-06-13T15:48:52.517298: step 3252, loss 0.797427, acc 0.5625, learning_rate 0.000905551\n","2023-06-13T15:48:54.945325: step 3253, loss 0.967918, acc 0.625, learning_rate 0.00090516\n","2023-06-13T15:48:57.088135: step 3254, loss 1.0612, acc 0.59375, learning_rate 0.00090477\n","2023-06-13T15:48:58.554025: step 3255, loss 1.2608, acc 0.65625, learning_rate 0.00090438\n","2023-06-13T15:49:00.019891: step 3256, loss 0.743941, acc 0.75, learning_rate 0.00090399\n","2023-06-13T15:49:01.473115: step 3257, loss 0.931196, acc 0.6875, learning_rate 0.0009036\n","2023-06-13T15:49:02.939538: step 3258, loss 1.0481, acc 0.625, learning_rate 0.00090321\n","2023-06-13T15:49:04.376195: step 3259, loss 0.806087, acc 0.71875, learning_rate 0.000902821\n","2023-06-13T15:49:05.840779: step 3260, loss 1.08288, acc 0.5625, learning_rate 0.000902432\n","2023-06-13T15:49:07.651773: step 3261, loss 1.32846, acc 0.6875, learning_rate 0.000902043\n","2023-06-13T15:49:10.267295: step 3262, loss 0.898099, acc 0.6875, learning_rate 0.000901654\n","2023-06-13T15:49:12.779920: step 3263, loss 0.830364, acc 0.6875, learning_rate 0.000901265\n","2023-06-13T15:49:15.092709: step 3264, loss 1.03065, acc 0.5625, learning_rate 0.000900876\n","2023-06-13T15:49:17.412104: step 3265, loss 1.22666, acc 0.625, learning_rate 0.000900488\n","2023-06-13T15:49:18.917318: step 3266, loss 1.61215, acc 0.375, learning_rate 0.0009001\n","2023-06-13T15:49:20.412182: step 3267, loss 1.12448, acc 0.6875, learning_rate 0.000899712\n","2023-06-13T15:49:21.883658: step 3268, loss 1.17994, acc 0.71875, learning_rate 0.000899324\n","2023-06-13T15:49:23.345712: step 3269, loss 1.25324, acc 0.53125, learning_rate 0.000898937\n","2023-06-13T15:49:24.822550: step 3270, loss 1.10195, acc 0.65625, learning_rate 0.000898549\n","2023-06-13T15:49:26.287966: step 3271, loss 0.583918, acc 0.75, learning_rate 0.000898162\n","2023-06-13T15:49:28.039657: step 3272, loss 1.13695, acc 0.6875, learning_rate 0.000897775\n","2023-06-13T15:49:30.755850: step 3273, loss 1.28018, acc 0.6875, learning_rate 0.000897388\n","2023-06-13T15:49:33.211007: step 3274, loss 0.617772, acc 0.78125, learning_rate 0.000897002\n","2023-06-13T15:49:35.589531: step 3275, loss 0.905246, acc 0.5625, learning_rate 0.000896615\n","2023-06-13T15:49:37.874575: step 3276, loss 0.951717, acc 0.6875, learning_rate 0.000896229\n","2023-06-13T15:49:39.367529: step 3277, loss 1.00787, acc 0.65625, learning_rate 0.000895843\n","2023-06-13T15:49:40.838910: step 3278, loss 0.972287, acc 0.625, learning_rate 0.000895457\n","2023-06-13T15:49:42.333328: step 3279, loss 0.917608, acc 0.6875, learning_rate 0.000895071\n","2023-06-13T15:49:43.806071: step 3280, loss 0.944753, acc 0.65625, learning_rate 0.000894686\n","2023-06-13T15:49:45.312308: step 3281, loss 0.938145, acc 0.71875, learning_rate 0.0008943\n","2023-06-13T15:49:46.762522: step 3282, loss 1.28079, acc 0.5625, learning_rate 0.000893915\n","2023-06-13T15:49:48.472306: step 3283, loss 1.36702, acc 0.59375, learning_rate 0.00089353\n","2023-06-13T15:49:51.028121: step 3284, loss 0.948888, acc 0.6875, learning_rate 0.000893146\n","2023-06-13T15:49:53.486212: step 3285, loss 1.00787, acc 0.71875, learning_rate 0.000892761\n","2023-06-13T15:49:55.921840: step 3286, loss 0.736863, acc 0.71875, learning_rate 0.000892377\n","2023-06-13T15:49:58.268809: step 3287, loss 1.38292, acc 0.5625, learning_rate 0.000891992\n","2023-06-13T15:49:59.761962: step 3288, loss 1.01239, acc 0.6875, learning_rate 0.000891608\n","2023-06-13T15:50:01.226237: step 3289, loss 1.01363, acc 0.71875, learning_rate 0.000891225\n","2023-06-13T15:50:02.692373: step 3290, loss 0.996576, acc 0.65625, learning_rate 0.000890841\n","2023-06-13T15:50:04.183356: step 3291, loss 0.793088, acc 0.71875, learning_rate 0.000890457\n","2023-06-13T15:50:05.700919: step 3292, loss 0.64279, acc 0.8125, learning_rate 0.000890074\n","2023-06-13T15:50:08.315602: step 3293, loss 0.887566, acc 0.5625, learning_rate 0.000889691\n","2023-06-13T15:50:11.061317: step 3294, loss 0.847844, acc 0.71875, learning_rate 0.000889308\n","2023-06-13T15:50:13.846351: step 3295, loss 1.40328, acc 0.53125, learning_rate 0.000888925\n","2023-06-13T15:50:16.578347: step 3296, loss 0.981751, acc 0.78125, learning_rate 0.000888543\n","2023-06-13T15:50:19.247770: step 3297, loss 0.721327, acc 0.8125, learning_rate 0.000888161\n","2023-06-13T15:50:21.802774: step 3298, loss 1.05724, acc 0.5625, learning_rate 0.000887778\n","2023-06-13T15:50:23.575985: step 3299, loss 1.31953, acc 0.416667, learning_rate 0.000887396\n","\n","Evaluation:\n","2023-06-13T15:50:52.106365: step 3300, loss 2.06802, acc 0.387845\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3300\n","\n","2023-06-13T15:50:53.741425: step 3300, loss 0.683382, acc 0.78125, learning_rate 0.000887015\n","2023-06-13T15:50:55.201113: step 3301, loss 0.603089, acc 0.8125, learning_rate 0.000886633\n","2023-06-13T15:50:57.608545: step 3302, loss 0.834073, acc 0.75, learning_rate 0.000886252\n","2023-06-13T15:51:00.274387: step 3303, loss 1.16053, acc 0.65625, learning_rate 0.00088587\n","2023-06-13T15:51:02.780051: step 3304, loss 0.746908, acc 0.78125, learning_rate 0.000885489\n","2023-06-13T15:51:05.202313: step 3305, loss 0.682516, acc 0.8125, learning_rate 0.000885108\n","2023-06-13T15:51:06.860754: step 3306, loss 0.88229, acc 0.71875, learning_rate 0.000884728\n","2023-06-13T15:51:08.330070: step 3307, loss 0.971421, acc 0.59375, learning_rate 0.000884347\n","2023-06-13T15:51:09.824392: step 3308, loss 0.841151, acc 0.625, learning_rate 0.000883967\n","2023-06-13T15:51:11.307943: step 3309, loss 0.852526, acc 0.71875, learning_rate 0.000883587\n","2023-06-13T15:51:12.732465: step 3310, loss 1.11805, acc 0.59375, learning_rate 0.000883207\n","2023-06-13T15:51:14.165187: step 3311, loss 0.692158, acc 0.71875, learning_rate 0.000882827\n","2023-06-13T15:51:15.619704: step 3312, loss 0.775704, acc 0.875, learning_rate 0.000882448\n","2023-06-13T15:51:18.187365: step 3313, loss 0.810631, acc 0.75, learning_rate 0.000882068\n","2023-06-13T15:51:20.656904: step 3314, loss 0.776054, acc 0.6875, learning_rate 0.000881689\n","2023-06-13T15:51:23.146476: step 3315, loss 0.776871, acc 0.75, learning_rate 0.00088131\n","2023-06-13T15:51:25.511185: step 3316, loss 0.650172, acc 0.75, learning_rate 0.000880931\n","2023-06-13T15:51:27.224941: step 3317, loss 0.569583, acc 0.84375, learning_rate 0.000880552\n","2023-06-13T15:51:28.709510: step 3318, loss 0.88913, acc 0.71875, learning_rate 0.000880174\n","2023-06-13T15:51:30.252336: step 3319, loss 0.866529, acc 0.65625, learning_rate 0.000879796\n","2023-06-13T15:51:31.713924: step 3320, loss 0.680177, acc 0.71875, learning_rate 0.000879418\n","2023-06-13T15:51:33.161470: step 3321, loss 0.801867, acc 0.78125, learning_rate 0.00087904\n","2023-06-13T15:51:34.619191: step 3322, loss 0.727612, acc 0.71875, learning_rate 0.000878662\n","2023-06-13T15:51:36.080571: step 3323, loss 0.688065, acc 0.71875, learning_rate 0.000878284\n","2023-06-13T15:51:38.744014: step 3324, loss 0.860074, acc 0.6875, learning_rate 0.000877907\n","2023-06-13T15:51:41.214613: step 3325, loss 0.513128, acc 0.84375, learning_rate 0.00087753\n","2023-06-13T15:51:43.600785: step 3326, loss 0.724903, acc 0.78125, learning_rate 0.000877153\n","2023-06-13T15:51:46.046317: step 3327, loss 0.823122, acc 0.6875, learning_rate 0.000876776\n","2023-06-13T15:51:47.780480: step 3328, loss 0.921649, acc 0.71875, learning_rate 0.000876399\n","2023-06-13T15:51:49.257586: step 3329, loss 0.595507, acc 0.8125, learning_rate 0.000876023\n","2023-06-13T15:51:50.728314: step 3330, loss 0.819966, acc 0.75, learning_rate 0.000875647\n","2023-06-13T15:51:52.215443: step 3331, loss 0.874985, acc 0.65625, learning_rate 0.00087527\n","2023-06-13T15:51:53.670888: step 3332, loss 0.755028, acc 0.71875, learning_rate 0.000874895\n","2023-06-13T15:51:55.137546: step 3333, loss 0.510167, acc 0.90625, learning_rate 0.000874519\n","2023-06-13T15:51:56.605205: step 3334, loss 0.795582, acc 0.6875, learning_rate 0.000874143\n","2023-06-13T15:51:58.925798: step 3335, loss 0.985686, acc 0.53125, learning_rate 0.000873768\n","2023-06-13T15:52:01.583179: step 3336, loss 0.842875, acc 0.75, learning_rate 0.000873393\n","2023-06-13T15:52:04.038627: step 3337, loss 0.874465, acc 0.65625, learning_rate 0.000873018\n","2023-06-13T15:52:06.397078: step 3338, loss 0.69344, acc 0.8125, learning_rate 0.000872643\n","2023-06-13T15:52:08.339659: step 3339, loss 0.585058, acc 0.84375, learning_rate 0.000872268\n","2023-06-13T15:52:09.846943: step 3340, loss 0.549207, acc 0.78125, learning_rate 0.000871894\n","2023-06-13T15:52:11.336336: step 3341, loss 0.818231, acc 0.6875, learning_rate 0.00087152\n","2023-06-13T15:52:12.807684: step 3342, loss 0.560528, acc 0.8125, learning_rate 0.000871145\n","2023-06-13T15:52:14.292214: step 3343, loss 0.667258, acc 0.8125, learning_rate 0.000870772\n","2023-06-13T15:52:15.780084: step 3344, loss 0.920524, acc 0.6875, learning_rate 0.000870398\n","2023-06-13T15:52:17.239605: step 3345, loss 0.69413, acc 0.75, learning_rate 0.000870024\n","2023-06-13T15:52:19.497919: step 3346, loss 0.412773, acc 0.84375, learning_rate 0.000869651\n","2023-06-13T15:52:21.999351: step 3347, loss 0.706933, acc 0.8125, learning_rate 0.000869278\n","2023-06-13T15:52:24.347847: step 3348, loss 0.49457, acc 0.84375, learning_rate 0.000868905\n","2023-06-13T15:52:26.789291: step 3349, loss 0.470624, acc 0.78125, learning_rate 0.000868532\n","2023-06-13T15:52:28.813851: step 3350, loss 0.851931, acc 0.75, learning_rate 0.000868159\n","2023-06-13T15:52:30.283046: step 3351, loss 0.5958, acc 0.78125, learning_rate 0.000867787\n","2023-06-13T15:52:31.812881: step 3352, loss 0.777109, acc 0.71875, learning_rate 0.000867415\n","2023-06-13T15:52:33.293146: step 3353, loss 0.564455, acc 0.84375, learning_rate 0.000867042\n","2023-06-13T15:52:34.732413: step 3354, loss 0.519149, acc 0.8125, learning_rate 0.00086667\n","2023-06-13T15:52:36.186347: step 3355, loss 0.73242, acc 0.78125, learning_rate 0.000866299\n","2023-06-13T15:52:37.670793: step 3356, loss 1.27504, acc 0.65625, learning_rate 0.000865927\n","2023-06-13T15:52:39.784089: step 3357, loss 0.858311, acc 0.6875, learning_rate 0.000865556\n","2023-06-13T15:52:42.438015: step 3358, loss 0.700798, acc 0.8125, learning_rate 0.000865185\n","2023-06-13T15:52:44.776960: step 3359, loss 0.28974, acc 0.9375, learning_rate 0.000864814\n","2023-06-13T15:52:47.187556: step 3360, loss 0.567736, acc 0.75, learning_rate 0.000864443\n","2023-06-13T15:52:49.234706: step 3361, loss 0.646573, acc 0.75, learning_rate 0.000864072\n","2023-06-13T15:52:50.715381: step 3362, loss 0.824168, acc 0.8125, learning_rate 0.000863702\n","2023-06-13T15:52:52.184409: step 3363, loss 0.896983, acc 0.625, learning_rate 0.000863331\n","2023-06-13T15:52:53.601311: step 3364, loss 0.713142, acc 0.8125, learning_rate 0.000862961\n","2023-06-13T15:52:55.057712: step 3365, loss 0.914394, acc 0.6875, learning_rate 0.000862591\n","2023-06-13T15:52:56.501734: step 3366, loss 0.831378, acc 0.71875, learning_rate 0.000862221\n","2023-06-13T15:52:57.994844: step 3367, loss 1.05735, acc 0.71875, learning_rate 0.000861852\n","2023-06-13T15:52:59.956640: step 3368, loss 0.632132, acc 0.8125, learning_rate 0.000861482\n","2023-06-13T15:53:02.649151: step 3369, loss 1.01368, acc 0.59375, learning_rate 0.000861113\n","2023-06-13T15:53:05.111665: step 3370, loss 0.662954, acc 0.75, learning_rate 0.000860744\n","2023-06-13T15:53:07.504567: step 3371, loss 0.810991, acc 0.8125, learning_rate 0.000860375\n","2023-06-13T15:53:09.492096: step 3372, loss 0.987839, acc 0.71875, learning_rate 0.000860007\n","2023-06-13T15:53:10.979042: step 3373, loss 0.622173, acc 0.78125, learning_rate 0.000859638\n","2023-06-13T15:53:12.415622: step 3374, loss 0.806196, acc 0.78125, learning_rate 0.00085927\n","2023-06-13T15:53:13.887485: step 3375, loss 1.12961, acc 0.71875, learning_rate 0.000858902\n","2023-06-13T15:53:15.314139: step 3376, loss 0.87667, acc 0.71875, learning_rate 0.000858534\n","2023-06-13T15:53:16.772537: step 3377, loss 0.527336, acc 0.84375, learning_rate 0.000858166\n","2023-06-13T15:53:18.222302: step 3378, loss 0.945112, acc 0.6875, learning_rate 0.000857798\n","2023-06-13T15:53:20.345259: step 3379, loss 0.886536, acc 0.75, learning_rate 0.000857431\n","2023-06-13T15:53:22.981000: step 3380, loss 0.492732, acc 0.8125, learning_rate 0.000857064\n","2023-06-13T15:53:25.423395: step 3381, loss 0.713501, acc 0.75, learning_rate 0.000856696\n","2023-06-13T15:53:27.777630: step 3382, loss 0.609138, acc 0.8125, learning_rate 0.00085633\n","2023-06-13T15:53:29.817713: step 3383, loss 0.826621, acc 0.75, learning_rate 0.000855963\n","2023-06-13T15:53:31.292256: step 3384, loss 0.652485, acc 0.75, learning_rate 0.000855596\n","2023-06-13T15:53:32.767497: step 3385, loss 0.750149, acc 0.6875, learning_rate 0.00085523\n","2023-06-13T15:53:34.307653: step 3386, loss 0.864885, acc 0.75, learning_rate 0.000854864\n","2023-06-13T15:53:35.776929: step 3387, loss 0.487947, acc 0.84375, learning_rate 0.000854498\n","2023-06-13T15:53:37.259276: step 3388, loss 0.935076, acc 0.65625, learning_rate 0.000854132\n","2023-06-13T15:53:38.766174: step 3389, loss 0.707217, acc 0.78125, learning_rate 0.000853766\n","2023-06-13T15:53:40.966165: step 3390, loss 0.75273, acc 0.75, learning_rate 0.000853401\n","2023-06-13T15:53:43.542279: step 3391, loss 0.758665, acc 0.78125, learning_rate 0.000853035\n","2023-06-13T15:53:45.880317: step 3392, loss 0.73587, acc 0.875, learning_rate 0.00085267\n","2023-06-13T15:53:48.566353: step 3393, loss 0.790939, acc 0.8125, learning_rate 0.000852305\n","2023-06-13T15:53:51.360149: step 3394, loss 0.833978, acc 0.6875, learning_rate 0.000851941\n","2023-06-13T15:53:53.988248: step 3395, loss 0.598032, acc 0.78125, learning_rate 0.000851576\n","2023-06-13T15:53:56.443322: step 3396, loss 0.632051, acc 0.78125, learning_rate 0.000851212\n","2023-06-13T15:53:58.651865: step 3397, loss 0.658384, acc 0.8125, learning_rate 0.000850847\n","2023-06-13T15:54:00.629815: step 3398, loss 1.06628, acc 0.5625, learning_rate 0.000850483\n","2023-06-13T15:54:02.095136: step 3399, loss 0.904519, acc 0.6875, learning_rate 0.000850119\n","\n","Evaluation:\n","2023-06-13T15:54:34.957776: step 3400, loss 2.21809, acc 0.38936\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3400\n","\n","2023-06-13T15:54:36.923624: step 3400, loss 0.871511, acc 0.65625, learning_rate 0.000849756\n","2023-06-13T15:54:38.390655: step 3401, loss 1.16936, acc 0.625, learning_rate 0.000849392\n","2023-06-13T15:54:39.925369: step 3402, loss 0.803582, acc 0.625, learning_rate 0.000849029\n","2023-06-13T15:54:41.369886: step 3403, loss 0.628106, acc 0.75, learning_rate 0.000848666\n","2023-06-13T15:54:42.815873: step 3404, loss 0.597998, acc 0.8125, learning_rate 0.000848303\n","2023-06-13T15:54:44.376872: step 3405, loss 0.785438, acc 0.6875, learning_rate 0.00084794\n","2023-06-13T15:54:45.864139: step 3406, loss 0.759858, acc 0.6875, learning_rate 0.000847577\n","2023-06-13T15:54:48.510610: step 3407, loss 0.698159, acc 0.78125, learning_rate 0.000847215\n","2023-06-13T15:54:51.051166: step 3408, loss 0.952532, acc 0.6875, learning_rate 0.000846852\n","2023-06-13T15:54:53.491106: step 3409, loss 0.683678, acc 0.71875, learning_rate 0.00084649\n","2023-06-13T15:54:55.964757: step 3410, loss 0.991589, acc 0.75, learning_rate 0.000846128\n","2023-06-13T15:54:57.460469: step 3411, loss 0.605303, acc 0.8125, learning_rate 0.000845766\n","2023-06-13T15:54:58.927262: step 3412, loss 0.657873, acc 0.8125, learning_rate 0.000845405\n","2023-06-13T15:55:00.386929: step 3413, loss 0.931178, acc 0.6875, learning_rate 0.000845043\n","2023-06-13T15:55:01.839595: step 3414, loss 0.879682, acc 0.71875, learning_rate 0.000844682\n","2023-06-13T15:55:03.266667: step 3415, loss 0.685113, acc 0.6875, learning_rate 0.000844321\n","2023-06-13T15:55:04.713746: step 3416, loss 0.551, acc 0.8125, learning_rate 0.00084396\n","2023-06-13T15:55:06.244733: step 3417, loss 1.03467, acc 0.65625, learning_rate 0.000843599\n","2023-06-13T15:55:08.827619: step 3418, loss 0.589982, acc 0.84375, learning_rate 0.000843239\n","2023-06-13T15:55:11.223668: step 3419, loss 0.821189, acc 0.78125, learning_rate 0.000842878\n","2023-06-13T15:55:13.557739: step 3420, loss 0.680859, acc 0.78125, learning_rate 0.000842518\n","2023-06-13T15:55:15.996962: step 3421, loss 0.497474, acc 0.90625, learning_rate 0.000842158\n","2023-06-13T15:55:17.560852: step 3422, loss 0.719897, acc 0.71875, learning_rate 0.000841798\n","2023-06-13T15:55:19.046693: step 3423, loss 0.602567, acc 0.78125, learning_rate 0.000841439\n","2023-06-13T15:55:20.534180: step 3424, loss 0.704402, acc 0.75, learning_rate 0.000841079\n","2023-06-13T15:55:22.015136: step 3425, loss 0.898353, acc 0.78125, learning_rate 0.00084072\n","2023-06-13T15:55:23.483474: step 3426, loss 0.905613, acc 0.71875, learning_rate 0.000840361\n","2023-06-13T15:55:24.954175: step 3427, loss 1.11048, acc 0.625, learning_rate 0.000840002\n","2023-06-13T15:55:26.427512: step 3428, loss 0.581742, acc 0.8125, learning_rate 0.000839643\n","2023-06-13T15:55:28.959657: step 3429, loss 0.876254, acc 0.625, learning_rate 0.000839284\n","2023-06-13T15:55:31.413719: step 3430, loss 0.750233, acc 0.6875, learning_rate 0.000838926\n","2023-06-13T15:55:33.778726: step 3431, loss 0.971773, acc 0.6875, learning_rate 0.000838567\n","2023-06-13T15:55:36.218843: step 3432, loss 0.742638, acc 0.71875, learning_rate 0.000838209\n","2023-06-13T15:55:37.984313: step 3433, loss 1.08172, acc 0.59375, learning_rate 0.000837851\n","2023-06-13T15:55:39.444905: step 3434, loss 1.04673, acc 0.6875, learning_rate 0.000837494\n","2023-06-13T15:55:40.957880: step 3435, loss 0.648378, acc 0.75, learning_rate 0.000837136\n","2023-06-13T15:55:42.412088: step 3436, loss 0.893734, acc 0.6875, learning_rate 0.000836779\n","2023-06-13T15:55:43.876574: step 3437, loss 0.81363, acc 0.65625, learning_rate 0.000836421\n","2023-06-13T15:55:45.353483: step 3438, loss 0.485277, acc 0.8125, learning_rate 0.000836064\n","2023-06-13T15:55:46.808969: step 3439, loss 0.613552, acc 0.8125, learning_rate 0.000835707\n","2023-06-13T15:55:49.191401: step 3440, loss 0.669357, acc 0.6875, learning_rate 0.000835351\n","2023-06-13T15:55:51.674541: step 3441, loss 0.680932, acc 0.84375, learning_rate 0.000834994\n","2023-06-13T15:55:54.001131: step 3442, loss 1.01109, acc 0.71875, learning_rate 0.000834638\n","2023-06-13T15:55:56.377509: step 3443, loss 0.824773, acc 0.71875, learning_rate 0.000834282\n","2023-06-13T15:55:58.359650: step 3444, loss 1.39717, acc 0.46875, learning_rate 0.000833925\n","2023-06-13T15:55:59.809285: step 3445, loss 0.889362, acc 0.6875, learning_rate 0.00083357\n","2023-06-13T15:56:01.290582: step 3446, loss 0.833355, acc 0.71875, learning_rate 0.000833214\n","2023-06-13T15:56:02.760614: step 3447, loss 0.697424, acc 0.78125, learning_rate 0.000832858\n","2023-06-13T15:56:04.226673: step 3448, loss 0.618446, acc 0.75, learning_rate 0.000832503\n","2023-06-13T15:56:05.636339: step 3449, loss 0.674421, acc 0.8125, learning_rate 0.000832148\n","2023-06-13T15:56:07.127874: step 3450, loss 0.666262, acc 0.78125, learning_rate 0.000831793\n","2023-06-13T15:56:09.139111: step 3451, loss 0.82546, acc 0.71875, learning_rate 0.000831438\n","2023-06-13T15:56:11.869522: step 3452, loss 0.896779, acc 0.75, learning_rate 0.000831083\n","2023-06-13T15:56:14.200456: step 3453, loss 0.974193, acc 0.6875, learning_rate 0.000830729\n","2023-06-13T15:56:16.660013: step 3454, loss 0.785565, acc 0.75, learning_rate 0.000830375\n","2023-06-13T15:56:18.643818: step 3455, loss 0.586485, acc 0.78125, learning_rate 0.00083002\n","2023-06-13T15:56:20.076384: step 3456, loss 0.873789, acc 0.71875, learning_rate 0.000829666\n","2023-06-13T15:56:21.523155: step 3457, loss 0.855113, acc 0.65625, learning_rate 0.000829313\n","2023-06-13T15:56:22.990853: step 3458, loss 1.16124, acc 0.625, learning_rate 0.000828959\n","2023-06-13T15:56:24.432029: step 3459, loss 0.871337, acc 0.78125, learning_rate 0.000828606\n","2023-06-13T15:56:25.911024: step 3460, loss 0.810418, acc 0.71875, learning_rate 0.000828252\n","2023-06-13T15:56:27.387701: step 3461, loss 0.952287, acc 0.71875, learning_rate 0.000827899\n","2023-06-13T15:56:29.390474: step 3462, loss 0.851364, acc 0.625, learning_rate 0.000827546\n","2023-06-13T15:56:31.911244: step 3463, loss 0.597141, acc 0.78125, learning_rate 0.000827193\n","2023-06-13T15:56:34.273827: step 3464, loss 0.575003, acc 0.875, learning_rate 0.000826841\n","2023-06-13T15:56:36.598947: step 3465, loss 0.528061, acc 0.84375, learning_rate 0.000826488\n","2023-06-13T15:56:38.997116: step 3466, loss 0.640957, acc 0.84375, learning_rate 0.000826136\n","2023-06-13T15:56:40.493975: step 3467, loss 0.93032, acc 0.65625, learning_rate 0.000825784\n","2023-06-13T15:56:41.987209: step 3468, loss 0.821398, acc 0.75, learning_rate 0.000825432\n","2023-06-13T15:56:43.447996: step 3469, loss 0.912807, acc 0.75, learning_rate 0.00082508\n","2023-06-13T15:56:44.878911: step 3470, loss 0.748759, acc 0.75, learning_rate 0.000824729\n","2023-06-13T15:56:46.362682: step 3471, loss 0.747059, acc 0.78125, learning_rate 0.000824377\n","2023-06-13T15:56:47.799672: step 3472, loss 0.59694, acc 0.8125, learning_rate 0.000824026\n","2023-06-13T15:56:49.598078: step 3473, loss 0.542751, acc 0.875, learning_rate 0.000823675\n","2023-06-13T15:56:52.224183: step 3474, loss 0.671959, acc 0.75, learning_rate 0.000823324\n","2023-06-13T15:56:54.544427: step 3475, loss 0.879498, acc 0.6875, learning_rate 0.000822974\n","2023-06-13T15:56:56.930057: step 3476, loss 0.680775, acc 0.71875, learning_rate 0.000822623\n","2023-06-13T15:56:59.317181: step 3477, loss 0.532244, acc 0.78125, learning_rate 0.000822273\n","2023-06-13T15:57:00.777412: step 3478, loss 0.950612, acc 0.6875, learning_rate 0.000821922\n","2023-06-13T15:57:02.206313: step 3479, loss 0.395216, acc 0.875, learning_rate 0.000821572\n","2023-06-13T15:57:03.638684: step 3480, loss 0.911671, acc 0.65625, learning_rate 0.000821222\n","2023-06-13T15:57:05.090981: step 3481, loss 0.684213, acc 0.78125, learning_rate 0.000820873\n","2023-06-13T15:57:06.525760: step 3482, loss 0.58657, acc 0.8125, learning_rate 0.000820523\n","2023-06-13T15:57:07.988221: step 3483, loss 0.851054, acc 0.6875, learning_rate 0.000820174\n","2023-06-13T15:57:09.507332: step 3484, loss 0.883819, acc 0.6875, learning_rate 0.000819825\n","2023-06-13T15:57:12.141837: step 3485, loss 0.730268, acc 0.75, learning_rate 0.000819476\n","2023-06-13T15:57:14.650714: step 3486, loss 0.842168, acc 0.71875, learning_rate 0.000819127\n","2023-06-13T15:57:16.970167: step 3487, loss 0.931398, acc 0.65625, learning_rate 0.000818778\n","2023-06-13T15:57:19.372470: step 3488, loss 0.935417, acc 0.75, learning_rate 0.00081843\n","2023-06-13T15:57:20.792893: step 3489, loss 0.766378, acc 0.71875, learning_rate 0.000818081\n","2023-06-13T15:57:22.273223: step 3490, loss 0.598122, acc 0.84375, learning_rate 0.000817733\n","2023-06-13T15:57:23.728340: step 3491, loss 0.530495, acc 0.8125, learning_rate 0.000817385\n","2023-06-13T15:57:25.205088: step 3492, loss 0.509633, acc 0.84375, learning_rate 0.000817037\n","2023-06-13T15:57:26.660053: step 3493, loss 0.908888, acc 0.6875, learning_rate 0.00081669\n","2023-06-13T15:57:28.146854: step 3494, loss 0.862176, acc 0.78125, learning_rate 0.000816342\n","2023-06-13T15:57:29.799795: step 3495, loss 0.578358, acc 0.75, learning_rate 0.000815995\n","2023-06-13T15:57:32.505998: step 3496, loss 1.19873, acc 0.75, learning_rate 0.000815648\n","2023-06-13T15:57:35.103746: step 3497, loss 0.798993, acc 0.6875, learning_rate 0.0008153\n","2023-06-13T15:57:37.544038: step 3498, loss 0.692105, acc 0.78125, learning_rate 0.000814954\n","2023-06-13T15:57:39.785162: step 3499, loss 0.611961, acc 0.78125, learning_rate 0.000814607\n","\n","Evaluation:\n","2023-06-13T15:58:07.725619: step 3500, loss 2.25477, acc 0.388148\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3500\n","\n","2023-06-13T15:58:09.407968: step 3500, loss 0.662441, acc 0.84375, learning_rate 0.000814261\n","2023-06-13T15:58:10.950355: step 3501, loss 0.713801, acc 0.78125, learning_rate 0.000813914\n","2023-06-13T15:58:13.594507: step 3502, loss 0.692739, acc 0.75, learning_rate 0.000813568\n","2023-06-13T15:58:15.939157: step 3503, loss 1.11072, acc 0.65625, learning_rate 0.000813222\n","2023-06-13T15:58:18.271138: step 3504, loss 0.583002, acc 0.78125, learning_rate 0.000812876\n","2023-06-13T15:58:20.820041: step 3505, loss 0.6678, acc 0.75, learning_rate 0.000812531\n","2023-06-13T15:58:22.453809: step 3506, loss 0.586565, acc 0.78125, learning_rate 0.000812185\n","2023-06-13T15:58:23.932927: step 3507, loss 0.822864, acc 0.8125, learning_rate 0.00081184\n","2023-06-13T15:58:25.402144: step 3508, loss 0.856427, acc 0.71875, learning_rate 0.000811495\n","2023-06-13T15:58:26.893210: step 3509, loss 0.922688, acc 0.75, learning_rate 0.00081115\n","2023-06-13T15:58:28.357143: step 3510, loss 0.842086, acc 0.65625, learning_rate 0.000810805\n","2023-06-13T15:58:29.820820: step 3511, loss 1.03071, acc 0.65625, learning_rate 0.00081046\n","2023-06-13T15:58:31.266550: step 3512, loss 0.620126, acc 0.8125, learning_rate 0.000810116\n","2023-06-13T15:58:33.764580: step 3513, loss 0.657472, acc 0.71875, learning_rate 0.000809771\n","2023-06-13T15:58:36.350119: step 3514, loss 1.30129, acc 0.65625, learning_rate 0.000809427\n","2023-06-13T15:58:38.706570: step 3515, loss 1.16893, acc 0.59375, learning_rate 0.000809083\n","2023-06-13T15:58:41.068641: step 3516, loss 0.842299, acc 0.6875, learning_rate 0.000808739\n","2023-06-13T15:58:42.877783: step 3517, loss 0.988967, acc 0.71875, learning_rate 0.000808396\n","2023-06-13T15:58:44.311969: step 3518, loss 0.721143, acc 0.78125, learning_rate 0.000808052\n","2023-06-13T15:58:45.749767: step 3519, loss 0.965837, acc 0.6875, learning_rate 0.000807709\n","2023-06-13T15:58:47.198352: step 3520, loss 1.06227, acc 0.65625, learning_rate 0.000807366\n","2023-06-13T15:58:48.636185: step 3521, loss 0.644473, acc 0.75, learning_rate 0.000807023\n","2023-06-13T15:58:50.087431: step 3522, loss 1.0796, acc 0.71875, learning_rate 0.00080668\n","2023-06-13T15:58:51.519415: step 3523, loss 0.60077, acc 0.75, learning_rate 0.000806337\n","2023-06-13T15:58:53.669327: step 3524, loss 0.900972, acc 0.75, learning_rate 0.000805995\n","2023-06-13T15:58:56.219569: step 3525, loss 0.517695, acc 0.84375, learning_rate 0.000805652\n","2023-06-13T15:58:58.606473: step 3526, loss 0.821841, acc 0.71875, learning_rate 0.00080531\n","2023-06-13T15:59:01.130824: step 3527, loss 1.1105, acc 0.6875, learning_rate 0.000804968\n","2023-06-13T15:59:03.041578: step 3528, loss 0.644337, acc 0.8125, learning_rate 0.000804626\n","2023-06-13T15:59:04.486167: step 3529, loss 0.593941, acc 0.78125, learning_rate 0.000804285\n","2023-06-13T15:59:05.936489: step 3530, loss 0.61775, acc 0.8125, learning_rate 0.000803943\n","2023-06-13T15:59:07.386273: step 3531, loss 0.902072, acc 0.71875, learning_rate 0.000803602\n","2023-06-13T15:59:08.865851: step 3532, loss 0.753564, acc 0.78125, learning_rate 0.000803261\n","2023-06-13T15:59:10.356403: step 3533, loss 1.0072, acc 0.65625, learning_rate 0.00080292\n","2023-06-13T15:59:11.856670: step 3534, loss 0.404811, acc 0.875, learning_rate 0.000802579\n","2023-06-13T15:59:14.042813: step 3535, loss 0.680941, acc 0.75, learning_rate 0.000802238\n","2023-06-13T15:59:16.597443: step 3536, loss 1.04607, acc 0.65625, learning_rate 0.000801898\n","2023-06-13T15:59:19.414673: step 3537, loss 1.09536, acc 0.5625, learning_rate 0.000801558\n","2023-06-13T15:59:22.252916: step 3538, loss 0.811365, acc 0.71875, learning_rate 0.000801217\n","2023-06-13T15:59:24.817260: step 3539, loss 0.875415, acc 0.75, learning_rate 0.000800877\n","2023-06-13T15:59:27.429783: step 3540, loss 0.967367, acc 0.71875, learning_rate 0.000800538\n","2023-06-13T15:59:29.839500: step 3541, loss 0.605381, acc 0.84375, learning_rate 0.000800198\n","2023-06-13T15:59:31.639540: step 3542, loss 0.623037, acc 0.6875, learning_rate 0.000799858\n","2023-06-13T15:59:33.105400: step 3543, loss 0.568085, acc 0.75, learning_rate 0.000799519\n","2023-06-13T15:59:34.564728: step 3544, loss 0.388003, acc 0.8125, learning_rate 0.00079918\n","2023-06-13T15:59:36.020054: step 3545, loss 0.957857, acc 0.71875, learning_rate 0.000798841\n","2023-06-13T15:59:38.221962: step 3546, loss 0.852204, acc 0.8125, learning_rate 0.000798502\n","2023-06-13T15:59:40.927193: step 3547, loss 0.663701, acc 0.875, learning_rate 0.000798163\n","2023-06-13T15:59:43.478264: step 3548, loss 0.828683, acc 0.6875, learning_rate 0.000797825\n","2023-06-13T15:59:45.990365: step 3549, loss 1.17372, acc 0.59375, learning_rate 0.000797486\n","2023-06-13T15:59:47.725359: step 3550, loss 0.779068, acc 0.75, learning_rate 0.000797148\n","2023-06-13T15:59:49.188502: step 3551, loss 0.773954, acc 0.625, learning_rate 0.00079681\n","2023-06-13T15:59:50.644524: step 3552, loss 0.537996, acc 0.78125, learning_rate 0.000796472\n","2023-06-13T15:59:52.116162: step 3553, loss 0.750041, acc 0.71875, learning_rate 0.000796135\n","2023-06-13T15:59:53.570274: step 3554, loss 0.451529, acc 0.875, learning_rate 0.000795797\n","2023-06-13T15:59:55.038384: step 3555, loss 0.688932, acc 0.8125, learning_rate 0.00079546\n","2023-06-13T15:59:56.499509: step 3556, loss 0.60936, acc 0.78125, learning_rate 0.000795122\n","2023-06-13T15:59:58.833241: step 3557, loss 1.00544, acc 0.6875, learning_rate 0.000794785\n","2023-06-13T16:00:01.429154: step 3558, loss 0.943429, acc 0.71875, learning_rate 0.000794449\n","2023-06-13T16:00:03.774747: step 3559, loss 0.804258, acc 0.71875, learning_rate 0.000794112\n","2023-06-13T16:00:06.191004: step 3560, loss 0.488586, acc 0.84375, learning_rate 0.000793775\n","2023-06-13T16:00:07.981708: step 3561, loss 0.715207, acc 0.71875, learning_rate 0.000793439\n","2023-06-13T16:00:09.438851: step 3562, loss 0.79007, acc 0.71875, learning_rate 0.000793103\n","2023-06-13T16:00:10.914597: step 3563, loss 0.551064, acc 0.75, learning_rate 0.000792767\n","2023-06-13T16:00:12.446844: step 3564, loss 0.77371, acc 0.65625, learning_rate 0.000792431\n","2023-06-13T16:00:13.931128: step 3565, loss 0.926834, acc 0.6875, learning_rate 0.000792095\n","2023-06-13T16:00:15.399248: step 3566, loss 0.800177, acc 0.6875, learning_rate 0.000791759\n","2023-06-13T16:00:16.849915: step 3567, loss 1.02332, acc 0.65625, learning_rate 0.000791424\n","2023-06-13T16:00:19.186439: step 3568, loss 0.639508, acc 0.6875, learning_rate 0.000791089\n","2023-06-13T16:00:21.820114: step 3569, loss 0.600091, acc 0.75, learning_rate 0.000790754\n","2023-06-13T16:00:24.254478: step 3570, loss 0.917021, acc 0.625, learning_rate 0.000790419\n","2023-06-13T16:00:26.749832: step 3571, loss 0.613665, acc 0.71875, learning_rate 0.000790084\n","2023-06-13T16:00:28.280315: step 3572, loss 0.634924, acc 0.84375, learning_rate 0.000789749\n","2023-06-13T16:00:29.715493: step 3573, loss 1.19507, acc 0.75, learning_rate 0.000789415\n","2023-06-13T16:00:31.157442: step 3574, loss 0.622124, acc 0.78125, learning_rate 0.00078908\n","2023-06-13T16:00:32.620384: step 3575, loss 0.952052, acc 0.8125, learning_rate 0.000788746\n","2023-06-13T16:00:34.086012: step 3576, loss 0.888014, acc 0.71875, learning_rate 0.000788412\n","2023-06-13T16:00:35.559835: step 3577, loss 0.855685, acc 0.625, learning_rate 0.000788079\n","2023-06-13T16:00:37.056506: step 3578, loss 0.854827, acc 0.71875, learning_rate 0.000787745\n","2023-06-13T16:00:39.700451: step 3579, loss 0.622084, acc 0.78125, learning_rate 0.000787412\n","2023-06-13T16:00:42.197032: step 3580, loss 0.979768, acc 0.6875, learning_rate 0.000787078\n","2023-06-13T16:00:44.507606: step 3581, loss 0.646095, acc 0.75, learning_rate 0.000786745\n","2023-06-13T16:00:46.895054: step 3582, loss 0.838245, acc 0.6875, learning_rate 0.000786412\n","2023-06-13T16:00:48.536136: step 3583, loss 0.900742, acc 0.71875, learning_rate 0.000786079\n","2023-06-13T16:00:50.015115: step 3584, loss 0.65823, acc 0.78125, learning_rate 0.000785747\n","2023-06-13T16:00:51.467492: step 3585, loss 0.587144, acc 0.84375, learning_rate 0.000785414\n","2023-06-13T16:00:52.930339: step 3586, loss 0.706849, acc 0.75, learning_rate 0.000785082\n","2023-06-13T16:00:54.410571: step 3587, loss 0.660101, acc 0.71875, learning_rate 0.00078475\n","2023-06-13T16:00:55.904812: step 3588, loss 1.24615, acc 0.6875, learning_rate 0.000784418\n","2023-06-13T16:00:57.363768: step 3589, loss 0.678928, acc 0.71875, learning_rate 0.000784086\n","2023-06-13T16:00:59.897848: step 3590, loss 1.04386, acc 0.65625, learning_rate 0.000783754\n","2023-06-13T16:01:02.362684: step 3591, loss 0.483858, acc 0.84375, learning_rate 0.000783422\n","2023-06-13T16:01:04.631518: step 3592, loss 0.534254, acc 0.84375, learning_rate 0.000783091\n","2023-06-13T16:01:07.062689: step 3593, loss 0.816728, acc 0.71875, learning_rate 0.00078276\n","2023-06-13T16:01:08.832457: step 3594, loss 0.326637, acc 0.90625, learning_rate 0.000782429\n","2023-06-13T16:01:10.271218: step 3595, loss 0.513326, acc 0.875, learning_rate 0.000782098\n","2023-06-13T16:01:11.767372: step 3596, loss 1.19176, acc 0.53125, learning_rate 0.000781767\n","2023-06-13T16:01:13.215855: step 3597, loss 0.724712, acc 0.71875, learning_rate 0.000781437\n","2023-06-13T16:01:14.650874: step 3598, loss 0.627014, acc 0.8125, learning_rate 0.000781106\n","2023-06-13T16:01:16.133084: step 3599, loss 0.859729, acc 0.6875, learning_rate 0.000780776\n","\n","Evaluation:\n","2023-06-13T16:01:48.766577: step 3600, loss 2.25659, acc 0.39027\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3600\n","\n","2023-06-13T16:01:50.434089: step 3600, loss 0.633985, acc 0.8125, learning_rate 0.000780446\n","2023-06-13T16:01:51.910427: step 3601, loss 0.856938, acc 0.75, learning_rate 0.000780116\n","2023-06-13T16:01:53.352262: step 3602, loss 0.649543, acc 0.75, learning_rate 0.000779786\n","2023-06-13T16:01:54.820735: step 3603, loss 0.60834, acc 0.875, learning_rate 0.000779457\n","2023-06-13T16:01:56.285384: step 3604, loss 0.664552, acc 0.8125, learning_rate 0.000779127\n","2023-06-13T16:01:57.744938: step 3605, loss 1.08986, acc 0.59375, learning_rate 0.000778798\n","2023-06-13T16:01:59.525660: step 3606, loss 0.70644, acc 0.8125, learning_rate 0.000778469\n","2023-06-13T16:02:02.105741: step 3607, loss 0.747427, acc 0.75, learning_rate 0.00077814\n","2023-06-13T16:02:04.543531: step 3608, loss 0.745365, acc 0.71875, learning_rate 0.000777811\n","2023-06-13T16:02:06.972629: step 3609, loss 0.953893, acc 0.6875, learning_rate 0.000777482\n","2023-06-13T16:02:09.360945: step 3610, loss 0.996454, acc 0.625, learning_rate 0.000777154\n","2023-06-13T16:02:10.794449: step 3611, loss 0.98769, acc 0.625, learning_rate 0.000776825\n","2023-06-13T16:02:12.282618: step 3612, loss 0.918958, acc 0.78125, learning_rate 0.000776497\n","2023-06-13T16:02:13.734648: step 3613, loss 0.633234, acc 0.84375, learning_rate 0.000776169\n","2023-06-13T16:02:15.215678: step 3614, loss 0.827357, acc 0.71875, learning_rate 0.000775841\n","2023-06-13T16:02:16.718868: step 3615, loss 0.589463, acc 0.78125, learning_rate 0.000775514\n","2023-06-13T16:02:18.173353: step 3616, loss 0.724542, acc 0.78125, learning_rate 0.000775186\n","2023-06-13T16:02:20.002651: step 3617, loss 0.591787, acc 0.78125, learning_rate 0.000774859\n","2023-06-13T16:02:22.597481: step 3618, loss 0.925598, acc 0.65625, learning_rate 0.000774531\n","2023-06-13T16:02:24.952767: step 3619, loss 0.570703, acc 0.8125, learning_rate 0.000774204\n","2023-06-13T16:02:27.407806: step 3620, loss 0.679784, acc 0.78125, learning_rate 0.000773877\n","2023-06-13T16:02:29.682383: step 3621, loss 0.718697, acc 0.75, learning_rate 0.000773551\n","2023-06-13T16:02:31.173739: step 3622, loss 1.04177, acc 0.625, learning_rate 0.000773224\n","2023-06-13T16:02:32.643464: step 3623, loss 0.988195, acc 0.6875, learning_rate 0.000772898\n","2023-06-13T16:02:34.153072: step 3624, loss 0.688827, acc 0.78125, learning_rate 0.000772571\n","2023-06-13T16:02:35.630090: step 3625, loss 0.641321, acc 0.8125, learning_rate 0.000772245\n","2023-06-13T16:02:37.105168: step 3626, loss 0.694204, acc 0.75, learning_rate 0.000771919\n","2023-06-13T16:02:38.557758: step 3627, loss 0.28674, acc 0.90625, learning_rate 0.000771594\n","2023-06-13T16:02:40.426018: step 3628, loss 1.02096, acc 0.6875, learning_rate 0.000771268\n","2023-06-13T16:02:43.064520: step 3629, loss 1.2511, acc 0.59375, learning_rate 0.000770942\n","2023-06-13T16:02:45.568383: step 3630, loss 0.765338, acc 0.78125, learning_rate 0.000770617\n","2023-06-13T16:02:48.022121: step 3631, loss 1.03345, acc 0.65625, learning_rate 0.000770292\n","2023-06-13T16:02:50.179618: step 3632, loss 0.866387, acc 0.65625, learning_rate 0.000769967\n","2023-06-13T16:02:51.653507: step 3633, loss 0.595171, acc 0.8125, learning_rate 0.000769642\n","2023-06-13T16:02:53.123773: step 3634, loss 0.654768, acc 0.75, learning_rate 0.000769317\n","2023-06-13T16:02:54.571122: step 3635, loss 0.552698, acc 0.84375, learning_rate 0.000768993\n","2023-06-13T16:02:56.039113: step 3636, loss 0.865499, acc 0.6875, learning_rate 0.000768668\n","2023-06-13T16:02:57.501135: step 3637, loss 0.834285, acc 0.6875, learning_rate 0.000768344\n","2023-06-13T16:02:58.958454: step 3638, loss 0.828343, acc 0.8125, learning_rate 0.00076802\n","2023-06-13T16:03:00.776627: step 3639, loss 0.721839, acc 0.78125, learning_rate 0.000767696\n","2023-06-13T16:03:03.413311: step 3640, loss 0.729647, acc 0.78125, learning_rate 0.000767373\n","2023-06-13T16:03:05.778949: step 3641, loss 0.559401, acc 0.84375, learning_rate 0.000767049\n","2023-06-13T16:03:08.211131: step 3642, loss 0.660007, acc 0.84375, learning_rate 0.000766725\n","2023-06-13T16:03:10.569183: step 3643, loss 0.620583, acc 0.84375, learning_rate 0.000766402\n","2023-06-13T16:03:12.032157: step 3644, loss 0.738415, acc 0.75, learning_rate 0.000766079\n","2023-06-13T16:03:13.498731: step 3645, loss 0.937487, acc 0.625, learning_rate 0.000765756\n","2023-06-13T16:03:14.961202: step 3646, loss 0.645696, acc 0.75, learning_rate 0.000765433\n","2023-06-13T16:03:16.429293: step 3647, loss 1.05552, acc 0.625, learning_rate 0.000765111\n","2023-06-13T16:03:17.865655: step 3648, loss 0.749101, acc 0.75, learning_rate 0.000764788\n","2023-06-13T16:03:19.323966: step 3649, loss 0.568911, acc 0.8125, learning_rate 0.000764466\n","2023-06-13T16:03:20.996325: step 3650, loss 1.02553, acc 0.71875, learning_rate 0.000764144\n","2023-06-13T16:03:23.658824: step 3651, loss 1.04815, acc 0.71875, learning_rate 0.000763822\n","2023-06-13T16:03:26.114355: step 3652, loss 0.568334, acc 0.84375, learning_rate 0.0007635\n","2023-06-13T16:03:28.561978: step 3653, loss 1.01811, acc 0.6875, learning_rate 0.000763178\n","2023-06-13T16:03:30.866011: step 3654, loss 0.929078, acc 0.6875, learning_rate 0.000762856\n","2023-06-13T16:03:32.336565: step 3655, loss 1.01151, acc 0.71875, learning_rate 0.000762535\n","2023-06-13T16:03:33.795656: step 3656, loss 0.47009, acc 0.90625, learning_rate 0.000762214\n","2023-06-13T16:03:35.239037: step 3657, loss 0.546147, acc 0.84375, learning_rate 0.000761893\n","2023-06-13T16:03:36.739729: step 3658, loss 0.62048, acc 0.71875, learning_rate 0.000761572\n","2023-06-13T16:03:38.221183: step 3659, loss 0.771017, acc 0.78125, learning_rate 0.000761251\n","2023-06-13T16:03:39.665345: step 3660, loss 0.677553, acc 0.75, learning_rate 0.00076093\n","2023-06-13T16:03:41.316354: step 3661, loss 0.910424, acc 0.6875, learning_rate 0.00076061\n","2023-06-13T16:03:43.909175: step 3662, loss 0.446574, acc 0.8125, learning_rate 0.00076029\n","2023-06-13T16:03:46.290250: step 3663, loss 0.676592, acc 0.75, learning_rate 0.000759969\n","2023-06-13T16:03:48.630764: step 3664, loss 1.40231, acc 0.65625, learning_rate 0.000759649\n","2023-06-13T16:03:50.974083: step 3665, loss 0.774078, acc 0.75, learning_rate 0.00075933\n","2023-06-13T16:03:52.655013: step 3666, loss 0.812062, acc 0.71875, learning_rate 0.00075901\n","2023-06-13T16:03:54.103418: step 3667, loss 0.724746, acc 0.8125, learning_rate 0.00075869\n","2023-06-13T16:03:55.562341: step 3668, loss 1.10257, acc 0.65625, learning_rate 0.000758371\n","2023-06-13T16:03:57.054587: step 3669, loss 0.586663, acc 0.8125, learning_rate 0.000758052\n","2023-06-13T16:03:58.519701: step 3670, loss 0.705999, acc 0.78125, learning_rate 0.000757733\n","2023-06-13T16:03:59.965829: step 3671, loss 0.62657, acc 0.875, learning_rate 0.000757414\n","2023-06-13T16:04:01.392856: step 3672, loss 0.882413, acc 0.625, learning_rate 0.000757095\n","2023-06-13T16:04:03.883649: step 3673, loss 0.797854, acc 0.71875, learning_rate 0.000756776\n","2023-06-13T16:04:06.437145: step 3674, loss 0.877654, acc 0.6875, learning_rate 0.000756458\n","2023-06-13T16:04:08.890705: step 3675, loss 0.743359, acc 0.71875, learning_rate 0.00075614\n","2023-06-13T16:04:11.310944: step 3676, loss 0.731611, acc 0.75, learning_rate 0.000755821\n","2023-06-13T16:04:12.905332: step 3677, loss 1.0339, acc 0.6875, learning_rate 0.000755503\n","2023-06-13T16:04:14.410913: step 3678, loss 0.767449, acc 0.75, learning_rate 0.000755186\n","2023-06-13T16:04:15.949134: step 3679, loss 0.73311, acc 0.78125, learning_rate 0.000754868\n","2023-06-13T16:04:17.442745: step 3680, loss 0.716458, acc 0.75, learning_rate 0.00075455\n","2023-06-13T16:04:18.942005: step 3681, loss 0.556977, acc 0.875, learning_rate 0.000754233\n","2023-06-13T16:04:20.420567: step 3682, loss 0.885033, acc 0.71875, learning_rate 0.000753916\n","2023-06-13T16:04:22.091208: step 3683, loss 1.00719, acc 0.59375, learning_rate 0.000753599\n","2023-06-13T16:04:24.699124: step 3684, loss 0.629681, acc 0.78125, learning_rate 0.000753282\n","2023-06-13T16:04:27.246907: step 3685, loss 0.707501, acc 0.8125, learning_rate 0.000752965\n","2023-06-13T16:04:29.656159: step 3686, loss 1.10004, acc 0.59375, learning_rate 0.000752648\n","2023-06-13T16:04:32.051556: step 3687, loss 0.614909, acc 0.78125, learning_rate 0.000752332\n","2023-06-13T16:04:33.499655: step 3688, loss 0.716262, acc 0.84375, learning_rate 0.000752016\n","2023-06-13T16:04:34.938253: step 3689, loss 0.447087, acc 0.84375, learning_rate 0.0007517\n","2023-06-13T16:04:36.394014: step 3690, loss 0.949839, acc 0.78125, learning_rate 0.000751384\n","2023-06-13T16:04:37.856560: step 3691, loss 0.67587, acc 0.8125, learning_rate 0.000751068\n","2023-06-13T16:04:39.329850: step 3692, loss 1.06833, acc 0.65625, learning_rate 0.000750752\n","2023-06-13T16:04:40.763691: step 3693, loss 0.742546, acc 0.71875, learning_rate 0.000750436\n","2023-06-13T16:04:42.421111: step 3694, loss 0.795261, acc 0.71875, learning_rate 0.000750121\n","2023-06-13T16:04:45.050005: step 3695, loss 0.683109, acc 0.78125, learning_rate 0.000749806\n","2023-06-13T16:04:47.590003: step 3696, loss 0.786366, acc 0.75, learning_rate 0.000749491\n","2023-06-13T16:04:49.970920: step 3697, loss 1.04959, acc 0.71875, learning_rate 0.000749176\n","2023-06-13T16:04:52.338842: step 3698, loss 1.05306, acc 0.59375, learning_rate 0.000748861\n","2023-06-13T16:04:53.860460: step 3699, loss 0.89309, acc 0.625, learning_rate 0.000748546\n","\n","Evaluation:\n","2023-06-13T16:05:21.911913: step 3700, loss 2.27647, acc 0.38451\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3700\n","\n","2023-06-13T16:05:23.601616: step 3700, loss 0.733008, acc 0.78125, learning_rate 0.000748232\n","2023-06-13T16:05:26.161629: step 3701, loss 0.773865, acc 0.78125, learning_rate 0.000747918\n","2023-06-13T16:05:28.740632: step 3702, loss 1.09097, acc 0.65625, learning_rate 0.000747604\n","2023-06-13T16:05:31.082946: step 3703, loss 1.09065, acc 0.625, learning_rate 0.00074729\n","2023-06-13T16:05:33.425332: step 3704, loss 0.939511, acc 0.71875, learning_rate 0.000746976\n","2023-06-13T16:05:35.016611: step 3705, loss 0.814411, acc 0.75, learning_rate 0.000746662\n","2023-06-13T16:05:37.195601: step 3706, loss 0.796064, acc 0.6875, learning_rate 0.000746348\n","2023-06-13T16:05:39.709607: step 3707, loss 0.403772, acc 0.8125, learning_rate 0.000746035\n","2023-06-13T16:05:42.199703: step 3708, loss 0.733647, acc 0.78125, learning_rate 0.000745722\n","2023-06-13T16:05:44.690706: step 3709, loss 0.944568, acc 0.625, learning_rate 0.000745409\n","2023-06-13T16:05:47.457383: step 3710, loss 0.682323, acc 0.75, learning_rate 0.000745096\n","2023-06-13T16:05:49.973191: step 3711, loss 0.580064, acc 0.84375, learning_rate 0.000744783\n","2023-06-13T16:05:52.388616: step 3712, loss 0.857282, acc 0.71875, learning_rate 0.00074447\n","2023-06-13T16:05:54.737386: step 3713, loss 0.403794, acc 0.84375, learning_rate 0.000744158\n","2023-06-13T16:05:56.601163: step 3714, loss 0.71404, acc 0.78125, learning_rate 0.000743845\n","2023-06-13T16:05:58.054591: step 3715, loss 0.605761, acc 0.75, learning_rate 0.000743533\n","2023-06-13T16:05:59.476803: step 3716, loss 0.908806, acc 0.6875, learning_rate 0.000743221\n","2023-06-13T16:06:00.914446: step 3717, loss 0.976623, acc 0.65625, learning_rate 0.000742909\n","2023-06-13T16:06:02.345447: step 3718, loss 0.668311, acc 0.75, learning_rate 0.000742598\n","2023-06-13T16:06:03.778907: step 3719, loss 0.856005, acc 0.75, learning_rate 0.000742286\n","2023-06-13T16:06:05.233012: step 3720, loss 0.615183, acc 0.8125, learning_rate 0.000741975\n","2023-06-13T16:06:07.286018: step 3721, loss 0.568893, acc 0.84375, learning_rate 0.000741663\n","2023-06-13T16:06:09.795890: step 3722, loss 0.839387, acc 0.65625, learning_rate 0.000741352\n","2023-06-13T16:06:12.205602: step 3723, loss 0.724164, acc 0.78125, learning_rate 0.000741041\n","2023-06-13T16:06:14.513046: step 3724, loss 0.689443, acc 0.75, learning_rate 0.00074073\n","2023-06-13T16:06:16.598099: step 3725, loss 0.873903, acc 0.71875, learning_rate 0.00074042\n","2023-06-13T16:06:18.069722: step 3726, loss 0.870722, acc 0.6875, learning_rate 0.000740109\n","2023-06-13T16:06:19.543352: step 3727, loss 0.97753, acc 0.6875, learning_rate 0.000739799\n","2023-06-13T16:06:20.971281: step 3728, loss 0.634261, acc 0.78125, learning_rate 0.000739489\n","2023-06-13T16:06:22.401993: step 3729, loss 1.02582, acc 0.625, learning_rate 0.000739179\n","2023-06-13T16:06:23.846654: step 3730, loss 0.703836, acc 0.78125, learning_rate 0.000738869\n","2023-06-13T16:06:25.271897: step 3731, loss 1.14819, acc 0.6875, learning_rate 0.000738559\n","2023-06-13T16:06:27.051898: step 3732, loss 0.777595, acc 0.71875, learning_rate 0.000738249\n","2023-06-13T16:06:29.556640: step 3733, loss 0.584548, acc 0.8125, learning_rate 0.00073794\n","2023-06-13T16:06:31.816953: step 3734, loss 1.26907, acc 0.625, learning_rate 0.00073763\n","2023-06-13T16:06:34.063060: step 3735, loss 0.564548, acc 0.8125, learning_rate 0.000737321\n","2023-06-13T16:06:36.357251: step 3736, loss 1.0256, acc 0.75, learning_rate 0.000737012\n","2023-06-13T16:06:38.023213: step 3737, loss 1.06026, acc 0.59375, learning_rate 0.000736703\n","2023-06-13T16:06:39.493479: step 3738, loss 0.64745, acc 0.8125, learning_rate 0.000736395\n","2023-06-13T16:06:40.947997: step 3739, loss 1.06415, acc 0.65625, learning_rate 0.000736086\n","2023-06-13T16:06:42.456170: step 3740, loss 0.654367, acc 0.78125, learning_rate 0.000735778\n","2023-06-13T16:06:43.919614: step 3741, loss 0.58736, acc 0.84375, learning_rate 0.000735469\n","2023-06-13T16:06:45.358464: step 3742, loss 0.889595, acc 0.71875, learning_rate 0.000735161\n","2023-06-13T16:06:46.803378: step 3743, loss 0.496209, acc 0.8125, learning_rate 0.000734853\n","2023-06-13T16:06:49.085412: step 3744, loss 1.11151, acc 0.75, learning_rate 0.000734545\n","2023-06-13T16:06:51.665712: step 3745, loss 0.740478, acc 0.8125, learning_rate 0.000734238\n","2023-06-13T16:06:53.891164: step 3746, loss 1.37097, acc 0.5, learning_rate 0.00073393\n","2023-06-13T16:06:56.193919: step 3747, loss 0.503605, acc 0.78125, learning_rate 0.000733623\n","2023-06-13T16:06:58.200448: step 3748, loss 0.894874, acc 0.6875, learning_rate 0.000733316\n","2023-06-13T16:06:59.650059: step 3749, loss 0.877268, acc 0.65625, learning_rate 0.000733009\n","2023-06-13T16:07:01.104428: step 3750, loss 1.16799, acc 0.6875, learning_rate 0.000732702\n","2023-06-13T16:07:02.527209: step 3751, loss 0.714099, acc 0.84375, learning_rate 0.000732395\n","2023-06-13T16:07:03.990579: step 3752, loss 0.746361, acc 0.71875, learning_rate 0.000732088\n","2023-06-13T16:07:05.414446: step 3753, loss 1.27785, acc 0.5625, learning_rate 0.000731782\n","2023-06-13T16:07:06.859454: step 3754, loss 0.464126, acc 0.84375, learning_rate 0.000731475\n","2023-06-13T16:07:08.648566: step 3755, loss 0.763481, acc 0.78125, learning_rate 0.000731169\n","2023-06-13T16:07:11.214292: step 3756, loss 0.698127, acc 0.8125, learning_rate 0.000730863\n","2023-06-13T16:07:13.684495: step 3757, loss 0.610575, acc 0.8125, learning_rate 0.000730557\n","2023-06-13T16:07:16.073069: step 3758, loss 0.966201, acc 0.59375, learning_rate 0.000730251\n","2023-06-13T16:07:18.286623: step 3759, loss 0.795483, acc 0.71875, learning_rate 0.000729946\n","2023-06-13T16:07:19.732655: step 3760, loss 0.955194, acc 0.75, learning_rate 0.00072964\n","2023-06-13T16:07:21.152706: step 3761, loss 1.24603, acc 0.625, learning_rate 0.000729335\n","2023-06-13T16:07:22.554374: step 3762, loss 0.955491, acc 0.625, learning_rate 0.00072903\n","2023-06-13T16:07:23.994381: step 3763, loss 1.04077, acc 0.65625, learning_rate 0.000728725\n","2023-06-13T16:07:25.411832: step 3764, loss 0.52346, acc 0.875, learning_rate 0.00072842\n","2023-06-13T16:07:26.841045: step 3765, loss 1.29246, acc 0.53125, learning_rate 0.000728115\n","2023-06-13T16:07:28.516699: step 3766, loss 0.787885, acc 0.6875, learning_rate 0.000727811\n","2023-06-13T16:07:31.166279: step 3767, loss 0.768027, acc 0.84375, learning_rate 0.000727506\n","2023-06-13T16:07:33.561894: step 3768, loss 0.620331, acc 0.8125, learning_rate 0.000727202\n","2023-06-13T16:07:35.867638: step 3769, loss 1.02868, acc 0.65625, learning_rate 0.000726898\n","2023-06-13T16:07:38.238503: step 3770, loss 0.343636, acc 0.96875, learning_rate 0.000726594\n","2023-06-13T16:07:39.700325: step 3771, loss 1.27169, acc 0.53125, learning_rate 0.00072629\n","2023-06-13T16:07:41.141731: step 3772, loss 0.753691, acc 0.78125, learning_rate 0.000725987\n","2023-06-13T16:07:42.577460: step 3773, loss 1.02038, acc 0.65625, learning_rate 0.000725683\n","2023-06-13T16:07:44.054410: step 3774, loss 0.695075, acc 0.78125, learning_rate 0.00072538\n","2023-06-13T16:07:45.492976: step 3775, loss 1.26793, acc 0.5625, learning_rate 0.000725076\n","2023-06-13T16:07:46.971188: step 3776, loss 0.590137, acc 0.84375, learning_rate 0.000724773\n","2023-06-13T16:07:48.722588: step 3777, loss 1.29926, acc 0.59375, learning_rate 0.00072447\n","2023-06-13T16:07:51.347511: step 3778, loss 0.75311, acc 0.71875, learning_rate 0.000724168\n","2023-06-13T16:07:53.912510: step 3779, loss 1.02363, acc 0.6875, learning_rate 0.000723865\n","2023-06-13T16:07:56.167112: step 3780, loss 0.891016, acc 0.5625, learning_rate 0.000723563\n","2023-06-13T16:07:58.464127: step 3781, loss 0.700273, acc 0.71875, learning_rate 0.00072326\n","2023-06-13T16:07:59.903653: step 3782, loss 0.560968, acc 0.84375, learning_rate 0.000722958\n","2023-06-13T16:08:01.353212: step 3783, loss 0.799602, acc 0.71875, learning_rate 0.000722656\n","2023-06-13T16:08:02.803643: step 3784, loss 0.907385, acc 0.71875, learning_rate 0.000722354\n","2023-06-13T16:08:04.272004: step 3785, loss 0.831512, acc 0.625, learning_rate 0.000722052\n","2023-06-13T16:08:05.723280: step 3786, loss 0.984819, acc 0.65625, learning_rate 0.000721751\n","2023-06-13T16:08:07.241061: step 3787, loss 1.12538, acc 0.65625, learning_rate 0.000721449\n","2023-06-13T16:08:08.859127: step 3788, loss 0.916791, acc 0.625, learning_rate 0.000721148\n","2023-06-13T16:08:11.407618: step 3789, loss 0.972463, acc 0.78125, learning_rate 0.000720847\n","2023-06-13T16:08:13.817058: step 3790, loss 1.03011, acc 0.65625, learning_rate 0.000720546\n","2023-06-13T16:08:16.128000: step 3791, loss 0.52686, acc 0.875, learning_rate 0.000720245\n","2023-06-13T16:08:18.455975: step 3792, loss 0.9477, acc 0.6875, learning_rate 0.000719944\n","2023-06-13T16:08:19.900884: step 3793, loss 0.88523, acc 0.6875, learning_rate 0.000719643\n","2023-06-13T16:08:21.340979: step 3794, loss 0.640379, acc 0.8125, learning_rate 0.000719343\n","2023-06-13T16:08:22.796492: step 3795, loss 0.968308, acc 0.65625, learning_rate 0.000719043\n","2023-06-13T16:08:24.243241: step 3796, loss 0.652005, acc 0.75, learning_rate 0.000718742\n","2023-06-13T16:08:25.716986: step 3797, loss 0.808595, acc 0.71875, learning_rate 0.000718442\n","2023-06-13T16:08:27.196007: step 3798, loss 0.775283, acc 0.71875, learning_rate 0.000718143\n","2023-06-13T16:08:28.823650: step 3799, loss 0.625534, acc 0.8125, learning_rate 0.000717843\n","\n","Evaluation:\n","2023-06-13T16:09:01.168821: step 3800, loss 2.26392, acc 0.384965\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3800\n","\n","2023-06-13T16:09:02.751198: step 3800, loss 0.971786, acc 0.65625, learning_rate 0.000717543\n","2023-06-13T16:09:04.190616: step 3801, loss 0.887851, acc 0.65625, learning_rate 0.000717244\n","2023-06-13T16:09:05.598352: step 3802, loss 0.667477, acc 0.8125, learning_rate 0.000716945\n","2023-06-13T16:09:07.047249: step 3803, loss 0.989272, acc 0.75, learning_rate 0.000716645\n","2023-06-13T16:09:08.477618: step 3804, loss 1.22286, acc 0.59375, learning_rate 0.000716346\n","2023-06-13T16:09:10.370393: step 3805, loss 0.806698, acc 0.75, learning_rate 0.000716048\n","2023-06-13T16:09:12.989687: step 3806, loss 1.12343, acc 0.5625, learning_rate 0.000715749\n","2023-06-13T16:09:15.432433: step 3807, loss 0.734517, acc 0.8125, learning_rate 0.00071545\n","2023-06-13T16:09:17.829968: step 3808, loss 0.871069, acc 0.625, learning_rate 0.000715152\n","2023-06-13T16:09:19.848584: step 3809, loss 0.513027, acc 0.75, learning_rate 0.000714854\n","2023-06-13T16:09:21.262838: step 3810, loss 1.17834, acc 0.65625, learning_rate 0.000714555\n","2023-06-13T16:09:22.712158: step 3811, loss 0.902574, acc 0.71875, learning_rate 0.000714257\n","2023-06-13T16:09:24.150243: step 3812, loss 0.790666, acc 0.71875, learning_rate 0.00071396\n","2023-06-13T16:09:25.567391: step 3813, loss 1.12308, acc 0.65625, learning_rate 0.000713662\n","2023-06-13T16:09:27.017387: step 3814, loss 0.579727, acc 0.84375, learning_rate 0.000713364\n","2023-06-13T16:09:28.434548: step 3815, loss 0.735215, acc 0.78125, learning_rate 0.000713067\n","2023-06-13T16:09:30.343639: step 3816, loss 0.659584, acc 0.71875, learning_rate 0.00071277\n","2023-06-13T16:09:32.932811: step 3817, loss 0.685116, acc 0.8125, learning_rate 0.000712473\n","2023-06-13T16:09:35.310251: step 3818, loss 0.676834, acc 0.75, learning_rate 0.000712176\n","2023-06-13T16:09:37.658614: step 3819, loss 0.585776, acc 0.75, learning_rate 0.000711879\n","2023-06-13T16:09:39.724619: step 3820, loss 0.810149, acc 0.6875, learning_rate 0.000711582\n","2023-06-13T16:09:41.171050: step 3821, loss 0.727268, acc 0.78125, learning_rate 0.000711286\n","2023-06-13T16:09:42.614798: step 3822, loss 0.603147, acc 0.75, learning_rate 0.000710989\n","2023-06-13T16:09:44.100721: step 3823, loss 0.777811, acc 0.78125, learning_rate 0.000710693\n","2023-06-13T16:09:45.554371: step 3824, loss 0.930014, acc 0.59375, learning_rate 0.000710397\n","2023-06-13T16:09:46.997311: step 3825, loss 0.77649, acc 0.71875, learning_rate 0.000710101\n","2023-06-13T16:09:48.422881: step 3826, loss 0.93095, acc 0.75, learning_rate 0.000709805\n","2023-06-13T16:09:50.293543: step 3827, loss 0.677641, acc 0.84375, learning_rate 0.000709509\n","2023-06-13T16:09:52.854839: step 3828, loss 0.55945, acc 0.78125, learning_rate 0.000709214\n","2023-06-13T16:09:55.239643: step 3829, loss 1.14742, acc 0.65625, learning_rate 0.000708918\n","2023-06-13T16:09:57.560307: step 3830, loss 0.535928, acc 0.90625, learning_rate 0.000708623\n","2023-06-13T16:09:59.762704: step 3831, loss 0.569589, acc 0.8125, learning_rate 0.000708328\n","2023-06-13T16:10:01.238613: step 3832, loss 0.720677, acc 0.75, learning_rate 0.000708033\n","2023-06-13T16:10:02.690877: step 3833, loss 0.814744, acc 0.625, learning_rate 0.000707738\n","2023-06-13T16:10:04.128358: step 3834, loss 0.766931, acc 0.65625, learning_rate 0.000707444\n","2023-06-13T16:10:05.560765: step 3835, loss 0.785435, acc 0.71875, learning_rate 0.000707149\n","2023-06-13T16:10:07.030625: step 3836, loss 0.684796, acc 0.75, learning_rate 0.000706855\n","2023-06-13T16:10:08.460418: step 3837, loss 0.905194, acc 0.65625, learning_rate 0.00070656\n","2023-06-13T16:10:10.284569: step 3838, loss 1.1543, acc 0.625, learning_rate 0.000706266\n","2023-06-13T16:10:12.833071: step 3839, loss 0.964115, acc 0.6875, learning_rate 0.000705972\n","2023-06-13T16:10:15.327828: step 3840, loss 0.88989, acc 0.65625, learning_rate 0.000705678\n","2023-06-13T16:10:17.712937: step 3841, loss 0.897215, acc 0.625, learning_rate 0.000705385\n","2023-06-13T16:10:19.926436: step 3842, loss 0.73245, acc 0.6875, learning_rate 0.000705091\n","2023-06-13T16:10:21.346911: step 3843, loss 0.664304, acc 0.78125, learning_rate 0.000704798\n","2023-06-13T16:10:22.786011: step 3844, loss 0.925139, acc 0.625, learning_rate 0.000704505\n","2023-06-13T16:10:24.241009: step 3845, loss 0.659307, acc 0.78125, learning_rate 0.000704212\n","2023-06-13T16:10:25.662377: step 3846, loss 0.715783, acc 0.8125, learning_rate 0.000703919\n","2023-06-13T16:10:27.082207: step 3847, loss 0.965243, acc 0.71875, learning_rate 0.000703626\n","2023-06-13T16:10:28.499269: step 3848, loss 0.892711, acc 0.6875, learning_rate 0.000703333\n","2023-06-13T16:10:30.214648: step 3849, loss 0.588696, acc 0.84375, learning_rate 0.00070304\n","2023-06-13T16:10:32.804162: step 3850, loss 0.665935, acc 0.875, learning_rate 0.000702748\n","2023-06-13T16:10:35.310416: step 3851, loss 0.523006, acc 0.84375, learning_rate 0.000702456\n","2023-06-13T16:10:37.689336: step 3852, loss 0.790453, acc 0.75, learning_rate 0.000702164\n","2023-06-13T16:10:39.856270: step 3853, loss 0.807874, acc 0.8125, learning_rate 0.000701872\n","2023-06-13T16:10:41.285392: step 3854, loss 0.825522, acc 0.6875, learning_rate 0.00070158\n","2023-06-13T16:10:42.717162: step 3855, loss 1.1844, acc 0.53125, learning_rate 0.000701288\n","2023-06-13T16:10:44.164288: step 3856, loss 0.819719, acc 0.71875, learning_rate 0.000700997\n","2023-06-13T16:10:45.616357: step 3857, loss 0.848173, acc 0.71875, learning_rate 0.000700705\n","2023-06-13T16:10:47.074240: step 3858, loss 1.02132, acc 0.71875, learning_rate 0.000700414\n","2023-06-13T16:10:48.524548: step 3859, loss 0.688389, acc 0.78125, learning_rate 0.000700123\n","2023-06-13T16:10:50.226542: step 3860, loss 0.863509, acc 0.75, learning_rate 0.000699832\n","2023-06-13T16:10:52.843976: step 3861, loss 1.01079, acc 0.625, learning_rate 0.000699541\n","2023-06-13T16:10:55.286993: step 3862, loss 1.18042, acc 0.59375, learning_rate 0.00069925\n","2023-06-13T16:10:57.872070: step 3863, loss 1.12745, acc 0.6875, learning_rate 0.00069896\n","2023-06-13T16:11:00.646634: step 3864, loss 0.455989, acc 0.8125, learning_rate 0.000698669\n","2023-06-13T16:11:03.331243: step 3865, loss 1.10436, acc 0.59375, learning_rate 0.000698379\n","2023-06-13T16:11:05.774479: step 3866, loss 0.935129, acc 0.71875, learning_rate 0.000698089\n","2023-06-13T16:11:08.159624: step 3867, loss 0.610651, acc 0.8125, learning_rate 0.000697799\n","2023-06-13T16:11:09.934117: step 3868, loss 0.672967, acc 0.75, learning_rate 0.000697509\n","2023-06-13T16:11:11.360236: step 3869, loss 0.719872, acc 0.65625, learning_rate 0.000697219\n","2023-06-13T16:11:12.785946: step 3870, loss 0.60006, acc 0.8125, learning_rate 0.00069693\n","2023-06-13T16:11:14.877578: step 3871, loss 0.717434, acc 0.75, learning_rate 0.00069664\n","2023-06-13T16:11:17.340186: step 3872, loss 0.693598, acc 0.75, learning_rate 0.000696351\n","2023-06-13T16:11:19.818268: step 3873, loss 0.820529, acc 0.71875, learning_rate 0.000696062\n","2023-06-13T16:11:22.225869: step 3874, loss 0.742856, acc 0.75, learning_rate 0.000695773\n","2023-06-13T16:11:24.077268: step 3875, loss 0.763497, acc 0.75, learning_rate 0.000695484\n","2023-06-13T16:11:25.538870: step 3876, loss 0.784355, acc 0.65625, learning_rate 0.000695195\n","2023-06-13T16:11:27.023518: step 3877, loss 0.817399, acc 0.6875, learning_rate 0.000694907\n","2023-06-13T16:11:28.447277: step 3878, loss 0.866535, acc 0.78125, learning_rate 0.000694618\n","2023-06-13T16:11:29.876903: step 3879, loss 0.646788, acc 0.75, learning_rate 0.00069433\n","2023-06-13T16:11:31.289773: step 3880, loss 0.884796, acc 0.78125, learning_rate 0.000694042\n","2023-06-13T16:11:32.704224: step 3881, loss 0.939023, acc 0.65625, learning_rate 0.000693754\n","2023-06-13T16:11:34.776941: step 3882, loss 0.712401, acc 0.71875, learning_rate 0.000693466\n","2023-06-13T16:11:37.341392: step 3883, loss 0.828206, acc 0.625, learning_rate 0.000693178\n","2023-06-13T16:11:39.651589: step 3884, loss 1.2268, acc 0.625, learning_rate 0.00069289\n","2023-06-13T16:11:42.021000: step 3885, loss 0.824696, acc 0.71875, learning_rate 0.000692603\n","2023-06-13T16:11:44.009741: step 3886, loss 0.516585, acc 0.875, learning_rate 0.000692316\n","2023-06-13T16:11:45.426629: step 3887, loss 0.542767, acc 0.78125, learning_rate 0.000692028\n","2023-06-13T16:11:46.843870: step 3888, loss 0.732608, acc 0.78125, learning_rate 0.000691741\n","2023-06-13T16:11:48.267166: step 3889, loss 0.887283, acc 0.625, learning_rate 0.000691454\n","2023-06-13T16:11:49.689987: step 3890, loss 0.476089, acc 0.875, learning_rate 0.000691168\n","2023-06-13T16:11:51.108083: step 3891, loss 0.789378, acc 0.71875, learning_rate 0.000690881\n","2023-06-13T16:11:52.515881: step 3892, loss 0.557072, acc 0.8125, learning_rate 0.000690594\n","2023-06-13T16:11:54.468465: step 3893, loss 1.18697, acc 0.65625, learning_rate 0.000690308\n","2023-06-13T16:11:57.094565: step 3894, loss 0.652406, acc 0.8125, learning_rate 0.000690022\n","2023-06-13T16:11:59.478959: step 3895, loss 0.975694, acc 0.65625, learning_rate 0.000689736\n","2023-06-13T16:12:01.825476: step 3896, loss 0.759687, acc 0.75, learning_rate 0.00068945\n","2023-06-13T16:12:03.882885: step 3897, loss 0.629685, acc 0.78125, learning_rate 0.000689164\n","2023-06-13T16:12:05.326280: step 3898, loss 0.575142, acc 0.78125, learning_rate 0.000688878\n","2023-06-13T16:12:06.755642: step 3899, loss 0.660987, acc 0.8125, learning_rate 0.000688593\n","\n","Evaluation:\n","2023-06-13T16:12:34.410544: step 3900, loss 2.28996, acc 0.385723\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-3900\n","\n","2023-06-13T16:12:37.195834: step 3900, loss 1.03919, acc 0.71875, learning_rate 0.000688307\n","2023-06-13T16:12:39.604021: step 3901, loss 0.407778, acc 0.8125, learning_rate 0.000688022\n","2023-06-13T16:12:42.035441: step 3902, loss 0.696041, acc 0.78125, learning_rate 0.000687737\n","2023-06-13T16:12:44.432245: step 3903, loss 1.05396, acc 0.71875, learning_rate 0.000687452\n","2023-06-13T16:12:46.091950: step 3904, loss 0.768275, acc 0.65625, learning_rate 0.000687167\n","2023-06-13T16:12:47.546299: step 3905, loss 0.878308, acc 0.6875, learning_rate 0.000686882\n","2023-06-13T16:12:48.995841: step 3906, loss 0.725848, acc 0.78125, learning_rate 0.000686598\n","2023-06-13T16:12:50.414021: step 3907, loss 0.571377, acc 0.84375, learning_rate 0.000686313\n","2023-06-13T16:12:51.870220: step 3908, loss 0.659281, acc 0.78125, learning_rate 0.000686029\n","2023-06-13T16:12:53.295333: step 3909, loss 0.77002, acc 0.71875, learning_rate 0.000685745\n","2023-06-13T16:12:54.742953: step 3910, loss 1.14509, acc 0.5, learning_rate 0.000685461\n","2023-06-13T16:12:57.122880: step 3911, loss 0.832233, acc 0.625, learning_rate 0.000685177\n","2023-06-13T16:12:59.689155: step 3912, loss 0.743656, acc 0.8125, learning_rate 0.000684893\n","2023-06-13T16:13:02.084922: step 3913, loss 0.581343, acc 0.84375, learning_rate 0.00068461\n","2023-06-13T16:13:04.417353: step 3914, loss 0.856322, acc 0.71875, learning_rate 0.000684326\n","2023-06-13T16:13:06.180948: step 3915, loss 0.61157, acc 0.8125, learning_rate 0.000684043\n","2023-06-13T16:13:07.617634: step 3916, loss 0.948705, acc 0.71875, learning_rate 0.00068376\n","2023-06-13T16:13:09.031493: step 3917, loss 0.761837, acc 0.6875, learning_rate 0.000683477\n","2023-06-13T16:13:10.448579: step 3918, loss 0.679958, acc 0.78125, learning_rate 0.000683194\n","2023-06-13T16:13:11.890707: step 3919, loss 0.429256, acc 0.875, learning_rate 0.000682911\n","2023-06-13T16:13:13.336031: step 3920, loss 0.766588, acc 0.65625, learning_rate 0.000682628\n","2023-06-13T16:13:14.786912: step 3921, loss 0.732562, acc 0.75, learning_rate 0.000682346\n","2023-06-13T16:13:16.919816: step 3922, loss 0.656207, acc 0.84375, learning_rate 0.000682064\n","2023-06-13T16:13:19.492419: step 3923, loss 0.96492, acc 0.71875, learning_rate 0.000681781\n","2023-06-13T16:13:21.917778: step 3924, loss 0.912306, acc 0.65625, learning_rate 0.000681499\n","2023-06-13T16:13:24.248142: step 3925, loss 0.850603, acc 0.71875, learning_rate 0.000681217\n","2023-06-13T16:13:26.138650: step 3926, loss 1.26454, acc 0.625, learning_rate 0.000680935\n","2023-06-13T16:13:27.582506: step 3927, loss 0.612627, acc 0.78125, learning_rate 0.000680654\n","2023-06-13T16:13:29.040524: step 3928, loss 0.828459, acc 0.6875, learning_rate 0.000680372\n","2023-06-13T16:13:30.449337: step 3929, loss 0.913822, acc 0.71875, learning_rate 0.000680091\n","2023-06-13T16:13:31.894869: step 3930, loss 0.96662, acc 0.6875, learning_rate 0.00067981\n","2023-06-13T16:13:33.340328: step 3931, loss 0.966557, acc 0.65625, learning_rate 0.000679528\n","2023-06-13T16:13:34.771447: step 3932, loss 0.628391, acc 0.75, learning_rate 0.000679247\n","2023-06-13T16:13:36.733917: step 3933, loss 0.523523, acc 0.8125, learning_rate 0.000678967\n","2023-06-13T16:13:39.368330: step 3934, loss 0.755468, acc 0.71875, learning_rate 0.000678686\n","2023-06-13T16:13:41.784864: step 3935, loss 0.618856, acc 0.8125, learning_rate 0.000678405\n","2023-06-13T16:13:44.248692: step 3936, loss 0.929679, acc 0.75, learning_rate 0.000678125\n","2023-06-13T16:13:46.198390: step 3937, loss 0.969785, acc 0.625, learning_rate 0.000677844\n","2023-06-13T16:13:47.643769: step 3938, loss 0.655244, acc 0.75, learning_rate 0.000677564\n","2023-06-13T16:13:49.132058: step 3939, loss 0.455736, acc 0.90625, learning_rate 0.000677284\n","2023-06-13T16:13:50.554981: step 3940, loss 0.607154, acc 0.78125, learning_rate 0.000677004\n","2023-06-13T16:13:51.957917: step 3941, loss 0.521069, acc 0.75, learning_rate 0.000676725\n","2023-06-13T16:13:53.365963: step 3942, loss 0.841646, acc 0.75, learning_rate 0.000676445\n","2023-06-13T16:13:54.791584: step 3943, loss 0.784524, acc 0.6875, learning_rate 0.000676165\n","2023-06-13T16:13:56.643578: step 3944, loss 1.16157, acc 0.65625, learning_rate 0.000675886\n","2023-06-13T16:13:59.254124: step 3945, loss 0.597653, acc 0.8125, learning_rate 0.000675607\n","2023-06-13T16:14:01.513080: step 3946, loss 0.847819, acc 0.6875, learning_rate 0.000675328\n","2023-06-13T16:14:04.026542: step 3947, loss 0.970572, acc 0.6875, learning_rate 0.000675049\n","2023-06-13T16:14:06.126622: step 3948, loss 0.683722, acc 0.78125, learning_rate 0.00067477\n","2023-06-13T16:14:07.551590: step 3949, loss 1.03229, acc 0.625, learning_rate 0.000674491\n","2023-06-13T16:14:08.948673: step 3950, loss 0.5914, acc 0.71875, learning_rate 0.000674213\n","2023-06-13T16:14:10.401325: step 3951, loss 0.724149, acc 0.75, learning_rate 0.000673934\n","2023-06-13T16:14:11.835449: step 3952, loss 0.918232, acc 0.71875, learning_rate 0.000673656\n","2023-06-13T16:14:13.230857: step 3953, loss 0.631028, acc 0.8125, learning_rate 0.000673378\n","2023-06-13T16:14:14.702269: step 3954, loss 0.812568, acc 0.6875, learning_rate 0.0006731\n","2023-06-13T16:14:16.478104: step 3955, loss 1.16366, acc 0.65625, learning_rate 0.000672822\n","2023-06-13T16:14:19.021010: step 3956, loss 0.752147, acc 0.8125, learning_rate 0.000672544\n","2023-06-13T16:14:21.427461: step 3957, loss 0.679364, acc 0.75, learning_rate 0.000672267\n","2023-06-13T16:14:23.999121: step 3958, loss 0.981575, acc 0.75, learning_rate 0.000671989\n","2023-06-13T16:14:25.912395: step 3959, loss 0.683905, acc 0.84375, learning_rate 0.000671712\n","2023-06-13T16:14:27.327093: step 3960, loss 1.03937, acc 0.59375, learning_rate 0.000671434\n","2023-06-13T16:14:28.763589: step 3961, loss 0.643278, acc 0.78125, learning_rate 0.000671157\n","2023-06-13T16:14:30.196776: step 3962, loss 0.7355, acc 0.75, learning_rate 0.00067088\n","2023-06-13T16:14:31.597224: step 3963, loss 0.569614, acc 0.90625, learning_rate 0.000670604\n","2023-06-13T16:14:33.020975: step 3964, loss 0.921372, acc 0.6875, learning_rate 0.000670327\n","2023-06-13T16:14:34.447819: step 3965, loss 0.620377, acc 0.75, learning_rate 0.00067005\n","2023-06-13T16:14:36.214872: step 3966, loss 0.592352, acc 0.71875, learning_rate 0.000669774\n","2023-06-13T16:14:38.585666: step 3967, loss 0.761232, acc 0.78125, learning_rate 0.000669498\n","2023-06-13T16:14:41.038921: step 3968, loss 0.478203, acc 0.90625, learning_rate 0.000669222\n","2023-06-13T16:14:43.351123: step 3969, loss 0.488028, acc 0.8125, learning_rate 0.000668946\n","2023-06-13T16:14:45.650943: step 3970, loss 1.16622, acc 0.5625, learning_rate 0.00066867\n","2023-06-13T16:14:47.068480: step 3971, loss 0.830445, acc 0.71875, learning_rate 0.000668394\n","2023-06-13T16:14:48.485266: step 3972, loss 1.00424, acc 0.53125, learning_rate 0.000668118\n","2023-06-13T16:14:49.912357: step 3973, loss 0.840827, acc 0.71875, learning_rate 0.000667843\n","2023-06-13T16:14:51.307090: step 3974, loss 0.929984, acc 0.71875, learning_rate 0.000667568\n","2023-06-13T16:14:52.699633: step 3975, loss 0.565602, acc 0.84375, learning_rate 0.000667292\n","2023-06-13T16:14:54.093264: step 3976, loss 1.0882, acc 0.6875, learning_rate 0.000667017\n","2023-06-13T16:14:55.510408: step 3977, loss 0.656804, acc 0.71875, learning_rate 0.000666742\n","2023-06-13T16:14:57.907051: step 3978, loss 0.577852, acc 0.8125, learning_rate 0.000666468\n","2023-06-13T16:15:00.490316: step 3979, loss 0.342381, acc 0.90625, learning_rate 0.000666193\n","2023-06-13T16:15:02.795503: step 3980, loss 0.614034, acc 0.8125, learning_rate 0.000665918\n","2023-06-13T16:15:05.016052: step 3981, loss 0.579464, acc 0.8125, learning_rate 0.000665644\n","2023-06-13T16:15:06.835947: step 3982, loss 0.697906, acc 0.78125, learning_rate 0.00066537\n","2023-06-13T16:15:08.268125: step 3983, loss 0.903803, acc 0.71875, learning_rate 0.000665096\n","2023-06-13T16:15:09.679541: step 3984, loss 0.820669, acc 0.6875, learning_rate 0.000664822\n","2023-06-13T16:15:11.159545: step 3985, loss 0.52869, acc 0.875, learning_rate 0.000664548\n","2023-06-13T16:15:12.599119: step 3986, loss 0.738174, acc 0.78125, learning_rate 0.000664274\n","2023-06-13T16:15:14.013507: step 3987, loss 0.754703, acc 0.75, learning_rate 0.000664\n","2023-06-13T16:15:15.470661: step 3988, loss 0.6086, acc 0.75, learning_rate 0.000663727\n","2023-06-13T16:15:17.426758: step 3989, loss 0.759423, acc 0.6875, learning_rate 0.000663454\n","2023-06-13T16:15:20.002270: step 3990, loss 0.723987, acc 0.78125, learning_rate 0.00066318\n","2023-06-13T16:15:22.360411: step 3991, loss 0.779415, acc 0.71875, learning_rate 0.000662907\n","2023-06-13T16:15:24.722451: step 3992, loss 1.00758, acc 0.65625, learning_rate 0.000662634\n","2023-06-13T16:15:26.664325: step 3993, loss 0.572978, acc 0.8125, learning_rate 0.000662362\n","2023-06-13T16:15:28.083138: step 3994, loss 0.669061, acc 0.78125, learning_rate 0.000662089\n","2023-06-13T16:15:29.517026: step 3995, loss 0.575507, acc 0.8125, learning_rate 0.000661816\n","2023-06-13T16:15:30.982722: step 3996, loss 0.931925, acc 0.6875, learning_rate 0.000661544\n","2023-06-13T16:15:32.380325: step 3997, loss 0.807935, acc 0.6875, learning_rate 0.000661272\n","2023-06-13T16:15:33.790285: step 3998, loss 0.691362, acc 0.78125, learning_rate 0.000660999\n","2023-06-13T16:15:35.216392: step 3999, loss 0.951972, acc 0.71875, learning_rate 0.000660727\n","\n","Evaluation:\n","2023-06-13T16:16:07.200525: step 4000, loss 2.33315, acc 0.388148\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4000\n","\n","2023-06-13T16:16:08.810827: step 4000, loss 0.785551, acc 0.78125, learning_rate 0.000660456\n","2023-06-13T16:16:10.250571: step 4001, loss 0.802216, acc 0.75, learning_rate 0.000660184\n","2023-06-13T16:16:11.697881: step 4002, loss 0.919751, acc 0.65625, learning_rate 0.000659912\n","2023-06-13T16:16:13.161678: step 4003, loss 0.866772, acc 0.6875, learning_rate 0.000659641\n","2023-06-13T16:16:14.680888: step 4004, loss 0.827964, acc 0.6875, learning_rate 0.000659369\n","2023-06-13T16:16:16.119470: step 4005, loss 1.14535, acc 0.6875, learning_rate 0.000659098\n","2023-06-13T16:16:17.763414: step 4006, loss 0.942079, acc 0.6875, learning_rate 0.000658827\n","2023-06-13T16:16:20.293344: step 4007, loss 0.622054, acc 0.8125, learning_rate 0.000658556\n","2023-06-13T16:16:22.710062: step 4008, loss 0.448288, acc 0.90625, learning_rate 0.000658285\n","2023-06-13T16:16:25.164037: step 4009, loss 0.744866, acc 0.8125, learning_rate 0.000658015\n","2023-06-13T16:16:27.383695: step 4010, loss 0.543472, acc 0.8125, learning_rate 0.000657744\n","2023-06-13T16:16:28.812997: step 4011, loss 0.779926, acc 0.71875, learning_rate 0.000657474\n","2023-06-13T16:16:30.234400: step 4012, loss 0.785966, acc 0.6875, learning_rate 0.000657203\n","2023-06-13T16:16:31.710996: step 4013, loss 0.853871, acc 0.6875, learning_rate 0.000656933\n","2023-06-13T16:16:33.140834: step 4014, loss 0.863774, acc 0.65625, learning_rate 0.000656663\n","2023-06-13T16:16:34.578421: step 4015, loss 0.54775, acc 0.78125, learning_rate 0.000656393\n","2023-06-13T16:16:36.012093: step 4016, loss 0.749355, acc 0.75, learning_rate 0.000656123\n","2023-06-13T16:16:37.644658: step 4017, loss 0.827379, acc 0.71875, learning_rate 0.000655854\n","2023-06-13T16:16:40.223952: step 4018, loss 0.628056, acc 0.71875, learning_rate 0.000655584\n","2023-06-13T16:16:42.728184: step 4019, loss 0.724723, acc 0.78125, learning_rate 0.000655315\n","2023-06-13T16:16:45.249400: step 4020, loss 0.699852, acc 0.8125, learning_rate 0.000655046\n","2023-06-13T16:16:47.337148: step 4021, loss 0.949701, acc 0.6875, learning_rate 0.000654776\n","2023-06-13T16:16:48.790730: step 4022, loss 0.702668, acc 0.75, learning_rate 0.000654507\n","2023-06-13T16:16:50.217445: step 4023, loss 0.708831, acc 0.8125, learning_rate 0.000654239\n","2023-06-13T16:16:51.659780: step 4024, loss 1.06342, acc 0.6875, learning_rate 0.00065397\n","2023-06-13T16:16:53.103036: step 4025, loss 1.20035, acc 0.625, learning_rate 0.000653701\n","2023-06-13T16:16:54.544565: step 4026, loss 0.953902, acc 0.75, learning_rate 0.000653433\n","2023-06-13T16:16:55.978630: step 4027, loss 0.643402, acc 0.75, learning_rate 0.000653164\n","2023-06-13T16:16:57.832290: step 4028, loss 0.834471, acc 0.78125, learning_rate 0.000652896\n","2023-06-13T16:17:00.399405: step 4029, loss 0.843429, acc 0.71875, learning_rate 0.000652628\n","2023-06-13T16:17:03.316739: step 4030, loss 0.513555, acc 0.84375, learning_rate 0.00065236\n","2023-06-13T16:17:06.039634: step 4031, loss 0.466122, acc 0.84375, learning_rate 0.000652092\n","2023-06-13T16:17:08.674117: step 4032, loss 0.712718, acc 0.65625, learning_rate 0.000651825\n","2023-06-13T16:17:11.348908: step 4033, loss 0.66686, acc 0.65625, learning_rate 0.000651557\n","2023-06-13T16:17:13.850690: step 4034, loss 0.732544, acc 0.71875, learning_rate 0.00065129\n","2023-06-13T16:17:15.995480: step 4035, loss 0.549345, acc 0.84375, learning_rate 0.000651022\n","2023-06-13T16:17:17.432929: step 4036, loss 0.805883, acc 0.6875, learning_rate 0.000650755\n","2023-06-13T16:17:18.854044: step 4037, loss 0.76029, acc 0.8125, learning_rate 0.000650488\n","2023-06-13T16:17:20.274173: step 4038, loss 0.664322, acc 0.78125, learning_rate 0.000650221\n","2023-06-13T16:17:21.708006: step 4039, loss 0.657779, acc 0.78125, learning_rate 0.000649954\n","2023-06-13T16:17:23.105736: step 4040, loss 0.925744, acc 0.65625, learning_rate 0.000649688\n","2023-06-13T16:17:25.409992: step 4041, loss 0.401055, acc 0.84375, learning_rate 0.000649421\n","2023-06-13T16:17:27.913057: step 4042, loss 0.760173, acc 0.65625, learning_rate 0.000649155\n","2023-06-13T16:17:30.236244: step 4043, loss 0.382271, acc 0.9375, learning_rate 0.000648888\n","2023-06-13T16:17:32.711025: step 4044, loss 0.692611, acc 0.75, learning_rate 0.000648622\n","2023-06-13T16:17:34.612520: step 4045, loss 0.663908, acc 0.75, learning_rate 0.000648356\n","2023-06-13T16:17:36.062032: step 4046, loss 0.683263, acc 0.84375, learning_rate 0.00064809\n","2023-06-13T16:17:37.518704: step 4047, loss 1.03309, acc 0.65625, learning_rate 0.000647825\n","2023-06-13T16:17:38.968815: step 4048, loss 0.740643, acc 0.75, learning_rate 0.000647559\n","2023-06-13T16:17:40.403673: step 4049, loss 0.792149, acc 0.65625, learning_rate 0.000647294\n","2023-06-13T16:17:41.846729: step 4050, loss 0.881384, acc 0.71875, learning_rate 0.000647028\n","2023-06-13T16:17:43.268991: step 4051, loss 0.863218, acc 0.6875, learning_rate 0.000646763\n","2023-06-13T16:17:45.174691: step 4052, loss 1.12647, acc 0.5625, learning_rate 0.000646498\n","2023-06-13T16:17:47.674533: step 4053, loss 0.451817, acc 0.90625, learning_rate 0.000646233\n","2023-06-13T16:17:50.101171: step 4054, loss 0.571935, acc 0.8125, learning_rate 0.000645968\n","2023-06-13T16:17:52.359815: step 4055, loss 0.956488, acc 0.71875, learning_rate 0.000645703\n","2023-06-13T16:17:54.579757: step 4056, loss 0.850356, acc 0.65625, learning_rate 0.000645439\n","2023-06-13T16:17:56.012522: step 4057, loss 0.693062, acc 0.8125, learning_rate 0.000645174\n","2023-06-13T16:17:57.480019: step 4058, loss 0.815162, acc 0.6875, learning_rate 0.00064491\n","2023-06-13T16:17:58.904091: step 4059, loss 0.832722, acc 0.71875, learning_rate 0.000644646\n","2023-06-13T16:18:00.336070: step 4060, loss 0.941073, acc 0.71875, learning_rate 0.000644382\n","2023-06-13T16:18:01.778955: step 4061, loss 1.38177, acc 0.53125, learning_rate 0.000644118\n","2023-06-13T16:18:03.253848: step 4062, loss 0.730029, acc 0.625, learning_rate 0.000643854\n","2023-06-13T16:18:04.845973: step 4063, loss 0.793184, acc 0.6875, learning_rate 0.00064359\n","2023-06-13T16:18:07.360564: step 4064, loss 0.947542, acc 0.78125, learning_rate 0.000643326\n","2023-06-13T16:18:09.694862: step 4065, loss 0.905592, acc 0.75, learning_rate 0.000643063\n","2023-06-13T16:18:11.970561: step 4066, loss 1.19778, acc 0.625, learning_rate 0.0006428\n","2023-06-13T16:18:14.374622: step 4067, loss 0.76894, acc 0.75, learning_rate 0.000642536\n","2023-06-13T16:18:15.977338: step 4068, loss 0.898358, acc 0.75, learning_rate 0.000642273\n","2023-06-13T16:18:17.421925: step 4069, loss 0.709835, acc 0.71875, learning_rate 0.00064201\n","2023-06-13T16:18:18.832317: step 4070, loss 0.756692, acc 0.71875, learning_rate 0.000641748\n","2023-06-13T16:18:20.280648: step 4071, loss 1.28538, acc 0.5625, learning_rate 0.000641485\n","2023-06-13T16:18:21.697560: step 4072, loss 1.09162, acc 0.71875, learning_rate 0.000641222\n","2023-06-13T16:18:23.127937: step 4073, loss 0.625487, acc 0.8125, learning_rate 0.00064096\n","2023-06-13T16:18:24.574616: step 4074, loss 0.611669, acc 0.75, learning_rate 0.000640698\n","2023-06-13T16:18:26.909734: step 4075, loss 1.21542, acc 0.5625, learning_rate 0.000640436\n","2023-06-13T16:18:29.415646: step 4076, loss 1.24802, acc 0.5, learning_rate 0.000640173\n","2023-06-13T16:18:31.779831: step 4077, loss 0.849328, acc 0.71875, learning_rate 0.000639912\n","2023-06-13T16:18:34.082828: step 4078, loss 0.967344, acc 0.6875, learning_rate 0.00063965\n","2023-06-13T16:18:35.800917: step 4079, loss 0.913557, acc 0.6875, learning_rate 0.000639388\n","2023-06-13T16:18:37.225241: step 4080, loss 0.744639, acc 0.71875, learning_rate 0.000639127\n","2023-06-13T16:18:38.664376: step 4081, loss 0.824634, acc 0.78125, learning_rate 0.000638865\n","2023-06-13T16:18:40.086375: step 4082, loss 0.964347, acc 0.625, learning_rate 0.000638604\n","2023-06-13T16:18:41.520938: step 4083, loss 0.587636, acc 0.78125, learning_rate 0.000638343\n","2023-06-13T16:18:42.920458: step 4084, loss 0.58667, acc 0.8125, learning_rate 0.000638082\n","2023-06-13T16:18:44.362408: step 4085, loss 1.2295, acc 0.625, learning_rate 0.000637821\n","2023-06-13T16:18:46.401423: step 4086, loss 0.774642, acc 0.6875, learning_rate 0.00063756\n","2023-06-13T16:18:48.854812: step 4087, loss 0.961053, acc 0.75, learning_rate 0.000637299\n","2023-06-13T16:18:51.030185: step 4088, loss 0.537957, acc 0.90625, learning_rate 0.000637039\n","2023-06-13T16:18:53.406453: step 4089, loss 0.990428, acc 0.65625, learning_rate 0.000636778\n","2023-06-13T16:18:55.476344: step 4090, loss 1.16599, acc 0.71875, learning_rate 0.000636518\n","2023-06-13T16:18:56.900377: step 4091, loss 0.683841, acc 0.71875, learning_rate 0.000636258\n","2023-06-13T16:18:58.322731: step 4092, loss 0.39168, acc 0.84375, learning_rate 0.000635998\n","2023-06-13T16:18:59.730604: step 4093, loss 1.02221, acc 0.65625, learning_rate 0.000635738\n","2023-06-13T16:19:01.143977: step 4094, loss 0.550848, acc 0.8125, learning_rate 0.000635478\n","2023-06-13T16:19:02.557093: step 4095, loss 0.93695, acc 0.71875, learning_rate 0.000635219\n","2023-06-13T16:19:04.047966: step 4096, loss 0.359184, acc 0.875, learning_rate 0.000634959\n","2023-06-13T16:19:05.643033: step 4097, loss 0.894905, acc 0.6875, learning_rate 0.0006347\n","2023-06-13T16:19:08.202388: step 4098, loss 0.643473, acc 0.84375, learning_rate 0.000634441\n","2023-06-13T16:19:10.582008: step 4099, loss 0.511719, acc 0.8125, learning_rate 0.000634181\n","\n","Evaluation:\n","2023-06-13T16:19:39.707171: step 4100, loss 2.31939, acc 0.386632\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4100\n","\n","2023-06-13T16:19:41.309783: step 4100, loss 0.751931, acc 0.75, learning_rate 0.000633922\n","2023-06-13T16:19:42.742240: step 4101, loss 0.895738, acc 0.6875, learning_rate 0.000633663\n","2023-06-13T16:19:44.152831: step 4102, loss 1.13122, acc 0.59375, learning_rate 0.000633405\n","2023-06-13T16:19:45.605430: step 4103, loss 0.54215, acc 0.78125, learning_rate 0.000633146\n","2023-06-13T16:19:47.368961: step 4104, loss 0.467904, acc 0.84375, learning_rate 0.000632888\n","2023-06-13T16:19:49.960189: step 4105, loss 0.835125, acc 0.6875, learning_rate 0.000632629\n","2023-06-13T16:19:52.259526: step 4106, loss 0.917473, acc 0.6875, learning_rate 0.000632371\n","2023-06-13T16:19:54.524100: step 4107, loss 0.931242, acc 0.65625, learning_rate 0.000632113\n","2023-06-13T16:19:56.828321: step 4108, loss 0.780462, acc 0.625, learning_rate 0.000631855\n","2023-06-13T16:19:58.254767: step 4109, loss 0.920407, acc 0.71875, learning_rate 0.000631597\n","2023-06-13T16:19:59.656433: step 4110, loss 0.713415, acc 0.71875, learning_rate 0.000631339\n","2023-06-13T16:20:01.093251: step 4111, loss 0.683579, acc 0.75, learning_rate 0.000631082\n","2023-06-13T16:20:02.521138: step 4112, loss 0.46003, acc 0.9375, learning_rate 0.000630824\n","2023-06-13T16:20:03.969896: step 4113, loss 0.597336, acc 0.78125, learning_rate 0.000630567\n","2023-06-13T16:20:05.408369: step 4114, loss 0.985706, acc 0.65625, learning_rate 0.000630309\n","2023-06-13T16:20:06.899928: step 4115, loss 0.867625, acc 0.625, learning_rate 0.000630052\n","2023-06-13T16:20:09.376036: step 4116, loss 0.360979, acc 0.9375, learning_rate 0.000629795\n","2023-06-13T16:20:11.847483: step 4117, loss 0.785005, acc 0.6875, learning_rate 0.000629538\n","2023-06-13T16:20:14.025272: step 4118, loss 0.757407, acc 0.59375, learning_rate 0.000629282\n","2023-06-13T16:20:16.260892: step 4119, loss 0.938, acc 0.5625, learning_rate 0.000629025\n","2023-06-13T16:20:17.999048: step 4120, loss 0.615431, acc 0.8125, learning_rate 0.000628768\n","2023-06-13T16:20:19.431207: step 4121, loss 0.867682, acc 0.6875, learning_rate 0.000628512\n","2023-06-13T16:20:20.891465: step 4122, loss 0.64547, acc 0.84375, learning_rate 0.000628256\n","2023-06-13T16:20:22.290703: step 4123, loss 0.714683, acc 0.6875, learning_rate 0.000628\n","2023-06-13T16:20:23.395389: step 4124, loss 1.02303, acc 0.75, learning_rate 0.000627744\n","2023-06-13T16:20:24.791587: step 4125, loss 0.549115, acc 0.8125, learning_rate 0.000627488\n","2023-06-13T16:20:26.219324: step 4126, loss 0.764558, acc 0.71875, learning_rate 0.000627232\n","2023-06-13T16:20:28.036986: step 4127, loss 0.91875, acc 0.75, learning_rate 0.000626976\n","2023-06-13T16:20:30.579191: step 4128, loss 0.892345, acc 0.625, learning_rate 0.000626721\n","2023-06-13T16:20:32.852452: step 4129, loss 0.524019, acc 0.875, learning_rate 0.000626465\n","2023-06-13T16:20:35.141211: step 4130, loss 0.638269, acc 0.8125, learning_rate 0.00062621\n","2023-06-13T16:20:37.297983: step 4131, loss 0.591171, acc 0.75, learning_rate 0.000625955\n","2023-06-13T16:20:38.912162: step 4132, loss 0.486665, acc 0.84375, learning_rate 0.0006257\n","2023-06-13T16:20:40.357230: step 4133, loss 0.682837, acc 0.71875, learning_rate 0.000625445\n","2023-06-13T16:20:41.892605: step 4134, loss 0.352488, acc 0.875, learning_rate 0.00062519\n","2023-06-13T16:20:43.303724: step 4135, loss 0.79002, acc 0.6875, learning_rate 0.000624936\n","2023-06-13T16:20:44.770663: step 4136, loss 0.532368, acc 0.84375, learning_rate 0.000624681\n","2023-06-13T16:20:46.226131: step 4137, loss 0.748609, acc 0.65625, learning_rate 0.000624427\n","2023-06-13T16:20:47.666420: step 4138, loss 0.836539, acc 0.71875, learning_rate 0.000624172\n","2023-06-13T16:20:50.137220: step 4139, loss 0.863559, acc 0.84375, learning_rate 0.000623918\n","2023-06-13T16:20:52.578843: step 4140, loss 0.652059, acc 0.8125, learning_rate 0.000623664\n","2023-06-13T16:20:54.893103: step 4141, loss 0.921713, acc 0.78125, learning_rate 0.00062341\n","2023-06-13T16:20:57.243777: step 4142, loss 0.795435, acc 0.71875, learning_rate 0.000623157\n","2023-06-13T16:20:59.048224: step 4143, loss 0.447028, acc 0.875, learning_rate 0.000622903\n","2023-06-13T16:21:00.543144: step 4144, loss 0.514835, acc 0.84375, learning_rate 0.000622649\n","2023-06-13T16:21:01.989510: step 4145, loss 0.473032, acc 0.84375, learning_rate 0.000622396\n","2023-06-13T16:21:03.426233: step 4146, loss 0.481998, acc 0.90625, learning_rate 0.000622143\n","2023-06-13T16:21:04.883249: step 4147, loss 0.735332, acc 0.8125, learning_rate 0.000621889\n","2023-06-13T16:21:06.379679: step 4148, loss 0.846264, acc 0.71875, learning_rate 0.000621636\n","2023-06-13T16:21:07.778037: step 4149, loss 0.678376, acc 0.84375, learning_rate 0.000621383\n","2023-06-13T16:21:10.046594: step 4150, loss 0.799944, acc 0.71875, learning_rate 0.000621131\n","2023-06-13T16:21:12.518130: step 4151, loss 0.700251, acc 0.8125, learning_rate 0.000620878\n","2023-06-13T16:21:14.824372: step 4152, loss 0.773951, acc 0.75, learning_rate 0.000620625\n","2023-06-13T16:21:17.169845: step 4153, loss 0.747685, acc 0.71875, learning_rate 0.000620373\n","2023-06-13T16:21:18.942130: step 4154, loss 0.489527, acc 0.78125, learning_rate 0.000620121\n","2023-06-13T16:21:20.367857: step 4155, loss 0.660307, acc 0.71875, learning_rate 0.000619868\n","2023-06-13T16:21:21.828468: step 4156, loss 0.564383, acc 0.75, learning_rate 0.000619616\n","2023-06-13T16:21:23.242341: step 4157, loss 0.69847, acc 0.71875, learning_rate 0.000619364\n","2023-06-13T16:21:24.690139: step 4158, loss 0.651039, acc 0.75, learning_rate 0.000619113\n","2023-06-13T16:21:26.154719: step 4159, loss 0.710733, acc 0.75, learning_rate 0.000618861\n","2023-06-13T16:21:27.592283: step 4160, loss 0.378111, acc 0.90625, learning_rate 0.000618609\n","2023-06-13T16:21:29.726128: step 4161, loss 0.58478, acc 0.78125, learning_rate 0.000618358\n","2023-06-13T16:21:32.286261: step 4162, loss 0.724599, acc 0.75, learning_rate 0.000618106\n","2023-06-13T16:21:34.604389: step 4163, loss 0.925482, acc 0.65625, learning_rate 0.000617855\n","2023-06-13T16:21:36.902004: step 4164, loss 0.829361, acc 0.75, learning_rate 0.000617604\n","2023-06-13T16:21:38.878221: step 4165, loss 0.690664, acc 0.84375, learning_rate 0.000617353\n","2023-06-13T16:21:40.311956: step 4166, loss 1.05904, acc 0.71875, learning_rate 0.000617102\n","2023-06-13T16:21:41.746107: step 4167, loss 0.735004, acc 0.71875, learning_rate 0.000616852\n","2023-06-13T16:21:43.144491: step 4168, loss 0.358737, acc 0.875, learning_rate 0.000616601\n","2023-06-13T16:21:44.574764: step 4169, loss 0.622496, acc 0.78125, learning_rate 0.00061635\n","2023-06-13T16:21:45.995753: step 4170, loss 0.469181, acc 0.875, learning_rate 0.0006161\n","2023-06-13T16:21:47.413528: step 4171, loss 0.494806, acc 0.84375, learning_rate 0.00061585\n","2023-06-13T16:21:49.208386: step 4172, loss 0.735808, acc 0.78125, learning_rate 0.0006156\n","2023-06-13T16:21:51.779880: step 4173, loss 1.19708, acc 0.6875, learning_rate 0.00061535\n","2023-06-13T16:21:54.234394: step 4174, loss 0.701974, acc 0.78125, learning_rate 0.0006151\n","2023-06-13T16:21:56.706104: step 4175, loss 0.896309, acc 0.6875, learning_rate 0.00061485\n","2023-06-13T16:21:58.532874: step 4176, loss 0.413412, acc 0.8125, learning_rate 0.0006146\n","2023-06-13T16:21:59.959051: step 4177, loss 0.669819, acc 0.78125, learning_rate 0.000614351\n","2023-06-13T16:22:01.383392: step 4178, loss 0.784245, acc 0.75, learning_rate 0.000614102\n","2023-06-13T16:22:02.813230: step 4179, loss 0.347723, acc 0.96875, learning_rate 0.000613852\n","2023-06-13T16:22:04.208337: step 4180, loss 0.683612, acc 0.71875, learning_rate 0.000613603\n","2023-06-13T16:22:05.681893: step 4181, loss 0.792053, acc 0.6875, learning_rate 0.000613354\n","2023-06-13T16:22:07.130754: step 4182, loss 1.23688, acc 0.71875, learning_rate 0.000613105\n","2023-06-13T16:22:08.940541: step 4183, loss 0.544175, acc 0.78125, learning_rate 0.000612856\n","2023-06-13T16:22:11.595652: step 4184, loss 0.651094, acc 0.8125, learning_rate 0.000612608\n","2023-06-13T16:22:13.923357: step 4185, loss 0.690775, acc 0.78125, learning_rate 0.000612359\n","2023-06-13T16:22:16.270581: step 4186, loss 0.7758, acc 0.625, learning_rate 0.000612111\n","2023-06-13T16:22:18.359778: step 4187, loss 0.896035, acc 0.6875, learning_rate 0.000611862\n","2023-06-13T16:22:19.778975: step 4188, loss 0.726105, acc 0.78125, learning_rate 0.000611614\n","2023-06-13T16:22:21.207022: step 4189, loss 0.50203, acc 0.84375, learning_rate 0.000611366\n","2023-06-13T16:22:22.623198: step 4190, loss 0.592564, acc 0.84375, learning_rate 0.000611118\n","2023-06-13T16:22:24.035278: step 4191, loss 1.22663, acc 0.6875, learning_rate 0.00061087\n","2023-06-13T16:22:25.468335: step 4192, loss 0.522871, acc 0.8125, learning_rate 0.000610623\n","2023-06-13T16:22:26.877102: step 4193, loss 0.383517, acc 0.8125, learning_rate 0.000610375\n","2023-06-13T16:22:28.483083: step 4194, loss 0.407762, acc 0.875, learning_rate 0.000610128\n","2023-06-13T16:22:31.069014: step 4195, loss 0.321414, acc 0.9375, learning_rate 0.00060988\n","2023-06-13T16:22:33.554913: step 4196, loss 0.706616, acc 0.78125, learning_rate 0.000609633\n","2023-06-13T16:22:35.941936: step 4197, loss 0.564487, acc 0.78125, learning_rate 0.000609386\n","2023-06-13T16:22:38.471459: step 4198, loss 0.695061, acc 0.75, learning_rate 0.000609139\n","2023-06-13T16:22:41.181875: step 4199, loss 0.472298, acc 0.84375, learning_rate 0.000608892\n","\n","Evaluation:\n","2023-06-13T16:23:12.881540: step 4200, loss 2.37714, acc 0.389664\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4200\n","\n","2023-06-13T16:23:15.557148: step 4200, loss 0.632653, acc 0.78125, learning_rate 0.000608645\n","2023-06-13T16:23:17.749244: step 4201, loss 0.617589, acc 0.8125, learning_rate 0.000608399\n","2023-06-13T16:23:20.043340: step 4202, loss 0.457071, acc 0.90625, learning_rate 0.000608152\n","2023-06-13T16:23:21.484306: step 4203, loss 0.772687, acc 0.71875, learning_rate 0.000607906\n","2023-06-13T16:23:22.890820: step 4204, loss 0.653449, acc 0.65625, learning_rate 0.000607659\n","2023-06-13T16:23:24.273481: step 4205, loss 0.780134, acc 0.8125, learning_rate 0.000607413\n","2023-06-13T16:23:25.727532: step 4206, loss 0.625265, acc 0.78125, learning_rate 0.000607167\n","2023-06-13T16:23:27.159861: step 4207, loss 0.614571, acc 0.84375, learning_rate 0.000606921\n","2023-06-13T16:23:28.603387: step 4208, loss 0.772347, acc 0.75, learning_rate 0.000606676\n","2023-06-13T16:23:30.142708: step 4209, loss 0.725677, acc 0.71875, learning_rate 0.00060643\n","2023-06-13T16:23:32.624381: step 4210, loss 0.988943, acc 0.625, learning_rate 0.000606184\n","2023-06-13T16:23:34.992099: step 4211, loss 0.727542, acc 0.6875, learning_rate 0.000605939\n","2023-06-13T16:23:37.426827: step 4212, loss 0.614414, acc 0.78125, learning_rate 0.000605694\n","2023-06-13T16:23:39.667551: step 4213, loss 0.872897, acc 0.71875, learning_rate 0.000605448\n","2023-06-13T16:23:41.246700: step 4214, loss 0.723834, acc 0.6875, learning_rate 0.000605203\n","2023-06-13T16:23:42.656828: step 4215, loss 0.51132, acc 0.8125, learning_rate 0.000604958\n","2023-06-13T16:23:44.055252: step 4216, loss 0.343511, acc 0.90625, learning_rate 0.000604714\n","2023-06-13T16:23:45.439498: step 4217, loss 0.657446, acc 0.8125, learning_rate 0.000604469\n","2023-06-13T16:23:46.861702: step 4218, loss 0.420658, acc 0.90625, learning_rate 0.000604224\n","2023-06-13T16:23:48.314645: step 4219, loss 0.646197, acc 0.78125, learning_rate 0.00060398\n","2023-06-13T16:23:49.725807: step 4220, loss 0.767528, acc 0.75, learning_rate 0.000603735\n","2023-06-13T16:23:52.022976: step 4221, loss 0.640099, acc 0.8125, learning_rate 0.000603491\n","2023-06-13T16:23:54.421840: step 4222, loss 0.573733, acc 0.875, learning_rate 0.000603247\n","2023-06-13T16:23:56.854710: step 4223, loss 0.458363, acc 0.875, learning_rate 0.000603003\n","2023-06-13T16:23:59.275023: step 4224, loss 0.490693, acc 0.84375, learning_rate 0.000602759\n","2023-06-13T16:24:01.021152: step 4225, loss 0.636293, acc 0.75, learning_rate 0.000602515\n","2023-06-13T16:24:02.451739: step 4226, loss 0.475368, acc 0.78125, learning_rate 0.000602272\n","2023-06-13T16:24:03.902971: step 4227, loss 0.711857, acc 0.75, learning_rate 0.000602028\n","2023-06-13T16:24:05.324428: step 4228, loss 0.736197, acc 0.78125, learning_rate 0.000601785\n","2023-06-13T16:24:06.766130: step 4229, loss 0.702058, acc 0.78125, learning_rate 0.000601541\n","2023-06-13T16:24:08.239112: step 4230, loss 0.64024, acc 0.8125, learning_rate 0.000601298\n","2023-06-13T16:24:09.650015: step 4231, loss 0.742651, acc 0.75, learning_rate 0.000601055\n","2023-06-13T16:24:11.865591: step 4232, loss 0.603317, acc 0.84375, learning_rate 0.000600812\n","2023-06-13T16:24:14.303104: step 4233, loss 1.01605, acc 0.75, learning_rate 0.000600569\n","2023-06-13T16:24:16.579338: step 4234, loss 0.621682, acc 0.78125, learning_rate 0.000600327\n","2023-06-13T16:24:18.873099: step 4235, loss 0.577985, acc 0.78125, learning_rate 0.000600084\n","2023-06-13T16:24:20.885494: step 4236, loss 0.966317, acc 0.75, learning_rate 0.000599842\n","2023-06-13T16:24:22.314724: step 4237, loss 0.607642, acc 0.8125, learning_rate 0.000599599\n","2023-06-13T16:24:23.703170: step 4238, loss 1.01927, acc 0.6875, learning_rate 0.000599357\n","2023-06-13T16:24:25.120766: step 4239, loss 0.899961, acc 0.71875, learning_rate 0.000599115\n","2023-06-13T16:24:26.553073: step 4240, loss 0.536169, acc 0.8125, learning_rate 0.000598873\n","2023-06-13T16:24:27.968948: step 4241, loss 0.377499, acc 0.875, learning_rate 0.000598631\n","2023-06-13T16:24:29.385402: step 4242, loss 0.991525, acc 0.65625, learning_rate 0.000598389\n","2023-06-13T16:24:31.093309: step 4243, loss 0.721985, acc 0.78125, learning_rate 0.000598147\n","2023-06-13T16:24:33.651868: step 4244, loss 0.668036, acc 0.71875, learning_rate 0.000597906\n","2023-06-13T16:24:36.131113: step 4245, loss 0.579689, acc 0.8125, learning_rate 0.000597664\n","2023-06-13T16:24:38.555624: step 4246, loss 0.418792, acc 0.84375, learning_rate 0.000597423\n","2023-06-13T16:24:40.563536: step 4247, loss 0.65684, acc 0.6875, learning_rate 0.000597182\n","2023-06-13T16:24:42.009530: step 4248, loss 0.699548, acc 0.78125, learning_rate 0.000596941\n","2023-06-13T16:24:43.414905: step 4249, loss 0.637228, acc 0.75, learning_rate 0.0005967\n","2023-06-13T16:24:44.852492: step 4250, loss 0.560415, acc 0.875, learning_rate 0.000596459\n","2023-06-13T16:24:46.275816: step 4251, loss 0.500081, acc 0.875, learning_rate 0.000596218\n","2023-06-13T16:24:47.719357: step 4252, loss 0.460055, acc 0.75, learning_rate 0.000595978\n","2023-06-13T16:24:49.140625: step 4253, loss 0.82231, acc 0.71875, learning_rate 0.000595737\n","2023-06-13T16:24:50.832840: step 4254, loss 0.681936, acc 0.8125, learning_rate 0.000595497\n","2023-06-13T16:24:53.295183: step 4255, loss 0.689808, acc 0.71875, learning_rate 0.000595257\n","2023-06-13T16:24:55.665555: step 4256, loss 0.641834, acc 0.78125, learning_rate 0.000595017\n","2023-06-13T16:24:58.080980: step 4257, loss 0.678788, acc 0.8125, learning_rate 0.000594777\n","2023-06-13T16:25:00.250729: step 4258, loss 0.792352, acc 0.71875, learning_rate 0.000594537\n","2023-06-13T16:25:01.764545: step 4259, loss 0.754623, acc 0.71875, learning_rate 0.000594297\n","2023-06-13T16:25:03.224595: step 4260, loss 0.537452, acc 0.78125, learning_rate 0.000594057\n","2023-06-13T16:25:04.673478: step 4261, loss 0.405122, acc 0.875, learning_rate 0.000593818\n","2023-06-13T16:25:06.128705: step 4262, loss 0.663513, acc 0.71875, learning_rate 0.000593578\n","2023-06-13T16:25:07.584977: step 4263, loss 1.01429, acc 0.65625, learning_rate 0.000593339\n","2023-06-13T16:25:09.115647: step 4264, loss 0.553983, acc 0.84375, learning_rate 0.0005931\n","2023-06-13T16:25:11.008887: step 4265, loss 0.933693, acc 0.6875, learning_rate 0.000592861\n","2023-06-13T16:25:13.643675: step 4266, loss 0.313309, acc 0.9375, learning_rate 0.000592622\n","2023-06-13T16:25:15.982502: step 4267, loss 0.459208, acc 0.8125, learning_rate 0.000592383\n","2023-06-13T16:25:18.317896: step 4268, loss 1.04269, acc 0.53125, learning_rate 0.000592144\n","2023-06-13T16:25:20.544008: step 4269, loss 0.912118, acc 0.71875, learning_rate 0.000591905\n","2023-06-13T16:25:21.984397: step 4270, loss 0.632307, acc 0.75, learning_rate 0.000591667\n","2023-06-13T16:25:23.397557: step 4271, loss 0.450416, acc 0.90625, learning_rate 0.000591428\n","2023-06-13T16:25:24.797458: step 4272, loss 0.505104, acc 0.84375, learning_rate 0.00059119\n","2023-06-13T16:25:26.223427: step 4273, loss 0.694561, acc 0.78125, learning_rate 0.000590952\n","2023-06-13T16:25:27.631180: step 4274, loss 0.969342, acc 0.6875, learning_rate 0.000590714\n","2023-06-13T16:25:29.048301: step 4275, loss 0.485782, acc 0.875, learning_rate 0.000590476\n","2023-06-13T16:25:30.629660: step 4276, loss 0.557024, acc 0.84375, learning_rate 0.000590238\n","2023-06-13T16:25:33.224929: step 4277, loss 0.994669, acc 0.71875, learning_rate 0.00059\n","2023-06-13T16:25:35.587693: step 4278, loss 0.665833, acc 0.78125, learning_rate 0.000589763\n","2023-06-13T16:25:37.855636: step 4279, loss 0.379421, acc 0.90625, learning_rate 0.000589525\n","2023-06-13T16:25:40.270935: step 4280, loss 0.612649, acc 0.75, learning_rate 0.000589288\n","2023-06-13T16:25:41.715839: step 4281, loss 0.468746, acc 0.8125, learning_rate 0.000589051\n","2023-06-13T16:25:43.145963: step 4282, loss 0.968929, acc 0.71875, learning_rate 0.000588814\n","2023-06-13T16:25:44.552392: step 4283, loss 0.652695, acc 0.78125, learning_rate 0.000588577\n","2023-06-13T16:25:45.959590: step 4284, loss 0.331828, acc 0.90625, learning_rate 0.00058834\n","2023-06-13T16:25:47.386458: step 4285, loss 0.780189, acc 0.71875, learning_rate 0.000588103\n","2023-06-13T16:25:48.762960: step 4286, loss 0.480158, acc 0.8125, learning_rate 0.000587866\n","2023-06-13T16:25:50.157120: step 4287, loss 0.727589, acc 0.75, learning_rate 0.00058763\n","2023-06-13T16:25:52.635467: step 4288, loss 0.833685, acc 0.65625, learning_rate 0.000587393\n","2023-06-13T16:25:55.045343: step 4289, loss 0.762478, acc 0.8125, learning_rate 0.000587157\n","2023-06-13T16:25:57.423931: step 4290, loss 0.629188, acc 0.8125, learning_rate 0.000586921\n","2023-06-13T16:25:59.689749: step 4291, loss 0.970916, acc 0.71875, learning_rate 0.000586685\n","2023-06-13T16:26:01.294025: step 4292, loss 0.418644, acc 0.9375, learning_rate 0.000586449\n","2023-06-13T16:26:02.698284: step 4293, loss 0.714983, acc 0.78125, learning_rate 0.000586213\n","2023-06-13T16:26:04.099100: step 4294, loss 0.673368, acc 0.8125, learning_rate 0.000585977\n","2023-06-13T16:26:05.530557: step 4295, loss 0.387566, acc 0.90625, learning_rate 0.000585741\n","2023-06-13T16:26:06.924493: step 4296, loss 0.825025, acc 0.71875, learning_rate 0.000585506\n","2023-06-13T16:26:08.349225: step 4297, loss 0.622618, acc 0.78125, learning_rate 0.00058527\n","2023-06-13T16:26:09.798199: step 4298, loss 0.692758, acc 0.8125, learning_rate 0.000585035\n","2023-06-13T16:26:11.970897: step 4299, loss 0.590074, acc 0.8125, learning_rate 0.0005848\n","\n","Evaluation:\n","2023-06-13T16:26:43.040061: step 4300, loss 2.45351, acc 0.384814\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4300\n","\n","2023-06-13T16:26:44.616096: step 4300, loss 1.00845, acc 0.65625, learning_rate 0.000584565\n","2023-06-13T16:26:46.040249: step 4301, loss 0.613714, acc 0.8125, learning_rate 0.00058433\n","2023-06-13T16:26:47.456945: step 4302, loss 0.5573, acc 0.8125, learning_rate 0.000584095\n","2023-06-13T16:26:48.882070: step 4303, loss 0.944163, acc 0.6875, learning_rate 0.00058386\n","2023-06-13T16:26:50.342876: step 4304, loss 0.539168, acc 0.78125, learning_rate 0.000583626\n","2023-06-13T16:26:51.908377: step 4305, loss 0.487369, acc 0.84375, learning_rate 0.000583391\n","2023-06-13T16:26:54.441955: step 4306, loss 0.378388, acc 0.875, learning_rate 0.000583157\n","2023-06-13T16:26:56.760879: step 4307, loss 0.721749, acc 0.84375, learning_rate 0.000582923\n","2023-06-13T16:26:59.088252: step 4308, loss 0.533086, acc 0.875, learning_rate 0.000582688\n","2023-06-13T16:27:01.332244: step 4309, loss 1.03506, acc 0.65625, learning_rate 0.000582454\n","2023-06-13T16:27:02.954843: step 4310, loss 0.873664, acc 0.75, learning_rate 0.00058222\n","2023-06-13T16:27:04.411404: step 4311, loss 0.639169, acc 0.75, learning_rate 0.000581987\n","2023-06-13T16:27:05.803423: step 4312, loss 0.439673, acc 0.875, learning_rate 0.000581753\n","2023-06-13T16:27:07.223729: step 4313, loss 0.806764, acc 0.71875, learning_rate 0.000581519\n","2023-06-13T16:27:08.596199: step 4314, loss 0.766576, acc 0.78125, learning_rate 0.000581286\n","2023-06-13T16:27:09.997261: step 4315, loss 0.557056, acc 0.84375, learning_rate 0.000581053\n","2023-06-13T16:27:11.417320: step 4316, loss 0.870696, acc 0.6875, learning_rate 0.000580819\n","2023-06-13T16:27:13.575213: step 4317, loss 0.538864, acc 0.78125, learning_rate 0.000580586\n","2023-06-13T16:27:16.031337: step 4318, loss 0.632443, acc 0.8125, learning_rate 0.000580353\n","2023-06-13T16:27:18.371395: step 4319, loss 0.486906, acc 0.78125, learning_rate 0.00058012\n","2023-06-13T16:27:20.708642: step 4320, loss 0.44387, acc 0.875, learning_rate 0.000579887\n","2023-06-13T16:27:22.650317: step 4321, loss 0.472386, acc 0.84375, learning_rate 0.000579655\n","2023-06-13T16:27:24.024402: step 4322, loss 0.656741, acc 0.75, learning_rate 0.000579422\n","2023-06-13T16:27:25.476327: step 4323, loss 0.406586, acc 0.8125, learning_rate 0.00057919\n","2023-06-13T16:27:26.892502: step 4324, loss 0.521244, acc 0.90625, learning_rate 0.000578957\n","2023-06-13T16:27:28.299941: step 4325, loss 0.451704, acc 0.8125, learning_rate 0.000578725\n","2023-06-13T16:27:29.735629: step 4326, loss 0.757508, acc 0.71875, learning_rate 0.000578493\n","2023-06-13T16:27:31.152389: step 4327, loss 0.321183, acc 0.90625, learning_rate 0.000578261\n","2023-06-13T16:27:32.883156: step 4328, loss 0.55238, acc 0.8125, learning_rate 0.000578029\n","2023-06-13T16:27:35.433902: step 4329, loss 0.661924, acc 0.84375, learning_rate 0.000577797\n","2023-06-13T16:27:37.740061: step 4330, loss 0.605981, acc 0.71875, learning_rate 0.000577566\n","2023-06-13T16:27:39.963672: step 4331, loss 0.482583, acc 0.84375, learning_rate 0.000577334\n","2023-06-13T16:27:42.263428: step 4332, loss 0.612599, acc 0.8125, learning_rate 0.000577103\n","2023-06-13T16:27:43.713863: step 4333, loss 0.674347, acc 0.6875, learning_rate 0.000576871\n","2023-06-13T16:27:45.130420: step 4334, loss 0.649186, acc 0.75, learning_rate 0.00057664\n","2023-06-13T16:27:46.529322: step 4335, loss 0.702261, acc 0.8125, learning_rate 0.000576409\n","2023-06-13T16:27:47.955804: step 4336, loss 0.532386, acc 0.78125, learning_rate 0.000576178\n","2023-06-13T16:27:49.394805: step 4337, loss 0.443192, acc 0.8125, learning_rate 0.000575947\n","2023-06-13T16:27:50.835054: step 4338, loss 0.538204, acc 0.8125, learning_rate 0.000575716\n","2023-06-13T16:27:52.346732: step 4339, loss 0.512691, acc 0.84375, learning_rate 0.000575486\n","2023-06-13T16:27:54.917293: step 4340, loss 0.735974, acc 0.75, learning_rate 0.000575255\n","2023-06-13T16:27:57.406642: step 4341, loss 0.397796, acc 0.9375, learning_rate 0.000575025\n","2023-06-13T16:27:59.673263: step 4342, loss 1.0372, acc 0.53125, learning_rate 0.000574794\n","2023-06-13T16:28:02.053276: step 4343, loss 0.578995, acc 0.75, learning_rate 0.000574564\n","2023-06-13T16:28:03.456832: step 4344, loss 0.785504, acc 0.78125, learning_rate 0.000574334\n","2023-06-13T16:28:04.883819: step 4345, loss 0.682731, acc 0.75, learning_rate 0.000574104\n","2023-06-13T16:28:06.298093: step 4346, loss 0.871575, acc 0.6875, learning_rate 0.000573874\n","2023-06-13T16:28:07.738529: step 4347, loss 0.715376, acc 0.78125, learning_rate 0.000573644\n","2023-06-13T16:28:09.179426: step 4348, loss 0.44079, acc 0.875, learning_rate 0.000573415\n","2023-06-13T16:28:10.612645: step 4349, loss 0.493481, acc 0.8125, learning_rate 0.000573185\n","2023-06-13T16:28:12.106480: step 4350, loss 0.497192, acc 0.78125, learning_rate 0.000572956\n","2023-06-13T16:28:14.589613: step 4351, loss 0.577882, acc 0.875, learning_rate 0.000572726\n","2023-06-13T16:28:16.930003: step 4352, loss 0.657774, acc 0.65625, learning_rate 0.000572497\n","2023-06-13T16:28:19.135101: step 4353, loss 0.755569, acc 0.75, learning_rate 0.000572268\n","2023-06-13T16:28:21.528032: step 4354, loss 0.471469, acc 0.875, learning_rate 0.000572039\n","2023-06-13T16:28:23.207085: step 4355, loss 0.892996, acc 0.59375, learning_rate 0.00057181\n","2023-06-13T16:28:24.618899: step 4356, loss 0.669801, acc 0.8125, learning_rate 0.000571581\n","2023-06-13T16:28:26.076347: step 4357, loss 0.900626, acc 0.75, learning_rate 0.000571353\n","2023-06-13T16:28:27.516981: step 4358, loss 0.556965, acc 0.8125, learning_rate 0.000571124\n","2023-06-13T16:28:28.933577: step 4359, loss 0.765863, acc 0.78125, learning_rate 0.000570896\n","2023-06-13T16:28:30.330979: step 4360, loss 0.578796, acc 0.8125, learning_rate 0.000570667\n","2023-06-13T16:28:31.732676: step 4361, loss 0.708551, acc 0.875, learning_rate 0.000570439\n","2023-06-13T16:28:33.722052: step 4362, loss 0.652675, acc 0.71875, learning_rate 0.000570211\n","2023-06-13T16:28:36.243395: step 4363, loss 0.823645, acc 0.65625, learning_rate 0.000569983\n","2023-06-13T16:28:38.623284: step 4364, loss 0.712213, acc 0.75, learning_rate 0.000569755\n","2023-06-13T16:28:40.918201: step 4365, loss 0.417187, acc 0.875, learning_rate 0.000569527\n","2023-06-13T16:28:42.952824: step 4366, loss 0.884935, acc 0.71875, learning_rate 0.0005693\n","2023-06-13T16:28:44.368460: step 4367, loss 0.921346, acc 0.59375, learning_rate 0.000569072\n","2023-06-13T16:28:45.796290: step 4368, loss 0.788568, acc 0.78125, learning_rate 0.000568845\n","2023-06-13T16:28:47.196443: step 4369, loss 0.577541, acc 0.875, learning_rate 0.000568617\n","2023-06-13T16:28:48.630241: step 4370, loss 0.693261, acc 0.71875, learning_rate 0.00056839\n","2023-06-13T16:28:50.050579: step 4371, loss 0.561057, acc 0.78125, learning_rate 0.000568163\n","2023-06-13T16:28:51.451091: step 4372, loss 0.871242, acc 0.6875, learning_rate 0.000567936\n","2023-06-13T16:28:53.147342: step 4373, loss 1.145, acc 0.6875, learning_rate 0.000567709\n","2023-06-13T16:28:55.700981: step 4374, loss 0.748018, acc 0.71875, learning_rate 0.000567482\n","2023-06-13T16:28:58.044076: step 4375, loss 1.17408, acc 0.5625, learning_rate 0.000567256\n","2023-06-13T16:29:00.382293: step 4376, loss 0.767444, acc 0.75, learning_rate 0.000567029\n","2023-06-13T16:29:02.630364: step 4377, loss 0.9086, acc 0.78125, learning_rate 0.000566803\n","2023-06-13T16:29:04.034232: step 4378, loss 0.730678, acc 0.78125, learning_rate 0.000566576\n","2023-06-13T16:29:05.465069: step 4379, loss 0.875015, acc 0.75, learning_rate 0.00056635\n","2023-06-13T16:29:06.891498: step 4380, loss 0.91299, acc 0.6875, learning_rate 0.000566124\n","2023-06-13T16:29:08.346422: step 4381, loss 0.506624, acc 0.84375, learning_rate 0.000565898\n","2023-06-13T16:29:09.774490: step 4382, loss 0.493006, acc 0.84375, learning_rate 0.000565672\n","2023-06-13T16:29:11.245685: step 4383, loss 0.698225, acc 0.78125, learning_rate 0.000565446\n","2023-06-13T16:29:13.723768: step 4384, loss 1.12257, acc 0.78125, learning_rate 0.000565221\n","2023-06-13T16:29:16.348857: step 4385, loss 1.01305, acc 0.5625, learning_rate 0.000564995\n","2023-06-13T16:29:19.065829: step 4386, loss 0.561008, acc 0.78125, learning_rate 0.00056477\n","2023-06-13T16:29:21.683995: step 4387, loss 0.803365, acc 0.78125, learning_rate 0.000564544\n","2023-06-13T16:29:24.255701: step 4388, loss 0.559112, acc 0.78125, learning_rate 0.000564319\n","2023-06-13T16:29:26.792468: step 4389, loss 0.880478, acc 0.8125, learning_rate 0.000564094\n","2023-06-13T16:29:29.265587: step 4390, loss 0.647388, acc 0.90625, learning_rate 0.000563869\n","2023-06-13T16:29:30.890253: step 4391, loss 0.64453, acc 0.75, learning_rate 0.000563644\n","2023-06-13T16:29:32.330885: step 4392, loss 0.852279, acc 0.6875, learning_rate 0.000563419\n","2023-06-13T16:29:33.746687: step 4393, loss 0.520882, acc 0.8125, learning_rate 0.000563194\n","2023-06-13T16:29:35.156810: step 4394, loss 0.673574, acc 0.875, learning_rate 0.00056297\n","2023-06-13T16:29:36.567249: step 4395, loss 0.751576, acc 0.8125, learning_rate 0.000562745\n","2023-06-13T16:29:37.970834: step 4396, loss 0.980272, acc 0.65625, learning_rate 0.000562521\n","2023-06-13T16:29:39.392764: step 4397, loss 0.777518, acc 0.78125, learning_rate 0.000562297\n","2023-06-13T16:29:41.559201: step 4398, loss 1.13638, acc 0.75, learning_rate 0.000562073\n","2023-06-13T16:29:44.029042: step 4399, loss 0.823032, acc 0.625, learning_rate 0.000561848\n","\n","Evaluation:\n","2023-06-13T16:30:13.758057: step 4400, loss 2.47726, acc 0.389512\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4400\n","\n","2023-06-13T16:30:15.328947: step 4400, loss 0.596807, acc 0.78125, learning_rate 0.000561625\n","2023-06-13T16:30:16.780038: step 4401, loss 0.825334, acc 0.65625, learning_rate 0.000561401\n","2023-06-13T16:30:18.190697: step 4402, loss 0.987317, acc 0.6875, learning_rate 0.000561177\n","2023-06-13T16:30:19.645182: step 4403, loss 0.561043, acc 0.78125, learning_rate 0.000560953\n","2023-06-13T16:30:21.045819: step 4404, loss 0.84378, acc 0.6875, learning_rate 0.00056073\n","2023-06-13T16:30:23.104175: step 4405, loss 0.507622, acc 0.84375, learning_rate 0.000560506\n","2023-06-13T16:30:25.568227: step 4406, loss 0.964988, acc 0.71875, learning_rate 0.000560283\n","2023-06-13T16:30:28.018690: step 4407, loss 0.415684, acc 0.90625, learning_rate 0.00056006\n","2023-06-13T16:30:30.341931: step 4408, loss 0.553861, acc 0.84375, learning_rate 0.000559837\n","2023-06-13T16:30:32.274383: step 4409, loss 0.713467, acc 0.78125, learning_rate 0.000559614\n","2023-06-13T16:30:33.679474: step 4410, loss 0.670873, acc 0.75, learning_rate 0.000559391\n","2023-06-13T16:30:35.091118: step 4411, loss 0.607442, acc 0.875, learning_rate 0.000559168\n","2023-06-13T16:30:36.500317: step 4412, loss 0.735157, acc 0.8125, learning_rate 0.000558946\n","2023-06-13T16:30:37.926077: step 4413, loss 0.350297, acc 0.90625, learning_rate 0.000558723\n","2023-06-13T16:30:39.360737: step 4414, loss 0.428841, acc 0.90625, learning_rate 0.000558501\n","2023-06-13T16:30:40.808844: step 4415, loss 0.74363, acc 0.75, learning_rate 0.000558278\n","2023-06-13T16:30:42.786323: step 4416, loss 0.393538, acc 0.90625, learning_rate 0.000558056\n","2023-06-13T16:30:45.310155: step 4417, loss 0.436556, acc 0.90625, learning_rate 0.000557834\n","2023-06-13T16:30:47.697646: step 4418, loss 0.533015, acc 0.875, learning_rate 0.000557612\n","2023-06-13T16:30:50.027547: step 4419, loss 0.50518, acc 0.84375, learning_rate 0.00055739\n","2023-06-13T16:30:52.135381: step 4420, loss 0.799203, acc 0.75, learning_rate 0.000557168\n","2023-06-13T16:30:53.541614: step 4421, loss 0.631826, acc 0.8125, learning_rate 0.000556947\n","2023-06-13T16:30:54.988395: step 4422, loss 0.578204, acc 0.8125, learning_rate 0.000556725\n","2023-06-13T16:30:56.400024: step 4423, loss 0.662881, acc 0.78125, learning_rate 0.000556504\n","2023-06-13T16:30:57.832784: step 4424, loss 0.82368, acc 0.71875, learning_rate 0.000556282\n","2023-06-13T16:30:59.227553: step 4425, loss 1.11248, acc 0.71875, learning_rate 0.000556061\n","2023-06-13T16:31:00.644627: step 4426, loss 0.546227, acc 0.84375, learning_rate 0.00055584\n","2023-06-13T16:31:02.134163: step 4427, loss 0.534971, acc 0.78125, learning_rate 0.000555619\n","2023-06-13T16:31:04.666196: step 4428, loss 0.883173, acc 0.65625, learning_rate 0.000555398\n","2023-06-13T16:31:07.118889: step 4429, loss 0.564723, acc 0.8125, learning_rate 0.000555177\n","2023-06-13T16:31:09.540261: step 4430, loss 1.00342, acc 0.71875, learning_rate 0.000554957\n","2023-06-13T16:31:11.815985: step 4431, loss 0.557365, acc 0.75, learning_rate 0.000554736\n","2023-06-13T16:31:13.227336: step 4432, loss 0.78272, acc 0.71875, learning_rate 0.000554515\n","2023-06-13T16:31:14.616259: step 4433, loss 0.544428, acc 0.84375, learning_rate 0.000554295\n","2023-06-13T16:31:16.038437: step 4434, loss 0.807214, acc 0.78125, learning_rate 0.000554075\n","2023-06-13T16:31:17.459107: step 4435, loss 0.69319, acc 0.78125, learning_rate 0.000553855\n","2023-06-13T16:31:18.888547: step 4436, loss 0.653001, acc 0.6875, learning_rate 0.000553635\n","2023-06-13T16:31:20.291101: step 4437, loss 1.05817, acc 0.65625, learning_rate 0.000553415\n","2023-06-13T16:31:21.734653: step 4438, loss 0.729179, acc 0.8125, learning_rate 0.000553195\n","2023-06-13T16:31:24.086874: step 4439, loss 0.831009, acc 0.71875, learning_rate 0.000552975\n","2023-06-13T16:31:26.497216: step 4440, loss 0.628065, acc 0.75, learning_rate 0.000552755\n","2023-06-13T16:31:28.788693: step 4441, loss 0.918121, acc 0.78125, learning_rate 0.000552536\n","2023-06-13T16:31:31.171170: step 4442, loss 0.834755, acc 0.6875, learning_rate 0.000552316\n","2023-06-13T16:31:32.879089: step 4443, loss 0.704454, acc 0.78125, learning_rate 0.000552097\n","2023-06-13T16:31:34.315150: step 4444, loss 0.52823, acc 0.84375, learning_rate 0.000551878\n","2023-06-13T16:31:35.732375: step 4445, loss 0.937565, acc 0.75, learning_rate 0.000551659\n","2023-06-13T16:31:37.168488: step 4446, loss 0.544458, acc 0.8125, learning_rate 0.00055144\n","2023-06-13T16:31:38.581529: step 4447, loss 0.761621, acc 0.75, learning_rate 0.000551221\n","2023-06-13T16:31:39.997499: step 4448, loss 0.938009, acc 0.6875, learning_rate 0.000551002\n","2023-06-13T16:31:41.411740: step 4449, loss 0.649576, acc 0.8125, learning_rate 0.000550783\n","2023-06-13T16:31:43.382265: step 4450, loss 0.573286, acc 0.84375, learning_rate 0.000550565\n","2023-06-13T16:31:45.946881: step 4451, loss 0.587467, acc 0.84375, learning_rate 0.000550346\n","2023-06-13T16:31:48.212939: step 4452, loss 0.750697, acc 0.78125, learning_rate 0.000550128\n","2023-06-13T16:31:50.628646: step 4453, loss 0.638676, acc 0.8125, learning_rate 0.00054991\n","2023-06-13T16:31:52.662124: step 4454, loss 0.407139, acc 0.84375, learning_rate 0.000549692\n","2023-06-13T16:31:54.089086: step 4455, loss 0.781125, acc 0.75, learning_rate 0.000549474\n","2023-06-13T16:31:55.507742: step 4456, loss 0.596613, acc 0.8125, learning_rate 0.000549256\n","2023-06-13T16:31:56.908159: step 4457, loss 0.531336, acc 0.8125, learning_rate 0.000549038\n","2023-06-13T16:31:58.328751: step 4458, loss 0.902844, acc 0.71875, learning_rate 0.00054882\n","2023-06-13T16:31:59.747672: step 4459, loss 0.593407, acc 0.78125, learning_rate 0.000548602\n","2023-06-13T16:32:01.176619: step 4460, loss 0.854093, acc 0.65625, learning_rate 0.000548385\n","2023-06-13T16:32:02.902439: step 4461, loss 0.757696, acc 0.75, learning_rate 0.000548168\n","2023-06-13T16:32:05.443069: step 4462, loss 0.449619, acc 0.875, learning_rate 0.00054795\n","2023-06-13T16:32:07.840078: step 4463, loss 0.535441, acc 0.75, learning_rate 0.000547733\n","2023-06-13T16:32:10.119227: step 4464, loss 0.829549, acc 0.75, learning_rate 0.000547516\n","2023-06-13T16:32:12.392975: step 4465, loss 0.660213, acc 0.71875, learning_rate 0.000547299\n","2023-06-13T16:32:13.812953: step 4466, loss 0.548313, acc 0.875, learning_rate 0.000547082\n","2023-06-13T16:32:15.256313: step 4467, loss 0.877671, acc 0.8125, learning_rate 0.000546865\n","2023-06-13T16:32:16.672491: step 4468, loss 0.634126, acc 0.75, learning_rate 0.000546649\n","2023-06-13T16:32:18.061607: step 4469, loss 0.68205, acc 0.75, learning_rate 0.000546432\n","2023-06-13T16:32:19.464998: step 4470, loss 0.483379, acc 0.875, learning_rate 0.000546216\n","2023-06-13T16:32:20.914092: step 4471, loss 0.941858, acc 0.6875, learning_rate 0.000545999\n","2023-06-13T16:32:22.356860: step 4472, loss 0.430828, acc 0.90625, learning_rate 0.000545783\n","2023-06-13T16:32:24.883074: step 4473, loss 0.578283, acc 0.875, learning_rate 0.000545567\n","2023-06-13T16:32:27.302656: step 4474, loss 0.487044, acc 0.90625, learning_rate 0.000545351\n","2023-06-13T16:32:29.624251: step 4475, loss 0.637501, acc 0.84375, learning_rate 0.000545135\n","2023-06-13T16:32:31.898840: step 4476, loss 0.779845, acc 0.6875, learning_rate 0.000544919\n","2023-06-13T16:32:33.554567: step 4477, loss 0.757637, acc 0.75, learning_rate 0.000544703\n","2023-06-13T16:32:34.978397: step 4478, loss 0.400243, acc 0.84375, learning_rate 0.000544488\n","2023-06-13T16:32:36.397685: step 4479, loss 0.656407, acc 0.71875, learning_rate 0.000544272\n","2023-06-13T16:32:37.803249: step 4480, loss 0.530323, acc 0.8125, learning_rate 0.000544057\n","2023-06-13T16:32:39.240926: step 4481, loss 1.09522, acc 0.71875, learning_rate 0.000543841\n","2023-06-13T16:32:40.682134: step 4482, loss 0.783578, acc 0.6875, learning_rate 0.000543626\n","2023-06-13T16:32:42.120923: step 4483, loss 0.533194, acc 0.8125, learning_rate 0.000543411\n","2023-06-13T16:32:44.195966: step 4484, loss 0.648472, acc 0.84375, learning_rate 0.000543196\n","2023-06-13T16:32:46.691181: step 4485, loss 0.718974, acc 0.6875, learning_rate 0.000542981\n","2023-06-13T16:32:49.016046: step 4486, loss 0.78571, acc 0.8125, learning_rate 0.000542766\n","2023-06-13T16:32:51.311686: step 4487, loss 0.625687, acc 0.75, learning_rate 0.000542552\n","2023-06-13T16:32:53.280211: step 4488, loss 0.637548, acc 0.84375, learning_rate 0.000542337\n","2023-06-13T16:32:54.692924: step 4489, loss 0.758155, acc 0.78125, learning_rate 0.000542123\n","2023-06-13T16:32:56.097671: step 4490, loss 0.660274, acc 0.78125, learning_rate 0.000541908\n","2023-06-13T16:32:57.570105: step 4491, loss 0.550509, acc 0.8125, learning_rate 0.000541694\n","2023-06-13T16:32:58.991741: step 4492, loss 0.969894, acc 0.59375, learning_rate 0.00054148\n","2023-06-13T16:33:00.402641: step 4493, loss 0.657749, acc 0.78125, learning_rate 0.000541266\n","2023-06-13T16:33:01.805345: step 4494, loss 0.945666, acc 0.5625, learning_rate 0.000541052\n","2023-06-13T16:33:03.377610: step 4495, loss 0.624296, acc 0.84375, learning_rate 0.000540838\n","2023-06-13T16:33:05.913773: step 4496, loss 0.522136, acc 0.8125, learning_rate 0.000540624\n","2023-06-13T16:33:08.355060: step 4497, loss 0.457773, acc 0.84375, learning_rate 0.000540411\n","2023-06-13T16:33:10.645519: step 4498, loss 0.91882, acc 0.65625, learning_rate 0.000540197\n","2023-06-13T16:33:13.002350: step 4499, loss 0.596502, acc 0.78125, learning_rate 0.000539984\n","\n","Evaluation:\n","2023-06-13T16:33:40.231978: step 4500, loss 2.47518, acc 0.386935\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4500\n","\n","2023-06-13T16:33:41.819119: step 4500, loss 0.913137, acc 0.59375, learning_rate 0.00053977\n","2023-06-13T16:33:43.221280: step 4501, loss 0.415664, acc 0.875, learning_rate 0.000539557\n","2023-06-13T16:33:45.096135: step 4502, loss 0.867478, acc 0.71875, learning_rate 0.000539344\n","2023-06-13T16:33:47.645949: step 4503, loss 0.834979, acc 0.71875, learning_rate 0.000539131\n","2023-06-13T16:33:50.065902: step 4504, loss 0.562065, acc 0.8125, learning_rate 0.000538918\n","2023-06-13T16:33:52.514530: step 4505, loss 0.60987, acc 0.75, learning_rate 0.000538705\n","2023-06-13T16:33:54.493213: step 4506, loss 0.410753, acc 0.90625, learning_rate 0.000538492\n","2023-06-13T16:33:55.904115: step 4507, loss 0.912588, acc 0.6875, learning_rate 0.00053828\n","2023-06-13T16:33:57.323054: step 4508, loss 0.391883, acc 0.84375, learning_rate 0.000538067\n","2023-06-13T16:33:58.739174: step 4509, loss 0.634753, acc 0.75, learning_rate 0.000537855\n","2023-06-13T16:34:00.141360: step 4510, loss 0.816695, acc 0.78125, learning_rate 0.000537642\n","2023-06-13T16:34:01.540411: step 4511, loss 0.822042, acc 0.84375, learning_rate 0.00053743\n","2023-06-13T16:34:02.968822: step 4512, loss 0.40446, acc 0.8125, learning_rate 0.000537218\n","2023-06-13T16:34:04.516262: step 4513, loss 0.713548, acc 0.8125, learning_rate 0.000537006\n","2023-06-13T16:34:07.207232: step 4514, loss 0.751254, acc 0.78125, learning_rate 0.000536794\n","2023-06-13T16:34:09.538902: step 4515, loss 0.90365, acc 0.6875, learning_rate 0.000536583\n","2023-06-13T16:34:11.899640: step 4516, loss 0.898647, acc 0.71875, learning_rate 0.000536371\n","2023-06-13T16:34:14.158032: step 4517, loss 0.699137, acc 0.78125, learning_rate 0.000536159\n","2023-06-13T16:34:15.618442: step 4518, loss 0.804847, acc 0.75, learning_rate 0.000535948\n","2023-06-13T16:34:17.046228: step 4519, loss 0.846755, acc 0.75, learning_rate 0.000535736\n","2023-06-13T16:34:18.449730: step 4520, loss 0.538178, acc 0.84375, learning_rate 0.000535525\n","2023-06-13T16:34:19.869816: step 4521, loss 0.580869, acc 0.75, learning_rate 0.000535314\n","2023-06-13T16:34:21.286855: step 4522, loss 0.62438, acc 0.8125, learning_rate 0.000535103\n","2023-06-13T16:34:22.742211: step 4523, loss 0.753515, acc 0.78125, learning_rate 0.000534892\n","2023-06-13T16:34:24.141971: step 4524, loss 0.678051, acc 0.71875, learning_rate 0.000534681\n","2023-06-13T16:34:26.299768: step 4525, loss 0.576458, acc 0.8125, learning_rate 0.00053447\n","2023-06-13T16:34:28.769680: step 4526, loss 0.512256, acc 0.84375, learning_rate 0.00053426\n","2023-06-13T16:34:31.006039: step 4527, loss 0.724826, acc 0.75, learning_rate 0.000534049\n","2023-06-13T16:34:33.307529: step 4528, loss 0.999572, acc 0.65625, learning_rate 0.000533839\n","2023-06-13T16:34:35.283210: step 4529, loss 0.572473, acc 0.8125, learning_rate 0.000533628\n","2023-06-13T16:34:36.685420: step 4530, loss 0.35116, acc 0.9375, learning_rate 0.000533418\n","2023-06-13T16:34:38.099446: step 4531, loss 0.755523, acc 0.78125, learning_rate 0.000533208\n","2023-06-13T16:34:39.528512: step 4532, loss 0.751021, acc 0.75, learning_rate 0.000532998\n","2023-06-13T16:34:40.917183: step 4533, loss 0.476821, acc 0.875, learning_rate 0.000532788\n","2023-06-13T16:34:42.366732: step 4534, loss 0.614656, acc 0.8125, learning_rate 0.000532578\n","2023-06-13T16:34:43.778389: step 4535, loss 0.809874, acc 0.75, learning_rate 0.000532368\n","2023-06-13T16:34:45.473220: step 4536, loss 0.625501, acc 0.84375, learning_rate 0.000532159\n","2023-06-13T16:34:48.009044: step 4537, loss 0.403751, acc 0.875, learning_rate 0.000531949\n","2023-06-13T16:34:50.292258: step 4538, loss 0.949588, acc 0.71875, learning_rate 0.00053174\n","2023-06-13T16:34:52.586081: step 4539, loss 0.728788, acc 0.78125, learning_rate 0.00053153\n","2023-06-13T16:34:54.879386: step 4540, loss 0.640235, acc 0.78125, learning_rate 0.000531321\n","2023-06-13T16:34:56.692755: step 4541, loss 0.630759, acc 0.84375, learning_rate 0.000531112\n","2023-06-13T16:34:59.217227: step 4542, loss 0.543064, acc 0.78125, learning_rate 0.000530903\n","2023-06-13T16:35:01.524306: step 4543, loss 0.894623, acc 0.71875, learning_rate 0.000530694\n","2023-06-13T16:35:03.827050: step 4544, loss 0.974706, acc 0.6875, learning_rate 0.000530485\n","2023-06-13T16:35:06.313174: step 4545, loss 0.58311, acc 0.84375, learning_rate 0.000530276\n","2023-06-13T16:35:08.773995: step 4546, loss 0.59594, acc 0.75, learning_rate 0.000530068\n","2023-06-13T16:35:11.130012: step 4547, loss 0.539674, acc 0.8125, learning_rate 0.000529859\n","2023-06-13T16:35:13.391130: step 4548, loss 0.454394, acc 0.8125, learning_rate 0.000529651\n","2023-06-13T16:35:15.589154: step 4549, loss 1.1448, acc 0.71875, learning_rate 0.000529442\n","2023-06-13T16:35:17.299548: step 4550, loss 0.418555, acc 0.84375, learning_rate 0.000529234\n","2023-06-13T16:35:18.708541: step 4551, loss 0.931575, acc 0.625, learning_rate 0.000529026\n","2023-06-13T16:35:20.150656: step 4552, loss 0.401487, acc 0.84375, learning_rate 0.000528818\n","2023-06-13T16:35:21.564069: step 4553, loss 0.624944, acc 0.84375, learning_rate 0.00052861\n","2023-06-13T16:35:23.025008: step 4554, loss 0.677469, acc 0.84375, learning_rate 0.000528402\n","2023-06-13T16:35:24.461351: step 4555, loss 0.606711, acc 0.8125, learning_rate 0.000528195\n","2023-06-13T16:35:25.889676: step 4556, loss 0.661503, acc 0.78125, learning_rate 0.000527987\n","2023-06-13T16:35:27.820858: step 4557, loss 0.652191, acc 0.75, learning_rate 0.000527779\n","2023-06-13T16:35:30.282988: step 4558, loss 0.945779, acc 0.6875, learning_rate 0.000527572\n","2023-06-13T16:35:32.565331: step 4559, loss 0.856346, acc 0.6875, learning_rate 0.000527365\n","2023-06-13T16:35:34.853916: step 4560, loss 0.671888, acc 0.6875, learning_rate 0.000527157\n","2023-06-13T16:35:37.010973: step 4561, loss 0.805399, acc 0.71875, learning_rate 0.00052695\n","2023-06-13T16:35:38.427866: step 4562, loss 0.467053, acc 0.78125, learning_rate 0.000526743\n","2023-06-13T16:35:39.846089: step 4563, loss 0.586063, acc 0.78125, learning_rate 0.000526536\n","2023-06-13T16:35:41.283490: step 4564, loss 0.708582, acc 0.84375, learning_rate 0.00052633\n","2023-06-13T16:35:42.674252: step 4565, loss 0.712248, acc 0.71875, learning_rate 0.000526123\n","2023-06-13T16:35:44.082056: step 4566, loss 0.369388, acc 0.875, learning_rate 0.000525916\n","2023-06-13T16:35:45.512245: step 4567, loss 0.730691, acc 0.6875, learning_rate 0.00052571\n","2023-06-13T16:35:47.106814: step 4568, loss 0.590586, acc 0.78125, learning_rate 0.000525503\n","2023-06-13T16:35:49.718158: step 4569, loss 0.494891, acc 0.8125, learning_rate 0.000525297\n","2023-06-13T16:35:52.187537: step 4570, loss 0.802173, acc 0.6875, learning_rate 0.000525091\n","2023-06-13T16:35:54.461093: step 4571, loss 1.04836, acc 0.75, learning_rate 0.000524885\n","2023-06-13T16:35:56.615053: step 4572, loss 0.554135, acc 0.78125, learning_rate 0.000524679\n","2023-06-13T16:35:58.199121: step 4573, loss 0.701839, acc 0.75, learning_rate 0.000524473\n","2023-06-13T16:35:59.585885: step 4574, loss 0.655447, acc 0.75, learning_rate 0.000524267\n","2023-06-13T16:36:00.987529: step 4575, loss 0.36144, acc 0.875, learning_rate 0.000524061\n","2023-06-13T16:36:02.411714: step 4576, loss 0.965303, acc 0.6875, learning_rate 0.000523856\n","2023-06-13T16:36:03.827130: step 4577, loss 0.836799, acc 0.71875, learning_rate 0.00052365\n","2023-06-13T16:36:05.238318: step 4578, loss 0.665049, acc 0.8125, learning_rate 0.000523445\n","2023-06-13T16:36:06.656910: step 4579, loss 0.407369, acc 0.84375, learning_rate 0.000523239\n","2023-06-13T16:36:08.803905: step 4580, loss 0.38303, acc 0.84375, learning_rate 0.000523034\n","2023-06-13T16:36:11.330534: step 4581, loss 0.710466, acc 0.75, learning_rate 0.000522829\n","2023-06-13T16:36:13.745234: step 4582, loss 0.678966, acc 0.78125, learning_rate 0.000522624\n","2023-06-13T16:36:16.044759: step 4583, loss 0.61231, acc 0.875, learning_rate 0.000522419\n","2023-06-13T16:36:17.873586: step 4584, loss 0.486351, acc 0.84375, learning_rate 0.000522214\n","2023-06-13T16:36:19.274717: step 4585, loss 0.796403, acc 0.6875, learning_rate 0.000522009\n","2023-06-13T16:36:20.694311: step 4586, loss 0.654904, acc 0.78125, learning_rate 0.000521805\n","2023-06-13T16:36:22.136678: step 4587, loss 0.805457, acc 0.84375, learning_rate 0.0005216\n","2023-06-13T16:36:23.566101: step 4588, loss 0.758662, acc 0.71875, learning_rate 0.000521396\n","2023-06-13T16:36:25.006469: step 4589, loss 0.684867, acc 0.78125, learning_rate 0.000521192\n","2023-06-13T16:36:26.379820: step 4590, loss 0.252616, acc 0.9375, learning_rate 0.000520987\n","2023-06-13T16:36:28.285170: step 4591, loss 0.870523, acc 0.75, learning_rate 0.000520783\n","2023-06-13T16:36:30.871520: step 4592, loss 0.803192, acc 0.75, learning_rate 0.000520579\n","2023-06-13T16:36:33.023306: step 4593, loss 0.599791, acc 0.78125, learning_rate 0.000520375\n","2023-06-13T16:36:35.338871: step 4594, loss 0.601647, acc 0.8125, learning_rate 0.000520171\n","2023-06-13T16:36:37.581262: step 4595, loss 0.700828, acc 0.8125, learning_rate 0.000519968\n","2023-06-13T16:36:39.011798: step 4596, loss 0.660363, acc 0.78125, learning_rate 0.000519764\n","2023-06-13T16:36:40.417451: step 4597, loss 0.402149, acc 0.84375, learning_rate 0.00051956\n","2023-06-13T16:36:41.827652: step 4598, loss 0.932302, acc 0.6875, learning_rate 0.000519357\n","2023-06-13T16:36:43.237194: step 4599, loss 0.78756, acc 0.75, learning_rate 0.000519154\n","\n","Evaluation:\n","2023-06-13T16:37:11.587771: step 4600, loss 2.50419, acc 0.385571\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4600\n","\n","2023-06-13T16:37:14.208956: step 4600, loss 0.780038, acc 0.6875, learning_rate 0.00051895\n","2023-06-13T16:37:16.623617: step 4601, loss 0.890907, acc 0.71875, learning_rate 0.000518747\n","2023-06-13T16:37:18.933040: step 4602, loss 0.510829, acc 0.8125, learning_rate 0.000518544\n","2023-06-13T16:37:20.506397: step 4603, loss 0.858316, acc 0.71875, learning_rate 0.000518341\n","2023-06-13T16:37:21.925866: step 4604, loss 0.418127, acc 0.9375, learning_rate 0.000518139\n","2023-06-13T16:37:23.402644: step 4605, loss 0.619008, acc 0.875, learning_rate 0.000517936\n","2023-06-13T16:37:24.854278: step 4606, loss 1.07651, acc 0.71875, learning_rate 0.000517733\n","2023-06-13T16:37:26.284421: step 4607, loss 0.765279, acc 0.65625, learning_rate 0.000517531\n","2023-06-13T16:37:27.693893: step 4608, loss 0.503515, acc 0.84375, learning_rate 0.000517328\n","2023-06-13T16:37:29.114241: step 4609, loss 1.0899, acc 0.71875, learning_rate 0.000517126\n","2023-06-13T16:37:31.479858: step 4610, loss 0.79171, acc 0.75, learning_rate 0.000516924\n","2023-06-13T16:37:33.977853: step 4611, loss 0.700728, acc 0.75, learning_rate 0.000516721\n","2023-06-13T16:37:36.275284: step 4612, loss 0.578725, acc 0.84375, learning_rate 0.000516519\n","2023-06-13T16:37:38.627621: step 4613, loss 0.46512, acc 0.84375, learning_rate 0.000516317\n","2023-06-13T16:37:40.366192: step 4614, loss 0.728046, acc 0.78125, learning_rate 0.000516115\n","2023-06-13T16:37:41.784382: step 4615, loss 0.723201, acc 0.65625, learning_rate 0.000515914\n","2023-06-13T16:37:43.200378: step 4616, loss 0.633274, acc 0.75, learning_rate 0.000515712\n","2023-06-13T16:37:44.612206: step 4617, loss 0.999648, acc 0.6875, learning_rate 0.00051551\n","2023-06-13T16:37:46.061621: step 4618, loss 1.02299, acc 0.625, learning_rate 0.000515309\n","2023-06-13T16:37:47.485273: step 4619, loss 0.58268, acc 0.71875, learning_rate 0.000515108\n","2023-06-13T16:37:48.868159: step 4620, loss 0.757034, acc 0.78125, learning_rate 0.000514906\n","2023-06-13T16:37:50.887033: step 4621, loss 1.05461, acc 0.65625, learning_rate 0.000514705\n","2023-06-13T16:37:53.289599: step 4622, loss 1.12537, acc 0.6875, learning_rate 0.000514504\n","2023-06-13T16:37:55.736017: step 4623, loss 0.977492, acc 0.75, learning_rate 0.000514303\n","2023-06-13T16:37:58.144216: step 4624, loss 0.769755, acc 0.78125, learning_rate 0.000514102\n","2023-06-13T16:38:00.196655: step 4625, loss 0.55669, acc 0.875, learning_rate 0.000513901\n","2023-06-13T16:38:01.587864: step 4626, loss 0.708107, acc 0.78125, learning_rate 0.000513701\n","2023-06-13T16:38:03.022593: step 4627, loss 0.407243, acc 0.875, learning_rate 0.0005135\n","2023-06-13T16:38:04.453943: step 4628, loss 0.905446, acc 0.6875, learning_rate 0.0005133\n","2023-06-13T16:38:05.883388: step 4629, loss 0.601503, acc 0.84375, learning_rate 0.000513099\n","2023-06-13T16:38:07.313215: step 4630, loss 0.340059, acc 0.875, learning_rate 0.000512899\n","2023-06-13T16:38:08.720939: step 4631, loss 0.487771, acc 0.84375, learning_rate 0.000512699\n","2023-06-13T16:38:10.285509: step 4632, loss 0.49814, acc 0.78125, learning_rate 0.000512499\n","2023-06-13T16:38:12.780848: step 4633, loss 0.491517, acc 0.84375, learning_rate 0.000512299\n","2023-06-13T16:38:15.165196: step 4634, loss 0.628507, acc 0.8125, learning_rate 0.000512099\n","2023-06-13T16:38:17.589117: step 4635, loss 0.473428, acc 0.84375, learning_rate 0.000511899\n","2023-06-13T16:38:19.895177: step 4636, loss 0.529751, acc 0.8125, learning_rate 0.000511699\n","2023-06-13T16:38:21.280386: step 4637, loss 0.36111, acc 0.90625, learning_rate 0.0005115\n","2023-06-13T16:38:22.728177: step 4638, loss 0.683556, acc 0.78125, learning_rate 0.0005113\n","2023-06-13T16:38:24.141491: step 4639, loss 0.565766, acc 0.78125, learning_rate 0.000511101\n","2023-06-13T16:38:25.564413: step 4640, loss 0.909992, acc 0.625, learning_rate 0.000510901\n","2023-06-13T16:38:26.997267: step 4641, loss 0.708957, acc 0.75, learning_rate 0.000510702\n","2023-06-13T16:38:28.424771: step 4642, loss 0.860111, acc 0.78125, learning_rate 0.000510503\n","2023-06-13T16:38:29.995778: step 4643, loss 0.407771, acc 0.9375, learning_rate 0.000510304\n","2023-06-13T16:38:32.605700: step 4644, loss 0.504222, acc 0.875, learning_rate 0.000510105\n","2023-06-13T16:38:35.133709: step 4645, loss 0.918504, acc 0.75, learning_rate 0.000509906\n","2023-06-13T16:38:37.500886: step 4646, loss 0.445648, acc 0.90625, learning_rate 0.000509707\n","2023-06-13T16:38:39.640976: step 4647, loss 0.445095, acc 0.84375, learning_rate 0.000509509\n","2023-06-13T16:38:41.023895: step 4648, loss 0.665129, acc 0.75, learning_rate 0.00050931\n","2023-06-13T16:38:42.452149: step 4649, loss 0.866158, acc 0.8125, learning_rate 0.000509112\n","2023-06-13T16:38:43.884656: step 4650, loss 0.551053, acc 0.78125, learning_rate 0.000508913\n","2023-06-13T16:38:45.331802: step 4651, loss 0.781102, acc 0.71875, learning_rate 0.000508715\n","2023-06-13T16:38:46.744937: step 4652, loss 0.620344, acc 0.78125, learning_rate 0.000508517\n","2023-06-13T16:38:48.162138: step 4653, loss 0.441215, acc 0.84375, learning_rate 0.000508319\n","2023-06-13T16:38:49.667841: step 4654, loss 0.616239, acc 0.78125, learning_rate 0.000508121\n","2023-06-13T16:38:52.263764: step 4655, loss 0.804234, acc 0.75, learning_rate 0.000507923\n","2023-06-13T16:38:54.678646: step 4656, loss 0.693574, acc 0.8125, learning_rate 0.000507725\n","2023-06-13T16:38:56.942056: step 4657, loss 1.089, acc 0.625, learning_rate 0.000507527\n","2023-06-13T16:38:59.250179: step 4658, loss 0.711563, acc 0.78125, learning_rate 0.00050733\n","2023-06-13T16:39:00.838450: step 4659, loss 0.857011, acc 0.78125, learning_rate 0.000507132\n","2023-06-13T16:39:02.235242: step 4660, loss 0.533714, acc 0.75, learning_rate 0.000506935\n","2023-06-13T16:39:03.640402: step 4661, loss 0.567583, acc 0.8125, learning_rate 0.000506738\n","2023-06-13T16:39:05.086242: step 4662, loss 0.903424, acc 0.59375, learning_rate 0.00050654\n","2023-06-13T16:39:06.506011: step 4663, loss 0.515697, acc 0.8125, learning_rate 0.000506343\n","2023-06-13T16:39:07.931721: step 4664, loss 0.623009, acc 0.78125, learning_rate 0.000506146\n","2023-06-13T16:39:09.314641: step 4665, loss 0.794836, acc 0.75, learning_rate 0.000505949\n","2023-06-13T16:39:11.528385: step 4666, loss 0.723739, acc 0.78125, learning_rate 0.000505752\n","2023-06-13T16:39:13.963823: step 4667, loss 0.962792, acc 0.625, learning_rate 0.000505556\n","2023-06-13T16:39:16.347891: step 4668, loss 0.541461, acc 0.75, learning_rate 0.000505359\n","2023-06-13T16:39:18.627405: step 4669, loss 0.577702, acc 0.75, learning_rate 0.000505162\n","2023-06-13T16:39:20.398852: step 4670, loss 0.903084, acc 0.65625, learning_rate 0.000504966\n","2023-06-13T16:39:21.800275: step 4671, loss 0.672169, acc 0.71875, learning_rate 0.00050477\n","2023-06-13T16:39:23.222890: step 4672, loss 0.52715, acc 0.84375, learning_rate 0.000504573\n","2023-06-13T16:39:24.620320: step 4673, loss 0.343821, acc 0.90625, learning_rate 0.000504377\n","2023-06-13T16:39:26.057689: step 4674, loss 0.625616, acc 0.8125, learning_rate 0.000504181\n","2023-06-13T16:39:27.520429: step 4675, loss 0.798728, acc 0.78125, learning_rate 0.000503985\n","2023-06-13T16:39:28.926281: step 4676, loss 0.679755, acc 0.75, learning_rate 0.000503789\n","2023-06-13T16:39:30.848090: step 4677, loss 0.732947, acc 0.84375, learning_rate 0.000503593\n","2023-06-13T16:39:33.402207: step 4678, loss 0.67969, acc 0.71875, learning_rate 0.000503398\n","2023-06-13T16:39:35.695234: step 4679, loss 0.676269, acc 0.65625, learning_rate 0.000503202\n","2023-06-13T16:39:38.086886: step 4680, loss 0.328191, acc 0.84375, learning_rate 0.000503007\n","2023-06-13T16:39:40.094580: step 4681, loss 0.651002, acc 0.8125, learning_rate 0.000502811\n","2023-06-13T16:39:41.529185: step 4682, loss 0.506014, acc 0.8125, learning_rate 0.000502616\n","2023-06-13T16:39:42.915839: step 4683, loss 0.809241, acc 0.75, learning_rate 0.000502421\n","2023-06-13T16:39:44.334163: step 4684, loss 0.704863, acc 0.78125, learning_rate 0.000502226\n","2023-06-13T16:39:45.776242: step 4685, loss 0.805948, acc 0.625, learning_rate 0.000502031\n","2023-06-13T16:39:47.201381: step 4686, loss 0.654035, acc 0.78125, learning_rate 0.000501836\n","2023-06-13T16:39:48.608055: step 4687, loss 0.625682, acc 0.71875, learning_rate 0.000501641\n","2023-06-13T16:39:50.247212: step 4688, loss 0.490042, acc 0.8125, learning_rate 0.000501446\n","2023-06-13T16:39:52.702352: step 4689, loss 0.49945, acc 0.90625, learning_rate 0.000501251\n","2023-06-13T16:39:54.989099: step 4690, loss 0.483921, acc 0.8125, learning_rate 0.000501057\n","2023-06-13T16:39:57.411541: step 4691, loss 0.651415, acc 0.8125, learning_rate 0.000500862\n","2023-06-13T16:39:59.767711: step 4692, loss 0.515435, acc 0.78125, learning_rate 0.000500668\n","2023-06-13T16:40:01.174268: step 4693, loss 1.29352, acc 0.6875, learning_rate 0.000500474\n","2023-06-13T16:40:02.573477: step 4694, loss 0.928098, acc 0.65625, learning_rate 0.00050028\n","2023-06-13T16:40:03.974368: step 4695, loss 0.648341, acc 0.75, learning_rate 0.000500085\n","2023-06-13T16:40:05.404195: step 4696, loss 0.856282, acc 0.6875, learning_rate 0.000499891\n","2023-06-13T16:40:06.826064: step 4697, loss 0.701149, acc 0.8125, learning_rate 0.000499698\n","2023-06-13T16:40:08.293339: step 4698, loss 0.746027, acc 0.75, learning_rate 0.000499504\n","2023-06-13T16:40:09.673988: step 4699, loss 0.448316, acc 0.8125, learning_rate 0.00049931\n","\n","Evaluation:\n","2023-06-13T16:40:42.547045: step 4700, loss 2.51279, acc 0.384056\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4700\n","\n","2023-06-13T16:40:45.339016: step 4700, loss 0.926892, acc 0.59375, learning_rate 0.000499116\n","2023-06-13T16:40:47.843072: step 4701, loss 0.55638, acc 0.8125, learning_rate 0.000498923\n","2023-06-13T16:40:49.907200: step 4702, loss 0.494949, acc 0.84375, learning_rate 0.000498729\n","2023-06-13T16:40:51.335522: step 4703, loss 0.681132, acc 0.75, learning_rate 0.000498536\n","2023-06-13T16:40:52.783271: step 4704, loss 0.951018, acc 0.71875, learning_rate 0.000498343\n","2023-06-13T16:40:54.176311: step 4705, loss 0.52762, acc 0.78125, learning_rate 0.00049815\n","2023-06-13T16:40:55.573598: step 4706, loss 0.480972, acc 0.75, learning_rate 0.000497957\n","2023-06-13T16:40:57.008994: step 4707, loss 0.573019, acc 0.84375, learning_rate 0.000497764\n","2023-06-13T16:40:59.099367: step 4708, loss 0.42222, acc 0.875, learning_rate 0.000497571\n","2023-06-13T16:41:01.616663: step 4709, loss 0.803556, acc 0.6875, learning_rate 0.000497378\n","2023-06-13T16:41:04.082641: step 4710, loss 0.958823, acc 0.6875, learning_rate 0.000497185\n","2023-06-13T16:41:06.440040: step 4711, loss 0.898668, acc 0.75, learning_rate 0.000496993\n","2023-06-13T16:41:08.269657: step 4712, loss 0.671725, acc 0.78125, learning_rate 0.0004968\n","2023-06-13T16:41:09.658650: step 4713, loss 0.675353, acc 0.75, learning_rate 0.000496608\n","2023-06-13T16:41:11.079478: step 4714, loss 0.891144, acc 0.75, learning_rate 0.000496416\n","2023-06-13T16:41:12.514261: step 4715, loss 0.713958, acc 0.75, learning_rate 0.000496223\n","2023-06-13T16:41:13.920274: step 4716, loss 0.574081, acc 0.8125, learning_rate 0.000496031\n","2023-06-13T16:41:15.358502: step 4717, loss 0.855715, acc 0.6875, learning_rate 0.000495839\n","2023-06-13T16:41:16.809710: step 4718, loss 0.887721, acc 0.8125, learning_rate 0.000495647\n","2023-06-13T16:41:18.762179: step 4719, loss 0.721664, acc 0.75, learning_rate 0.000495456\n","2023-06-13T16:41:21.138039: step 4720, loss 0.726037, acc 0.71875, learning_rate 0.000495264\n","2023-06-13T16:41:23.577489: step 4721, loss 0.434359, acc 0.84375, learning_rate 0.000495072\n","2023-06-13T16:41:25.874697: step 4722, loss 0.594921, acc 0.8125, learning_rate 0.000494881\n","2023-06-13T16:41:28.049354: step 4723, loss 0.422756, acc 0.875, learning_rate 0.000494689\n","2023-06-13T16:41:29.476697: step 4724, loss 0.271696, acc 0.9375, learning_rate 0.000494498\n","2023-06-13T16:41:30.907150: step 4725, loss 0.9564, acc 0.78125, learning_rate 0.000494306\n","2023-06-13T16:41:32.362463: step 4726, loss 0.353692, acc 0.875, learning_rate 0.000494115\n","2023-06-13T16:41:33.786296: step 4727, loss 0.539546, acc 0.875, learning_rate 0.000493924\n","2023-06-13T16:41:35.208411: step 4728, loss 0.747579, acc 0.75, learning_rate 0.000493733\n","2023-06-13T16:41:36.644035: step 4729, loss 0.73584, acc 0.6875, learning_rate 0.000493542\n","2023-06-13T16:41:38.422105: step 4730, loss 0.810672, acc 0.78125, learning_rate 0.000493351\n","2023-06-13T16:41:40.961590: step 4731, loss 0.822013, acc 0.78125, learning_rate 0.000493161\n","2023-06-13T16:41:43.352144: step 4732, loss 0.55791, acc 0.8125, learning_rate 0.00049297\n","2023-06-13T16:41:45.551211: step 4733, loss 0.389638, acc 0.875, learning_rate 0.000492779\n","2023-06-13T16:41:47.848194: step 4734, loss 0.351341, acc 0.875, learning_rate 0.000492589\n","2023-06-13T16:41:49.293676: step 4735, loss 0.361377, acc 0.84375, learning_rate 0.000492399\n","2023-06-13T16:41:50.706513: step 4736, loss 0.803154, acc 0.71875, learning_rate 0.000492208\n","2023-06-13T16:41:52.135959: step 4737, loss 0.682656, acc 0.71875, learning_rate 0.000492018\n","2023-06-13T16:41:53.556494: step 4738, loss 0.5089, acc 0.84375, learning_rate 0.000491828\n","2023-06-13T16:41:54.955074: step 4739, loss 0.631579, acc 0.8125, learning_rate 0.000491638\n","2023-06-13T16:41:56.356697: step 4740, loss 0.841901, acc 0.6875, learning_rate 0.000491448\n","2023-06-13T16:41:57.809562: step 4741, loss 0.678365, acc 0.78125, learning_rate 0.000491258\n","2023-06-13T16:42:00.107936: step 4742, loss 0.340883, acc 0.9375, learning_rate 0.000491069\n","2023-06-13T16:42:02.555608: step 4743, loss 0.47834, acc 0.8125, learning_rate 0.000490879\n","2023-06-13T16:42:04.803775: step 4744, loss 1.22723, acc 0.6875, learning_rate 0.00049069\n","2023-06-13T16:42:07.099373: step 4745, loss 0.610311, acc 0.9375, learning_rate 0.0004905\n","2023-06-13T16:42:08.999495: step 4746, loss 0.722097, acc 0.71875, learning_rate 0.000490311\n","2023-06-13T16:42:10.432692: step 4747, loss 0.618524, acc 0.8125, learning_rate 0.000490122\n","2023-06-13T16:42:11.826163: step 4748, loss 0.897456, acc 0.65625, learning_rate 0.000489932\n","2023-06-13T16:42:13.229674: step 4749, loss 0.657147, acc 0.75, learning_rate 0.000489743\n","2023-06-13T16:42:14.648478: step 4750, loss 0.825648, acc 0.78125, learning_rate 0.000489554\n","2023-06-13T16:42:16.087382: step 4751, loss 0.525934, acc 0.78125, learning_rate 0.000489365\n","2023-06-13T16:42:17.478267: step 4752, loss 0.456908, acc 0.8125, learning_rate 0.000489177\n","2023-06-13T16:42:19.239877: step 4753, loss 0.816316, acc 0.6875, learning_rate 0.000488988\n","2023-06-13T16:42:21.760625: step 4754, loss 0.521484, acc 0.8125, learning_rate 0.000488799\n","2023-06-13T16:42:24.093156: step 4755, loss 0.9296, acc 0.625, learning_rate 0.000488611\n","2023-06-13T16:42:26.575757: step 4756, loss 0.831582, acc 0.65625, learning_rate 0.000488422\n","2023-06-13T16:42:28.638173: step 4757, loss 0.708598, acc 0.75, learning_rate 0.000488234\n","2023-06-13T16:42:30.032481: step 4758, loss 0.295606, acc 0.9375, learning_rate 0.000488046\n","2023-06-13T16:42:31.431555: step 4759, loss 1.0877, acc 0.625, learning_rate 0.000487858\n","2023-06-13T16:42:32.868074: step 4760, loss 0.776068, acc 0.6875, learning_rate 0.00048767\n","2023-06-13T16:42:34.301771: step 4761, loss 0.750717, acc 0.84375, learning_rate 0.000487482\n","2023-06-13T16:42:35.739258: step 4762, loss 0.668204, acc 0.6875, learning_rate 0.000487294\n","2023-06-13T16:42:37.214699: step 4763, loss 0.865953, acc 0.625, learning_rate 0.000487106\n","2023-06-13T16:42:39.083428: step 4764, loss 0.303591, acc 0.875, learning_rate 0.000486918\n","2023-06-13T16:42:41.490502: step 4765, loss 0.578951, acc 0.8125, learning_rate 0.000486731\n","2023-06-13T16:42:43.893332: step 4766, loss 0.524481, acc 0.75, learning_rate 0.000486543\n","2023-06-13T16:42:46.302002: step 4767, loss 1.08605, acc 0.625, learning_rate 0.000486356\n","2023-06-13T16:42:48.388347: step 4768, loss 0.488018, acc 0.84375, learning_rate 0.000486168\n","2023-06-13T16:42:49.826415: step 4769, loss 0.859024, acc 0.78125, learning_rate 0.000485981\n","2023-06-13T16:42:51.235968: step 4770, loss 0.801366, acc 0.75, learning_rate 0.000485794\n","2023-06-13T16:42:52.667379: step 4771, loss 0.838896, acc 0.71875, learning_rate 0.000485607\n","2023-06-13T16:42:54.075820: step 4772, loss 0.703283, acc 0.78125, learning_rate 0.00048542\n","2023-06-13T16:42:55.495758: step 4773, loss 0.585489, acc 0.71875, learning_rate 0.000485233\n","2023-06-13T16:42:56.923187: step 4774, loss 0.507666, acc 0.84375, learning_rate 0.000485046\n","2023-06-13T16:42:58.540175: step 4775, loss 0.936876, acc 0.625, learning_rate 0.00048486\n","2023-06-13T16:43:01.042428: step 4776, loss 1.01465, acc 0.65625, learning_rate 0.000484673\n","2023-06-13T16:43:03.393614: step 4777, loss 0.818603, acc 0.71875, learning_rate 0.000484486\n","2023-06-13T16:43:05.648410: step 4778, loss 0.630844, acc 0.78125, learning_rate 0.0004843\n","2023-06-13T16:43:07.885433: step 4779, loss 0.536865, acc 0.8125, learning_rate 0.000484114\n","2023-06-13T16:43:09.565005: step 4780, loss 0.924079, acc 0.78125, learning_rate 0.000483927\n","2023-06-13T16:43:10.966564: step 4781, loss 0.505516, acc 0.8125, learning_rate 0.000483741\n","2023-06-13T16:43:12.386212: step 4782, loss 0.743228, acc 0.8125, learning_rate 0.000483555\n","2023-06-13T16:43:13.799820: step 4783, loss 0.952791, acc 0.8125, learning_rate 0.000483369\n","2023-06-13T16:43:15.196051: step 4784, loss 0.628414, acc 0.78125, learning_rate 0.000483183\n","2023-06-13T16:43:16.617495: step 4785, loss 0.895886, acc 0.6875, learning_rate 0.000482998\n","2023-06-13T16:43:18.042905: step 4786, loss 0.357823, acc 0.875, learning_rate 0.000482812\n","2023-06-13T16:43:20.051135: step 4787, loss 0.731327, acc 0.78125, learning_rate 0.000482626\n","2023-06-13T16:43:22.503701: step 4788, loss 0.29882, acc 0.9375, learning_rate 0.000482441\n","2023-06-13T16:43:24.760183: step 4789, loss 0.702037, acc 0.71875, learning_rate 0.000482255\n","2023-06-13T16:43:27.071579: step 4790, loss 0.48862, acc 0.8125, learning_rate 0.00048207\n","2023-06-13T16:43:29.126275: step 4791, loss 0.707933, acc 0.78125, learning_rate 0.000481885\n","2023-06-13T16:43:30.563838: step 4792, loss 0.467399, acc 0.78125, learning_rate 0.000481699\n","2023-06-13T16:43:31.992744: step 4793, loss 0.460747, acc 0.90625, learning_rate 0.000481514\n","2023-06-13T16:43:33.401933: step 4794, loss 0.792983, acc 0.65625, learning_rate 0.000481329\n","2023-06-13T16:43:34.794974: step 4795, loss 0.879205, acc 0.6875, learning_rate 0.000481145\n","2023-06-13T16:43:36.196859: step 4796, loss 0.866522, acc 0.8125, learning_rate 0.00048096\n","2023-06-13T16:43:37.616697: step 4797, loss 0.906337, acc 0.71875, learning_rate 0.000480775\n","2023-06-13T16:43:39.318194: step 4798, loss 0.862972, acc 0.6875, learning_rate 0.00048059\n","2023-06-13T16:43:41.746254: step 4799, loss 0.509276, acc 0.8125, learning_rate 0.000480406\n","\n","Evaluation:\n","2023-06-13T16:44:12.253069: step 4800, loss 2.48321, acc 0.388148\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4800\n","\n","2023-06-13T16:44:13.819718: step 4800, loss 0.657145, acc 0.78125, learning_rate 0.000480221\n","2023-06-13T16:44:15.230105: step 4801, loss 0.425481, acc 0.90625, learning_rate 0.000480037\n","2023-06-13T16:44:16.652153: step 4802, loss 0.852426, acc 0.71875, learning_rate 0.000479853\n","2023-06-13T16:44:18.042157: step 4803, loss 0.693178, acc 0.78125, learning_rate 0.000479669\n","2023-06-13T16:44:19.464458: step 4804, loss 0.750981, acc 0.71875, learning_rate 0.000479484\n","2023-06-13T16:44:21.210360: step 4805, loss 0.437893, acc 0.90625, learning_rate 0.0004793\n","2023-06-13T16:44:23.782226: step 4806, loss 0.839407, acc 0.8125, learning_rate 0.000479117\n","2023-06-13T16:44:26.127662: step 4807, loss 0.652354, acc 0.75, learning_rate 0.000478933\n","2023-06-13T16:44:28.461130: step 4808, loss 0.93324, acc 0.65625, learning_rate 0.000478749\n","2023-06-13T16:44:30.678240: step 4809, loss 0.853047, acc 0.71875, learning_rate 0.000478565\n","2023-06-13T16:44:32.106061: step 4810, loss 0.447847, acc 0.84375, learning_rate 0.000478382\n","2023-06-13T16:44:33.511422: step 4811, loss 0.78269, acc 0.75, learning_rate 0.000478198\n","2023-06-13T16:44:34.953236: step 4812, loss 0.667369, acc 0.78125, learning_rate 0.000478015\n","2023-06-13T16:44:36.360527: step 4813, loss 0.607967, acc 0.71875, learning_rate 0.000477832\n","2023-06-13T16:44:37.794319: step 4814, loss 0.555984, acc 0.875, learning_rate 0.000477648\n","2023-06-13T16:44:39.214937: step 4815, loss 0.599721, acc 0.78125, learning_rate 0.000477465\n","2023-06-13T16:44:40.633467: step 4816, loss 0.575639, acc 0.8125, learning_rate 0.000477282\n","2023-06-13T16:44:43.155977: step 4817, loss 0.504849, acc 0.84375, learning_rate 0.000477099\n","2023-06-13T16:44:45.441472: step 4818, loss 0.646131, acc 0.78125, learning_rate 0.000476917\n","2023-06-13T16:44:47.689830: step 4819, loss 0.863649, acc 0.6875, learning_rate 0.000476734\n","2023-06-13T16:44:49.987509: step 4820, loss 0.718203, acc 0.71875, learning_rate 0.000476551\n","2023-06-13T16:44:51.865871: step 4821, loss 0.721206, acc 0.71875, learning_rate 0.000476369\n","2023-06-13T16:44:53.286240: step 4822, loss 0.743903, acc 0.75, learning_rate 0.000476186\n","2023-06-13T16:44:54.668982: step 4823, loss 0.708241, acc 0.71875, learning_rate 0.000476004\n","2023-06-13T16:44:56.075963: step 4824, loss 0.721362, acc 0.84375, learning_rate 0.000475821\n","2023-06-13T16:44:57.496344: step 4825, loss 0.467266, acc 0.875, learning_rate 0.000475639\n","2023-06-13T16:44:58.926158: step 4826, loss 0.958922, acc 0.75, learning_rate 0.000475457\n","2023-06-13T16:45:00.315731: step 4827, loss 0.616724, acc 0.78125, learning_rate 0.000475275\n","2023-06-13T16:45:02.128380: step 4828, loss 0.59091, acc 0.75, learning_rate 0.000475093\n","2023-06-13T16:45:04.676478: step 4829, loss 0.561863, acc 0.8125, learning_rate 0.000474911\n","2023-06-13T16:45:07.073004: step 4830, loss 0.532627, acc 0.84375, learning_rate 0.000474729\n","2023-06-13T16:45:09.478197: step 4831, loss 0.558017, acc 0.8125, learning_rate 0.000474548\n","2023-06-13T16:45:11.467195: step 4832, loss 0.398341, acc 0.84375, learning_rate 0.000474366\n","2023-06-13T16:45:12.896289: step 4833, loss 0.748698, acc 0.8125, learning_rate 0.000474184\n","2023-06-13T16:45:14.301599: step 4834, loss 0.639008, acc 0.65625, learning_rate 0.000474003\n","2023-06-13T16:45:15.741959: step 4835, loss 0.404935, acc 0.84375, learning_rate 0.000473822\n","2023-06-13T16:45:17.175042: step 4836, loss 0.669611, acc 0.6875, learning_rate 0.00047364\n","2023-06-13T16:45:18.583015: step 4837, loss 0.893058, acc 0.71875, learning_rate 0.000473459\n","2023-06-13T16:45:20.036417: step 4838, loss 0.460741, acc 0.8125, learning_rate 0.000473278\n","2023-06-13T16:45:21.650338: step 4839, loss 0.795702, acc 0.75, learning_rate 0.000473097\n","2023-06-13T16:45:24.199238: step 4840, loss 0.736461, acc 0.71875, learning_rate 0.000472916\n","2023-06-13T16:45:26.552924: step 4841, loss 0.896638, acc 0.65625, learning_rate 0.000472735\n","2023-06-13T16:45:28.822835: step 4842, loss 0.678845, acc 0.71875, learning_rate 0.000472555\n","2023-06-13T16:45:31.107387: step 4843, loss 0.735129, acc 0.78125, learning_rate 0.000472374\n","2023-06-13T16:45:32.608338: step 4844, loss 0.850204, acc 0.6875, learning_rate 0.000472193\n","2023-06-13T16:45:34.016603: step 4845, loss 0.754909, acc 0.78125, learning_rate 0.000472013\n","2023-06-13T16:45:35.446946: step 4846, loss 0.597399, acc 0.84375, learning_rate 0.000471833\n","2023-06-13T16:45:36.899889: step 4847, loss 0.709158, acc 0.78125, learning_rate 0.000471652\n","2023-06-13T16:45:38.297215: step 4848, loss 1.03225, acc 0.71875, learning_rate 0.000471472\n","2023-06-13T16:45:39.701079: step 4849, loss 0.663067, acc 0.8125, learning_rate 0.000471292\n","2023-06-13T16:45:41.128345: step 4850, loss 0.56994, acc 0.8125, learning_rate 0.000471112\n","2023-06-13T16:45:43.433472: step 4851, loss 0.715812, acc 0.71875, learning_rate 0.000470932\n","2023-06-13T16:45:46.001468: step 4852, loss 0.787335, acc 0.75, learning_rate 0.000470752\n","2023-06-13T16:45:48.371781: step 4853, loss 0.770194, acc 0.75, learning_rate 0.000470572\n","2023-06-13T16:45:50.695079: step 4854, loss 0.551374, acc 0.90625, learning_rate 0.000470393\n","2023-06-13T16:45:52.380076: step 4855, loss 0.852192, acc 0.75, learning_rate 0.000470213\n","2023-06-13T16:45:54.185500: step 4856, loss 0.739443, acc 0.71875, learning_rate 0.000470034\n","2023-06-13T16:45:56.849991: step 4857, loss 0.58144, acc 0.8125, learning_rate 0.000469854\n","2023-06-13T16:45:59.290274: step 4858, loss 0.612443, acc 0.8125, learning_rate 0.000469675\n","2023-06-13T16:46:01.772382: step 4859, loss 0.871384, acc 0.625, learning_rate 0.000469496\n","2023-06-13T16:46:04.480947: step 4860, loss 0.996178, acc 0.75, learning_rate 0.000469316\n","2023-06-13T16:46:07.024105: step 4861, loss 0.669257, acc 0.78125, learning_rate 0.000469137\n","2023-06-13T16:46:09.394666: step 4862, loss 0.702899, acc 0.78125, learning_rate 0.000468958\n","2023-06-13T16:46:11.742153: step 4863, loss 0.910713, acc 0.71875, learning_rate 0.00046878\n","2023-06-13T16:46:13.711304: step 4864, loss 0.867418, acc 0.71875, learning_rate 0.000468601\n","2023-06-13T16:46:15.137208: step 4865, loss 0.645742, acc 0.75, learning_rate 0.000468422\n","2023-06-13T16:46:16.526237: step 4866, loss 0.648474, acc 0.78125, learning_rate 0.000468243\n","2023-06-13T16:46:17.961281: step 4867, loss 0.716854, acc 0.78125, learning_rate 0.000468065\n","2023-06-13T16:46:19.365166: step 4868, loss 0.887627, acc 0.71875, learning_rate 0.000467886\n","2023-06-13T16:46:20.743591: step 4869, loss 0.596281, acc 0.78125, learning_rate 0.000467708\n","2023-06-13T16:46:22.131858: step 4870, loss 0.683491, acc 0.84375, learning_rate 0.00046753\n","2023-06-13T16:46:23.848581: step 4871, loss 0.769062, acc 0.6875, learning_rate 0.000467351\n","2023-06-13T16:46:26.261979: step 4872, loss 0.584649, acc 0.75, learning_rate 0.000467173\n","2023-06-13T16:46:28.542688: step 4873, loss 0.705176, acc 0.71875, learning_rate 0.000466995\n","2023-06-13T16:46:30.869701: step 4874, loss 0.655486, acc 0.8125, learning_rate 0.000466817\n","2023-06-13T16:46:33.235067: step 4875, loss 0.488325, acc 0.875, learning_rate 0.000466639\n","2023-06-13T16:46:34.727401: step 4876, loss 0.605979, acc 0.78125, learning_rate 0.000466462\n","2023-06-13T16:46:36.155006: step 4877, loss 0.531468, acc 0.8125, learning_rate 0.000466284\n","2023-06-13T16:46:37.629056: step 4878, loss 0.831567, acc 0.75, learning_rate 0.000466106\n","2023-06-13T16:46:39.063627: step 4879, loss 0.722356, acc 0.78125, learning_rate 0.000465929\n","2023-06-13T16:46:40.466806: step 4880, loss 0.651732, acc 0.78125, learning_rate 0.000465751\n","2023-06-13T16:46:41.881736: step 4881, loss 0.649854, acc 0.78125, learning_rate 0.000465574\n","2023-06-13T16:46:43.298882: step 4882, loss 0.561135, acc 0.8125, learning_rate 0.000465397\n","2023-06-13T16:46:45.676700: step 4883, loss 0.786031, acc 0.75, learning_rate 0.00046522\n","2023-06-13T16:46:48.066499: step 4884, loss 0.625635, acc 0.71875, learning_rate 0.000465043\n","2023-06-13T16:46:50.410153: step 4885, loss 0.515006, acc 0.78125, learning_rate 0.000464866\n","2023-06-13T16:46:52.798246: step 4886, loss 0.678774, acc 0.75, learning_rate 0.000464689\n","2023-06-13T16:46:54.503901: step 4887, loss 0.565606, acc 0.78125, learning_rate 0.000464512\n","2023-06-13T16:46:55.926159: step 4888, loss 0.678607, acc 0.75, learning_rate 0.000464335\n","2023-06-13T16:46:57.360177: step 4889, loss 0.679753, acc 0.75, learning_rate 0.000464158\n","2023-06-13T16:46:58.790974: step 4890, loss 0.601797, acc 0.75, learning_rate 0.000463982\n","2023-06-13T16:47:00.201449: step 4891, loss 0.562313, acc 0.78125, learning_rate 0.000463805\n","2023-06-13T16:47:01.596167: step 4892, loss 0.806288, acc 0.78125, learning_rate 0.000463629\n","2023-06-13T16:47:03.012176: step 4893, loss 0.846164, acc 0.6875, learning_rate 0.000463453\n","2023-06-13T16:47:05.075440: step 4894, loss 0.50139, acc 0.84375, learning_rate 0.000463276\n","2023-06-13T16:47:07.616011: step 4895, loss 0.477823, acc 0.8125, learning_rate 0.0004631\n","2023-06-13T16:47:09.991869: step 4896, loss 0.541007, acc 0.8125, learning_rate 0.000462924\n","2023-06-13T16:47:12.249273: step 4897, loss 0.56546, acc 0.78125, learning_rate 0.000462748\n","2023-06-13T16:47:14.203147: step 4898, loss 0.655483, acc 0.8125, learning_rate 0.000462572\n","2023-06-13T16:47:15.612274: step 4899, loss 0.566838, acc 0.84375, learning_rate 0.000462397\n","\n","Evaluation:\n","2023-06-13T16:47:42.886220: step 4900, loss 2.52621, acc 0.382085\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-4900\n","\n","2023-06-13T16:47:44.542529: step 4900, loss 0.491531, acc 0.84375, learning_rate 0.000462221\n","2023-06-13T16:47:47.068732: step 4901, loss 0.576751, acc 0.84375, learning_rate 0.000462045\n","2023-06-13T16:47:49.452340: step 4902, loss 1.01638, acc 0.75, learning_rate 0.00046187\n","2023-06-13T16:47:51.916632: step 4903, loss 0.677565, acc 0.75, learning_rate 0.000461694\n","2023-06-13T16:47:54.201887: step 4904, loss 0.555998, acc 0.875, learning_rate 0.000461519\n","2023-06-13T16:47:55.766724: step 4905, loss 0.603801, acc 0.8125, learning_rate 0.000461344\n","2023-06-13T16:47:57.176017: step 4906, loss 0.391805, acc 0.875, learning_rate 0.000461168\n","2023-06-13T16:47:58.633214: step 4907, loss 0.982209, acc 0.6875, learning_rate 0.000460993\n","2023-06-13T16:48:00.070008: step 4908, loss 0.861152, acc 0.78125, learning_rate 0.000460818\n","2023-06-13T16:48:01.590173: step 4909, loss 0.376649, acc 0.84375, learning_rate 0.000460643\n","2023-06-13T16:48:03.023132: step 4910, loss 0.836881, acc 0.6875, learning_rate 0.000460468\n","2023-06-13T16:48:04.453925: step 4911, loss 0.594767, acc 0.8125, learning_rate 0.000460294\n","2023-06-13T16:48:06.780914: step 4912, loss 0.392342, acc 0.875, learning_rate 0.000460119\n","2023-06-13T16:48:09.201678: step 4913, loss 0.786843, acc 0.78125, learning_rate 0.000459944\n","2023-06-13T16:48:11.436061: step 4914, loss 0.828752, acc 0.625, learning_rate 0.00045977\n","2023-06-13T16:48:13.910136: step 4915, loss 0.647842, acc 0.78125, learning_rate 0.000459595\n","2023-06-13T16:48:15.727346: step 4916, loss 0.659087, acc 0.6875, learning_rate 0.000459421\n","2023-06-13T16:48:17.167686: step 4917, loss 0.604643, acc 0.78125, learning_rate 0.000459247\n","2023-06-13T16:48:18.594966: step 4918, loss 0.786377, acc 0.71875, learning_rate 0.000459072\n","2023-06-13T16:48:20.019837: step 4919, loss 0.880058, acc 0.59375, learning_rate 0.000458898\n","2023-06-13T16:48:21.431371: step 4920, loss 0.834305, acc 0.71875, learning_rate 0.000458724\n","2023-06-13T16:48:22.840545: step 4921, loss 0.595345, acc 0.84375, learning_rate 0.00045855\n","2023-06-13T16:48:24.300718: step 4922, loss 0.5953, acc 0.78125, learning_rate 0.000458377\n","2023-06-13T16:48:26.453729: step 4923, loss 0.813287, acc 0.75, learning_rate 0.000458203\n","2023-06-13T16:48:28.902585: step 4924, loss 0.374489, acc 0.84375, learning_rate 0.000458029\n","2023-06-13T16:48:31.253272: step 4925, loss 0.406316, acc 0.90625, learning_rate 0.000457855\n","2023-06-13T16:48:33.592429: step 4926, loss 0.736039, acc 0.8125, learning_rate 0.000457682\n","2023-06-13T16:48:35.439488: step 4927, loss 0.738972, acc 0.75, learning_rate 0.000457509\n","2023-06-13T16:48:36.856299: step 4928, loss 0.611613, acc 0.78125, learning_rate 0.000457335\n","2023-06-13T16:48:38.328784: step 4929, loss 0.823857, acc 0.71875, learning_rate 0.000457162\n","2023-06-13T16:48:39.755611: step 4930, loss 0.572159, acc 0.84375, learning_rate 0.000456989\n","2023-06-13T16:48:41.166616: step 4931, loss 0.341135, acc 0.90625, learning_rate 0.000456816\n","2023-06-13T16:48:42.584337: step 4932, loss 0.401828, acc 0.84375, learning_rate 0.000456643\n","2023-06-13T16:48:44.021112: step 4933, loss 0.682806, acc 0.71875, learning_rate 0.00045647\n","2023-06-13T16:48:45.948295: step 4934, loss 0.641251, acc 0.78125, learning_rate 0.000456297\n","2023-06-13T16:48:48.439340: step 4935, loss 0.466514, acc 0.8125, learning_rate 0.000456124\n","2023-06-13T16:48:50.836876: step 4936, loss 0.9761, acc 0.6875, learning_rate 0.000455951\n","2023-06-13T16:48:53.169615: step 4937, loss 1.13729, acc 0.65625, learning_rate 0.000455779\n","2023-06-13T16:48:55.176380: step 4938, loss 0.552941, acc 0.84375, learning_rate 0.000455606\n","2023-06-13T16:48:56.577055: step 4939, loss 0.747626, acc 0.71875, learning_rate 0.000455434\n","2023-06-13T16:48:58.015447: step 4940, loss 0.987057, acc 0.75, learning_rate 0.000455262\n","2023-06-13T16:48:59.434365: step 4941, loss 0.697812, acc 0.75, learning_rate 0.000455089\n","2023-06-13T16:49:00.828624: step 4942, loss 0.629845, acc 0.75, learning_rate 0.000454917\n","2023-06-13T16:49:02.281294: step 4943, loss 0.4438, acc 0.875, learning_rate 0.000454745\n","2023-06-13T16:49:03.676109: step 4944, loss 1.44094, acc 0.5625, learning_rate 0.000454573\n","2023-06-13T16:49:05.324416: step 4945, loss 0.757857, acc 0.6875, learning_rate 0.000454401\n","2023-06-13T16:49:07.745876: step 4946, loss 0.768592, acc 0.8125, learning_rate 0.000454229\n","2023-06-13T16:49:10.138548: step 4947, loss 0.839196, acc 0.71875, learning_rate 0.000454057\n","2023-06-13T16:49:12.514146: step 4948, loss 0.845605, acc 0.6875, learning_rate 0.000453886\n","2023-06-13T16:49:14.411125: step 4949, loss 1.03821, acc 0.625, learning_rate 0.000453714\n","2023-06-13T16:49:15.935307: step 4950, loss 0.576697, acc 0.71875, learning_rate 0.000453543\n","2023-06-13T16:49:17.348382: step 4951, loss 0.51071, acc 0.84375, learning_rate 0.000453371\n","2023-06-13T16:49:18.776360: step 4952, loss 0.602801, acc 0.8125, learning_rate 0.0004532\n","2023-06-13T16:49:20.173746: step 4953, loss 0.520178, acc 0.84375, learning_rate 0.000453029\n","2023-06-13T16:49:21.582539: step 4954, loss 0.476306, acc 0.875, learning_rate 0.000452858\n","2023-06-13T16:49:23.043704: step 4955, loss 0.515101, acc 0.875, learning_rate 0.000452686\n","2023-06-13T16:49:24.433895: step 4956, loss 0.378041, acc 0.84375, learning_rate 0.000452515\n","2023-06-13T16:49:26.665975: step 4957, loss 0.364334, acc 0.875, learning_rate 0.000452344\n","2023-06-13T16:49:29.131114: step 4958, loss 0.536822, acc 0.84375, learning_rate 0.000452174\n","2023-06-13T16:49:31.561810: step 4959, loss 0.542245, acc 0.8125, learning_rate 0.000452003\n","2023-06-13T16:49:33.818908: step 4960, loss 0.497664, acc 0.875, learning_rate 0.000451832\n","2023-06-13T16:49:35.651575: step 4961, loss 0.533202, acc 0.8125, learning_rate 0.000451662\n","2023-06-13T16:49:37.068350: step 4962, loss 0.449486, acc 0.875, learning_rate 0.000451491\n","2023-06-13T16:49:38.503642: step 4963, loss 0.719622, acc 0.71875, learning_rate 0.000451321\n","2023-06-13T16:49:39.951046: step 4964, loss 0.620277, acc 0.84375, learning_rate 0.00045115\n","2023-06-13T16:49:41.385194: step 4965, loss 0.63842, acc 0.8125, learning_rate 0.00045098\n","2023-06-13T16:49:42.811039: step 4966, loss 0.642234, acc 0.75, learning_rate 0.00045081\n","2023-06-13T16:49:44.218759: step 4967, loss 0.831675, acc 0.6875, learning_rate 0.00045064\n","2023-06-13T16:49:46.248376: step 4968, loss 0.543371, acc 0.84375, learning_rate 0.00045047\n","2023-06-13T16:49:48.959205: step 4969, loss 0.856211, acc 0.71875, learning_rate 0.0004503\n","2023-06-13T16:49:51.404301: step 4970, loss 0.483302, acc 0.78125, learning_rate 0.00045013\n","2023-06-13T16:49:53.613455: step 4971, loss 0.451826, acc 0.84375, learning_rate 0.00044996\n","2023-06-13T16:49:55.783973: step 4972, loss 0.859218, acc 0.78125, learning_rate 0.000449791\n","2023-06-13T16:49:57.314476: step 4973, loss 0.524571, acc 0.90625, learning_rate 0.000449621\n","2023-06-13T16:49:58.771170: step 4974, loss 0.574621, acc 0.75, learning_rate 0.000449451\n","2023-06-13T16:50:00.219872: step 4975, loss 0.929077, acc 0.75, learning_rate 0.000449282\n","2023-06-13T16:50:01.706295: step 4976, loss 0.814417, acc 0.6875, learning_rate 0.000449113\n","2023-06-13T16:50:03.155754: step 4977, loss 0.781825, acc 0.78125, learning_rate 0.000448943\n","2023-06-13T16:50:04.574653: step 4978, loss 0.392909, acc 0.8125, learning_rate 0.000448774\n","2023-06-13T16:50:06.283187: step 4979, loss 0.936571, acc 0.71875, learning_rate 0.000448605\n","2023-06-13T16:50:09.028259: step 4980, loss 0.639512, acc 0.78125, learning_rate 0.000448436\n","2023-06-13T16:50:11.570648: step 4981, loss 0.45498, acc 0.875, learning_rate 0.000448267\n","2023-06-13T16:50:14.004918: step 4982, loss 0.605891, acc 0.84375, learning_rate 0.000448098\n","2023-06-13T16:50:16.377927: step 4983, loss 0.666071, acc 0.78125, learning_rate 0.000447929\n","2023-06-13T16:50:17.906302: step 4984, loss 0.888189, acc 0.65625, learning_rate 0.000447761\n","2023-06-13T16:50:19.371496: step 4985, loss 0.878165, acc 0.71875, learning_rate 0.000447592\n","2023-06-13T16:50:20.861465: step 4986, loss 0.576072, acc 0.8125, learning_rate 0.000447424\n","2023-06-13T16:50:22.325171: step 4987, loss 0.468534, acc 0.84375, learning_rate 0.000447255\n","2023-06-13T16:50:23.784391: step 4988, loss 0.734805, acc 0.75, learning_rate 0.000447087\n","2023-06-13T16:50:25.233012: step 4989, loss 0.448688, acc 0.84375, learning_rate 0.000446918\n","2023-06-13T16:50:27.106554: step 4990, loss 0.438781, acc 0.84375, learning_rate 0.00044675\n","2023-06-13T16:50:29.694402: step 4991, loss 0.663965, acc 0.75, learning_rate 0.000446582\n","2023-06-13T16:50:32.228441: step 4992, loss 0.607425, acc 0.84375, learning_rate 0.000446414\n","2023-06-13T16:50:34.564907: step 4993, loss 0.624228, acc 0.78125, learning_rate 0.000446246\n","2023-06-13T16:50:36.818496: step 4994, loss 0.918053, acc 0.8125, learning_rate 0.000446078\n","2023-06-13T16:50:38.296783: step 4995, loss 0.546189, acc 0.84375, learning_rate 0.00044591\n","2023-06-13T16:50:39.772637: step 4996, loss 0.43674, acc 0.90625, learning_rate 0.000445743\n","2023-06-13T16:50:41.222493: step 4997, loss 0.435826, acc 0.8125, learning_rate 0.000445575\n","2023-06-13T16:50:42.734259: step 4998, loss 1.07215, acc 0.71875, learning_rate 0.000445407\n","2023-06-13T16:50:44.208479: step 4999, loss 0.728891, acc 0.8125, learning_rate 0.00044524\n","\n","Evaluation:\n","2023-06-13T16:51:16.506732: step 5000, loss 2.54886, acc 0.386178\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5000\n","\n","2023-06-13T16:51:19.405105: step 5000, loss 0.436485, acc 0.84375, learning_rate 0.000445073\n","2023-06-13T16:51:21.949459: step 5001, loss 0.410146, acc 0.84375, learning_rate 0.000444905\n","2023-06-13T16:51:24.485644: step 5002, loss 0.517146, acc 0.84375, learning_rate 0.000444738\n","2023-06-13T16:51:26.599582: step 5003, loss 0.55077, acc 0.84375, learning_rate 0.000444571\n","2023-06-13T16:51:28.049547: step 5004, loss 0.657076, acc 0.84375, learning_rate 0.000444404\n","2023-06-13T16:51:29.512223: step 5005, loss 0.424964, acc 0.90625, learning_rate 0.000444237\n","2023-06-13T16:51:30.952550: step 5006, loss 0.935344, acc 0.75, learning_rate 0.00044407\n","2023-06-13T16:51:32.411552: step 5007, loss 0.783996, acc 0.6875, learning_rate 0.000443903\n","2023-06-13T16:51:33.842856: step 5008, loss 0.793518, acc 0.71875, learning_rate 0.000443736\n","2023-06-13T16:51:35.291323: step 5009, loss 0.492362, acc 0.875, learning_rate 0.00044357\n","2023-06-13T16:51:37.535505: step 5010, loss 0.536727, acc 0.875, learning_rate 0.000443403\n","2023-06-13T16:51:40.081785: step 5011, loss 0.925445, acc 0.75, learning_rate 0.000443236\n","2023-06-13T16:51:42.509101: step 5012, loss 0.583704, acc 0.75, learning_rate 0.00044307\n","2023-06-13T16:51:44.878360: step 5013, loss 0.696826, acc 0.75, learning_rate 0.000442904\n","2023-06-13T16:51:46.717698: step 5014, loss 0.754139, acc 0.8125, learning_rate 0.000442737\n","2023-06-13T16:51:48.186443: step 5015, loss 0.767039, acc 0.75, learning_rate 0.000442571\n","2023-06-13T16:51:49.647425: step 5016, loss 0.58463, acc 0.78125, learning_rate 0.000442405\n","2023-06-13T16:51:51.123326: step 5017, loss 0.802351, acc 0.6875, learning_rate 0.000442239\n","2023-06-13T16:51:52.578738: step 5018, loss 0.617829, acc 0.71875, learning_rate 0.000442073\n","2023-06-13T16:51:54.054177: step 5019, loss 0.571077, acc 0.75, learning_rate 0.000441907\n","2023-06-13T16:51:55.513583: step 5020, loss 0.386352, acc 0.875, learning_rate 0.000441742\n","2023-06-13T16:51:57.541381: step 5021, loss 0.663302, acc 0.84375, learning_rate 0.000441576\n","2023-06-13T16:52:00.223646: step 5022, loss 0.830125, acc 0.71875, learning_rate 0.00044141\n","2023-06-13T16:52:02.688034: step 5023, loss 0.707113, acc 0.71875, learning_rate 0.000441245\n","2023-06-13T16:52:05.158604: step 5024, loss 0.598208, acc 0.8125, learning_rate 0.000441079\n","2023-06-13T16:52:07.017296: step 5025, loss 0.783792, acc 0.75, learning_rate 0.000440914\n","2023-06-13T16:52:08.481649: step 5026, loss 0.406128, acc 0.875, learning_rate 0.000440749\n","2023-06-13T16:52:09.990550: step 5027, loss 0.452648, acc 0.8125, learning_rate 0.000440583\n","2023-06-13T16:52:11.485108: step 5028, loss 0.392046, acc 0.875, learning_rate 0.000440418\n","2023-06-13T16:52:13.013639: step 5029, loss 0.904217, acc 0.65625, learning_rate 0.000440253\n","2023-06-13T16:52:14.498200: step 5030, loss 0.509213, acc 0.78125, learning_rate 0.000440088\n","2023-06-13T16:52:16.019269: step 5031, loss 0.620304, acc 0.78125, learning_rate 0.000439923\n","2023-06-13T16:52:18.545689: step 5032, loss 0.667982, acc 0.78125, learning_rate 0.000439758\n","2023-06-13T16:52:21.164267: step 5033, loss 0.615697, acc 0.8125, learning_rate 0.000439594\n","2023-06-13T16:52:23.718476: step 5034, loss 0.528337, acc 0.8125, learning_rate 0.000439429\n","2023-06-13T16:52:26.216791: step 5035, loss 0.512717, acc 0.84375, learning_rate 0.000439264\n","2023-06-13T16:52:27.742021: step 5036, loss 0.775037, acc 0.875, learning_rate 0.0004391\n","2023-06-13T16:52:29.229739: step 5037, loss 0.693907, acc 0.78125, learning_rate 0.000438935\n","2023-06-13T16:52:30.663477: step 5038, loss 0.606247, acc 0.75, learning_rate 0.000438771\n","2023-06-13T16:52:32.116096: step 5039, loss 0.914903, acc 0.71875, learning_rate 0.000438607\n","2023-06-13T16:52:33.624440: step 5040, loss 0.848319, acc 0.75, learning_rate 0.000438443\n","2023-06-13T16:52:35.105610: step 5041, loss 0.845808, acc 0.59375, learning_rate 0.000438279\n","2023-06-13T16:52:36.723851: step 5042, loss 0.468748, acc 0.84375, learning_rate 0.000438115\n","2023-06-13T16:52:39.304448: step 5043, loss 0.55646, acc 0.8125, learning_rate 0.000437951\n","2023-06-13T16:52:41.664131: step 5044, loss 0.897981, acc 0.71875, learning_rate 0.000437787\n","2023-06-13T16:52:44.045961: step 5045, loss 0.504889, acc 0.875, learning_rate 0.000437623\n","2023-06-13T16:52:46.352138: step 5046, loss 0.538657, acc 0.84375, learning_rate 0.000437459\n","2023-06-13T16:52:48.054356: step 5047, loss 0.554216, acc 0.84375, learning_rate 0.000437296\n","2023-06-13T16:52:49.582170: step 5048, loss 0.884384, acc 0.625, learning_rate 0.000437132\n","2023-06-13T16:52:51.019886: step 5049, loss 0.638842, acc 0.75, learning_rate 0.000436969\n","2023-06-13T16:52:52.461141: step 5050, loss 0.797283, acc 0.71875, learning_rate 0.000436805\n","2023-06-13T16:52:53.875503: step 5051, loss 0.601636, acc 0.8125, learning_rate 0.000436642\n","2023-06-13T16:52:55.335039: step 5052, loss 0.548349, acc 0.8125, learning_rate 0.000436479\n","2023-06-13T16:52:56.766371: step 5053, loss 0.537223, acc 0.84375, learning_rate 0.000436316\n","2023-06-13T16:52:59.002901: step 5054, loss 0.520781, acc 0.84375, learning_rate 0.000436152\n","2023-06-13T16:53:01.589757: step 5055, loss 0.522494, acc 0.84375, learning_rate 0.000435989\n","2023-06-13T16:53:03.952074: step 5056, loss 0.541303, acc 0.84375, learning_rate 0.000435827\n","2023-06-13T16:53:06.373803: step 5057, loss 0.719201, acc 0.78125, learning_rate 0.000435664\n","2023-06-13T16:53:08.228611: step 5058, loss 0.419984, acc 0.90625, learning_rate 0.000435501\n","2023-06-13T16:53:09.688288: step 5059, loss 0.547954, acc 0.78125, learning_rate 0.000435338\n","2023-06-13T16:53:11.146649: step 5060, loss 0.349683, acc 0.9375, learning_rate 0.000435176\n","2023-06-13T16:53:12.621693: step 5061, loss 0.328753, acc 0.90625, learning_rate 0.000435013\n","2023-06-13T16:53:14.096270: step 5062, loss 0.598588, acc 0.78125, learning_rate 0.000434851\n","2023-06-13T16:53:15.572209: step 5063, loss 0.460235, acc 0.8125, learning_rate 0.000434688\n","2023-06-13T16:53:17.043837: step 5064, loss 0.439766, acc 0.90625, learning_rate 0.000434526\n","2023-06-13T16:53:19.331421: step 5065, loss 0.715252, acc 0.78125, learning_rate 0.000434364\n","2023-06-13T16:53:21.852911: step 5066, loss 0.505651, acc 0.8125, learning_rate 0.000434202\n","2023-06-13T16:53:24.301550: step 5067, loss 0.506124, acc 0.84375, learning_rate 0.00043404\n","2023-06-13T16:53:26.777392: step 5068, loss 0.466866, acc 0.78125, learning_rate 0.000433878\n","2023-06-13T16:53:28.483631: step 5069, loss 0.519009, acc 0.78125, learning_rate 0.000433716\n","2023-06-13T16:53:29.957991: step 5070, loss 0.59849, acc 0.84375, learning_rate 0.000433554\n","2023-06-13T16:53:31.444936: step 5071, loss 0.811382, acc 0.71875, learning_rate 0.000433392\n","2023-06-13T16:53:32.877123: step 5072, loss 0.359307, acc 0.875, learning_rate 0.000433231\n","2023-06-13T16:53:34.318014: step 5073, loss 0.439792, acc 0.90625, learning_rate 0.000433069\n","2023-06-13T16:53:35.774751: step 5074, loss 0.634745, acc 0.78125, learning_rate 0.000432908\n","2023-06-13T16:53:37.214400: step 5075, loss 0.591745, acc 0.90625, learning_rate 0.000432746\n","2023-06-13T16:53:39.561615: step 5076, loss 0.376604, acc 0.90625, learning_rate 0.000432585\n","2023-06-13T16:53:42.052282: step 5077, loss 0.617712, acc 0.71875, learning_rate 0.000432424\n","2023-06-13T16:53:44.375699: step 5078, loss 0.617551, acc 0.8125, learning_rate 0.000432262\n","2023-06-13T16:53:46.850336: step 5079, loss 0.539371, acc 0.8125, learning_rate 0.000432101\n","2023-06-13T16:53:48.696743: step 5080, loss 0.649606, acc 0.8125, learning_rate 0.00043194\n","2023-06-13T16:53:50.147621: step 5081, loss 0.743983, acc 0.75, learning_rate 0.000431779\n","2023-06-13T16:53:51.618338: step 5082, loss 0.740213, acc 0.6875, learning_rate 0.000431618\n","2023-06-13T16:53:53.134257: step 5083, loss 0.761149, acc 0.78125, learning_rate 0.000431458\n","2023-06-13T16:53:54.604393: step 5084, loss 0.5675, acc 0.875, learning_rate 0.000431297\n","2023-06-13T16:53:56.057741: step 5085, loss 0.592307, acc 0.8125, learning_rate 0.000431136\n","2023-06-13T16:53:57.592116: step 5086, loss 0.567529, acc 0.84375, learning_rate 0.000430976\n","2023-06-13T16:54:00.000994: step 5087, loss 0.813978, acc 0.75, learning_rate 0.000430815\n","2023-06-13T16:54:02.600024: step 5088, loss 0.229172, acc 0.9375, learning_rate 0.000430655\n","2023-06-13T16:54:05.052089: step 5089, loss 0.342913, acc 0.875, learning_rate 0.000430494\n","2023-06-13T16:54:07.438025: step 5090, loss 0.772983, acc 0.6875, learning_rate 0.000430334\n","2023-06-13T16:54:09.223667: step 5091, loss 1.0374, acc 0.65625, learning_rate 0.000430174\n","2023-06-13T16:54:10.668466: step 5092, loss 0.526452, acc 0.78125, learning_rate 0.000430014\n","2023-06-13T16:54:12.160523: step 5093, loss 0.508588, acc 0.8125, learning_rate 0.000429854\n","2023-06-13T16:54:13.628141: step 5094, loss 0.958754, acc 0.71875, learning_rate 0.000429694\n","2023-06-13T16:54:15.089973: step 5095, loss 0.809927, acc 0.75, learning_rate 0.000429534\n","2023-06-13T16:54:16.573105: step 5096, loss 0.793921, acc 0.71875, learning_rate 0.000429374\n","2023-06-13T16:54:18.056931: step 5097, loss 0.593047, acc 0.84375, learning_rate 0.000429215\n","2023-06-13T16:54:20.422578: step 5098, loss 0.6984, acc 0.78125, learning_rate 0.000429055\n","2023-06-13T16:54:22.928947: step 5099, loss 0.652567, acc 0.71875, learning_rate 0.000428895\n","\n","Evaluation:\n","2023-06-13T16:54:53.329231: step 5100, loss 2.58859, acc 0.383904\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5100\n","\n","2023-06-13T16:54:54.977724: step 5100, loss 0.888107, acc 0.71875, learning_rate 0.000428736\n","2023-06-13T16:54:56.459588: step 5101, loss 0.606173, acc 0.875, learning_rate 0.000428577\n","2023-06-13T16:54:57.936897: step 5102, loss 0.791193, acc 0.75, learning_rate 0.000428417\n","2023-06-13T16:54:59.384687: step 5103, loss 0.582063, acc 0.8125, learning_rate 0.000428258\n","2023-06-13T16:55:01.812157: step 5104, loss 0.495132, acc 0.8125, learning_rate 0.000428099\n","2023-06-13T16:55:04.313130: step 5105, loss 0.786284, acc 0.8125, learning_rate 0.00042794\n","2023-06-13T16:55:06.711350: step 5106, loss 0.733614, acc 0.6875, learning_rate 0.000427781\n","2023-06-13T16:55:09.107752: step 5107, loss 0.895353, acc 0.78125, learning_rate 0.000427622\n","2023-06-13T16:55:10.958307: step 5108, loss 0.63325, acc 0.78125, learning_rate 0.000427463\n","2023-06-13T16:55:12.405118: step 5109, loss 0.681763, acc 0.78125, learning_rate 0.000427304\n","2023-06-13T16:55:13.859750: step 5110, loss 0.554869, acc 0.78125, learning_rate 0.000427146\n","2023-06-13T16:55:15.300329: step 5111, loss 0.752563, acc 0.78125, learning_rate 0.000426987\n","2023-06-13T16:55:16.763002: step 5112, loss 0.719616, acc 0.75, learning_rate 0.000426828\n","2023-06-13T16:55:18.196530: step 5113, loss 0.435395, acc 0.90625, learning_rate 0.00042667\n","2023-06-13T16:55:19.662290: step 5114, loss 0.573128, acc 0.84375, learning_rate 0.000426511\n","2023-06-13T16:55:21.852125: step 5115, loss 0.565894, acc 0.90625, learning_rate 0.000426353\n","2023-06-13T16:55:24.389942: step 5116, loss 0.539357, acc 0.8125, learning_rate 0.000426195\n","2023-06-13T16:55:26.759051: step 5117, loss 0.696429, acc 0.65625, learning_rate 0.000426037\n","2023-06-13T16:55:29.068382: step 5118, loss 0.679568, acc 0.65625, learning_rate 0.000425879\n","2023-06-13T16:55:31.080167: step 5119, loss 0.552382, acc 0.75, learning_rate 0.000425721\n","2023-06-13T16:55:32.518679: step 5120, loss 0.587238, acc 0.84375, learning_rate 0.000425563\n","2023-06-13T16:55:33.988071: step 5121, loss 0.495343, acc 0.875, learning_rate 0.000425405\n","2023-06-13T16:55:35.418959: step 5122, loss 0.572168, acc 0.84375, learning_rate 0.000425247\n","2023-06-13T16:55:36.837572: step 5123, loss 0.8088, acc 0.75, learning_rate 0.000425089\n","2023-06-13T16:55:38.307050: step 5124, loss 0.927657, acc 0.78125, learning_rate 0.000424932\n","2023-06-13T16:55:39.771437: step 5125, loss 0.569523, acc 0.78125, learning_rate 0.000424774\n","2023-06-13T16:55:41.624853: step 5126, loss 0.424614, acc 0.84375, learning_rate 0.000424617\n","2023-06-13T16:55:44.214553: step 5127, loss 0.467581, acc 0.78125, learning_rate 0.000424459\n","2023-06-13T16:55:46.621692: step 5128, loss 0.827545, acc 0.6875, learning_rate 0.000424302\n","2023-06-13T16:55:48.906548: step 5129, loss 0.216696, acc 0.90625, learning_rate 0.000424145\n","2023-06-13T16:55:51.221335: step 5130, loss 0.717987, acc 0.875, learning_rate 0.000423988\n","2023-06-13T16:55:52.671227: step 5131, loss 0.363695, acc 0.875, learning_rate 0.00042383\n","2023-06-13T16:55:54.108271: step 5132, loss 0.475502, acc 0.875, learning_rate 0.000423673\n","2023-06-13T16:55:55.538103: step 5133, loss 0.421189, acc 0.90625, learning_rate 0.000423517\n","2023-06-13T16:55:57.016628: step 5134, loss 0.43813, acc 0.875, learning_rate 0.00042336\n","2023-06-13T16:55:58.494804: step 5135, loss 0.491313, acc 0.84375, learning_rate 0.000423203\n","2023-06-13T16:55:59.960101: step 5136, loss 0.550986, acc 0.8125, learning_rate 0.000423046\n","2023-06-13T16:56:01.626616: step 5137, loss 0.501526, acc 0.84375, learning_rate 0.00042289\n","2023-06-13T16:56:04.193409: step 5138, loss 0.496074, acc 0.84375, learning_rate 0.000422733\n","2023-06-13T16:56:06.639267: step 5139, loss 0.656839, acc 0.78125, learning_rate 0.000422576\n","2023-06-13T16:56:09.083696: step 5140, loss 0.707371, acc 0.75, learning_rate 0.00042242\n","2023-06-13T16:56:11.357700: step 5141, loss 0.615173, acc 0.78125, learning_rate 0.000422264\n","2023-06-13T16:56:12.817750: step 5142, loss 0.638883, acc 0.75, learning_rate 0.000422107\n","2023-06-13T16:56:14.242901: step 5143, loss 0.653868, acc 0.8125, learning_rate 0.000421951\n","2023-06-13T16:56:15.725062: step 5144, loss 0.261485, acc 0.9375, learning_rate 0.000421795\n","2023-06-13T16:56:17.178184: step 5145, loss 0.559446, acc 0.875, learning_rate 0.000421639\n","2023-06-13T16:56:18.894709: step 5146, loss 0.673679, acc 0.8125, learning_rate 0.000421483\n","2023-06-13T16:56:21.564357: step 5147, loss 0.542927, acc 0.84375, learning_rate 0.000421327\n","2023-06-13T16:56:24.459401: step 5148, loss 0.488628, acc 0.78125, learning_rate 0.000421171\n","2023-06-13T16:56:27.251674: step 5149, loss 0.727404, acc 0.78125, learning_rate 0.000421016\n","2023-06-13T16:56:29.842035: step 5150, loss 0.7361, acc 0.78125, learning_rate 0.00042086\n","2023-06-13T16:56:32.499897: step 5151, loss 1.10546, acc 0.59375, learning_rate 0.000420705\n","2023-06-13T16:56:34.937735: step 5152, loss 0.567667, acc 0.78125, learning_rate 0.000420549\n","2023-06-13T16:56:37.036917: step 5153, loss 0.592561, acc 0.75, learning_rate 0.000420394\n","2023-06-13T16:56:38.495596: step 5154, loss 0.957918, acc 0.6875, learning_rate 0.000420238\n","2023-06-13T16:56:39.980780: step 5155, loss 0.392777, acc 0.875, learning_rate 0.000420083\n","2023-06-13T16:56:41.480922: step 5156, loss 0.299715, acc 0.9375, learning_rate 0.000419928\n","2023-06-13T16:56:42.922749: step 5157, loss 0.470589, acc 0.84375, learning_rate 0.000419773\n","2023-06-13T16:56:44.373584: step 5158, loss 0.823999, acc 0.71875, learning_rate 0.000419618\n","2023-06-13T16:56:45.817120: step 5159, loss 0.544921, acc 0.90625, learning_rate 0.000419463\n","2023-06-13T16:56:47.610951: step 5160, loss 0.661211, acc 0.75, learning_rate 0.000419308\n","2023-06-13T16:56:50.181076: step 5161, loss 0.331183, acc 0.875, learning_rate 0.000419153\n","2023-06-13T16:56:52.621088: step 5162, loss 0.610206, acc 0.71875, learning_rate 0.000418998\n","2023-06-13T16:56:55.010270: step 5163, loss 0.531288, acc 0.8125, learning_rate 0.000418843\n","2023-06-13T16:56:57.265442: step 5164, loss 0.61731, acc 0.8125, learning_rate 0.000418689\n","2023-06-13T16:56:58.740694: step 5165, loss 0.57304, acc 0.875, learning_rate 0.000418534\n","2023-06-13T16:57:00.188214: step 5166, loss 0.579287, acc 0.8125, learning_rate 0.00041838\n","2023-06-13T16:57:01.627752: step 5167, loss 0.569193, acc 0.8125, learning_rate 0.000418226\n","2023-06-13T16:57:03.082825: step 5168, loss 0.360229, acc 0.90625, learning_rate 0.000418071\n","2023-06-13T16:57:04.533824: step 5169, loss 0.571705, acc 0.84375, learning_rate 0.000417917\n","2023-06-13T16:57:05.985244: step 5170, loss 0.75677, acc 0.84375, learning_rate 0.000417763\n","2023-06-13T16:57:07.726551: step 5171, loss 0.456822, acc 0.8125, learning_rate 0.000417609\n","2023-06-13T16:57:10.333993: step 5172, loss 0.621127, acc 0.75, learning_rate 0.000417455\n","2023-06-13T16:57:12.649502: step 5173, loss 0.608849, acc 0.8125, learning_rate 0.000417301\n","2023-06-13T16:57:14.968301: step 5174, loss 0.883807, acc 0.78125, learning_rate 0.000417147\n","2023-06-13T16:57:17.176976: step 5175, loss 0.439953, acc 0.9375, learning_rate 0.000416993\n","2023-06-13T16:57:18.821468: step 5176, loss 0.616201, acc 0.78125, learning_rate 0.00041684\n","2023-06-13T16:57:20.290179: step 5177, loss 0.312362, acc 0.875, learning_rate 0.000416686\n","2023-06-13T16:57:21.733501: step 5178, loss 0.551713, acc 0.875, learning_rate 0.000416532\n","2023-06-13T16:57:23.167514: step 5179, loss 0.480728, acc 0.84375, learning_rate 0.000416379\n","2023-06-13T16:57:24.618135: step 5180, loss 0.635217, acc 0.75, learning_rate 0.000416225\n","2023-06-13T16:57:26.088113: step 5181, loss 0.436879, acc 0.84375, learning_rate 0.000416072\n","2023-06-13T16:57:27.546492: step 5182, loss 0.419453, acc 0.875, learning_rate 0.000415919\n","2023-06-13T16:57:30.012456: step 5183, loss 0.854207, acc 0.75, learning_rate 0.000415766\n","2023-06-13T16:57:32.458084: step 5184, loss 0.782991, acc 0.75, learning_rate 0.000415613\n","2023-06-13T16:57:34.830980: step 5185, loss 0.565159, acc 0.8125, learning_rate 0.00041546\n","2023-06-13T16:57:37.105194: step 5186, loss 0.736479, acc 0.78125, learning_rate 0.000415307\n","2023-06-13T16:57:39.001035: step 5187, loss 0.5158, acc 0.84375, learning_rate 0.000415154\n","2023-06-13T16:57:40.433085: step 5188, loss 0.343402, acc 0.90625, learning_rate 0.000415001\n","2023-06-13T16:57:41.890944: step 5189, loss 0.699814, acc 0.75, learning_rate 0.000414848\n","2023-06-13T16:57:43.348232: step 5190, loss 0.342944, acc 0.90625, learning_rate 0.000414695\n","2023-06-13T16:57:44.812077: step 5191, loss 0.815942, acc 0.71875, learning_rate 0.000414543\n","2023-06-13T16:57:46.263936: step 5192, loss 0.694132, acc 0.65625, learning_rate 0.00041439\n","2023-06-13T16:57:47.712894: step 5193, loss 0.827372, acc 0.8125, learning_rate 0.000414238\n","2023-06-13T16:57:49.691330: step 5194, loss 0.570132, acc 0.875, learning_rate 0.000414086\n","2023-06-13T16:57:52.269512: step 5195, loss 0.81544, acc 0.625, learning_rate 0.000413933\n","2023-06-13T16:57:54.603717: step 5196, loss 1.06516, acc 0.75, learning_rate 0.000413781\n","2023-06-13T16:57:56.897147: step 5197, loss 0.566866, acc 0.75, learning_rate 0.000413629\n","2023-06-13T16:57:59.187258: step 5198, loss 0.680432, acc 0.71875, learning_rate 0.000413477\n","2023-06-13T16:58:00.651122: step 5199, loss 0.397908, acc 0.84375, learning_rate 0.000413325\n","\n","Evaluation:\n","2023-06-13T16:58:28.387125: step 5200, loss 2.61231, acc 0.385117\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5200\n","\n","2023-06-13T16:58:30.066379: step 5200, loss 0.560311, acc 0.8125, learning_rate 0.000413173\n","2023-06-13T16:58:32.660584: step 5201, loss 0.395995, acc 0.84375, learning_rate 0.000413021\n","2023-06-13T16:58:35.013577: step 5202, loss 0.707035, acc 0.78125, learning_rate 0.000412869\n","2023-06-13T16:58:37.592584: step 5203, loss 0.691513, acc 0.75, learning_rate 0.000412718\n","2023-06-13T16:58:39.895724: step 5204, loss 0.651259, acc 0.8125, learning_rate 0.000412566\n","2023-06-13T16:58:41.466205: step 5205, loss 0.555731, acc 0.78125, learning_rate 0.000412414\n","2023-06-13T16:58:42.918628: step 5206, loss 0.453821, acc 0.8125, learning_rate 0.000412263\n","2023-06-13T16:58:44.375049: step 5207, loss 0.714132, acc 0.78125, learning_rate 0.000412112\n","2023-06-13T16:58:45.840487: step 5208, loss 0.508141, acc 0.8125, learning_rate 0.00041196\n","2023-06-13T16:58:47.309349: step 5209, loss 0.715864, acc 0.75, learning_rate 0.000411809\n","2023-06-13T16:58:48.734756: step 5210, loss 0.684968, acc 0.6875, learning_rate 0.000411658\n","2023-06-13T16:58:50.189086: step 5211, loss 0.481335, acc 0.84375, learning_rate 0.000411507\n","2023-06-13T16:58:52.557662: step 5212, loss 0.547866, acc 0.84375, learning_rate 0.000411356\n","2023-06-13T16:58:55.089147: step 5213, loss 0.847973, acc 0.65625, learning_rate 0.000411205\n","2023-06-13T16:58:57.362722: step 5214, loss 0.807026, acc 0.8125, learning_rate 0.000411054\n","2023-06-13T16:58:59.709972: step 5215, loss 0.887384, acc 0.71875, learning_rate 0.000410903\n","2023-06-13T16:59:01.622695: step 5216, loss 0.757611, acc 0.71875, learning_rate 0.000410752\n","2023-06-13T16:59:03.071057: step 5217, loss 0.609547, acc 0.8125, learning_rate 0.000410601\n","2023-06-13T16:59:04.498988: step 5218, loss 0.819549, acc 0.625, learning_rate 0.000410451\n","2023-06-13T16:59:05.969699: step 5219, loss 0.511238, acc 0.75, learning_rate 0.0004103\n","2023-06-13T16:59:07.445515: step 5220, loss 0.424362, acc 0.78125, learning_rate 0.00041015\n","2023-06-13T16:59:08.909488: step 5221, loss 0.52763, acc 0.78125, learning_rate 0.000409999\n","2023-06-13T16:59:10.398412: step 5222, loss 0.786791, acc 0.71875, learning_rate 0.000409849\n","2023-06-13T16:59:12.517624: step 5223, loss 0.691029, acc 0.78125, learning_rate 0.000409699\n","2023-06-13T16:59:15.094559: step 5224, loss 0.446174, acc 0.8125, learning_rate 0.000409549\n","2023-06-13T16:59:17.514072: step 5225, loss 0.478645, acc 0.8125, learning_rate 0.000409399\n","2023-06-13T16:59:19.958646: step 5226, loss 0.424744, acc 0.875, learning_rate 0.000409249\n","2023-06-13T16:59:21.878897: step 5227, loss 0.425009, acc 0.875, learning_rate 0.000409099\n","2023-06-13T16:59:23.352814: step 5228, loss 0.480832, acc 0.8125, learning_rate 0.000408949\n","2023-06-13T16:59:24.791577: step 5229, loss 0.776391, acc 0.71875, learning_rate 0.000408799\n","2023-06-13T16:59:26.253361: step 5230, loss 0.359592, acc 0.9375, learning_rate 0.000408649\n","2023-06-13T16:59:27.662952: step 5231, loss 0.676239, acc 0.78125, learning_rate 0.0004085\n","2023-06-13T16:59:29.150599: step 5232, loss 0.795858, acc 0.6875, learning_rate 0.00040835\n","2023-06-13T16:59:30.574845: step 5233, loss 0.436106, acc 0.9375, learning_rate 0.000408201\n","2023-06-13T16:59:32.687573: step 5234, loss 0.776756, acc 0.71875, learning_rate 0.000408051\n","2023-06-13T16:59:35.280895: step 5235, loss 0.367391, acc 0.9375, learning_rate 0.000407902\n","2023-06-13T16:59:37.661479: step 5236, loss 0.920404, acc 0.6875, learning_rate 0.000407752\n","2023-06-13T16:59:40.029492: step 5237, loss 0.648258, acc 0.71875, learning_rate 0.000407603\n","2023-06-13T16:59:41.973631: step 5238, loss 0.712343, acc 0.71875, learning_rate 0.000407454\n","2023-06-13T16:59:43.413222: step 5239, loss 0.305038, acc 0.9375, learning_rate 0.000407305\n","2023-06-13T16:59:44.862421: step 5240, loss 0.581213, acc 0.78125, learning_rate 0.000407156\n","2023-06-13T16:59:46.310008: step 5241, loss 0.396472, acc 0.875, learning_rate 0.000407007\n","2023-06-13T16:59:47.752758: step 5242, loss 1.06937, acc 0.75, learning_rate 0.000406858\n","2023-06-13T16:59:49.223466: step 5243, loss 0.53603, acc 0.84375, learning_rate 0.000406709\n","2023-06-13T16:59:50.677568: step 5244, loss 0.747319, acc 0.71875, learning_rate 0.000406561\n","2023-06-13T16:59:52.676949: step 5245, loss 0.698037, acc 0.78125, learning_rate 0.000406412\n","2023-06-13T16:59:55.246277: step 5246, loss 0.43544, acc 0.84375, learning_rate 0.000406263\n","2023-06-13T16:59:57.613784: step 5247, loss 0.373841, acc 0.8125, learning_rate 0.000406115\n","2023-06-13T16:59:59.968973: step 5248, loss 0.881669, acc 0.6875, learning_rate 0.000405967\n","2023-06-13T17:00:02.197426: step 5249, loss 0.66115, acc 0.78125, learning_rate 0.000405818\n","2023-06-13T17:00:03.660777: step 5250, loss 0.478214, acc 0.8125, learning_rate 0.00040567\n","2023-06-13T17:00:05.144810: step 5251, loss 0.721894, acc 0.8125, learning_rate 0.000405522\n","2023-06-13T17:00:06.623485: step 5252, loss 0.662189, acc 0.78125, learning_rate 0.000405374\n","2023-06-13T17:00:08.083780: step 5253, loss 0.84786, acc 0.65625, learning_rate 0.000405225\n","2023-06-13T17:00:09.541102: step 5254, loss 0.524518, acc 0.8125, learning_rate 0.000405077\n","2023-06-13T17:00:10.980498: step 5255, loss 1.29191, acc 0.65625, learning_rate 0.00040493\n","2023-06-13T17:00:12.968921: step 5256, loss 0.81951, acc 0.78125, learning_rate 0.000404782\n","2023-06-13T17:00:15.573286: step 5257, loss 0.514364, acc 0.8125, learning_rate 0.000404634\n","2023-06-13T17:00:18.053965: step 5258, loss 0.561598, acc 0.78125, learning_rate 0.000404486\n","2023-06-13T17:00:20.428773: step 5259, loss 0.572386, acc 0.84375, learning_rate 0.000404339\n","2023-06-13T17:00:22.502083: step 5260, loss 1.06474, acc 0.71875, learning_rate 0.000404191\n","2023-06-13T17:00:23.941636: step 5261, loss 0.770367, acc 0.75, learning_rate 0.000404043\n","2023-06-13T17:00:25.406254: step 5262, loss 0.662304, acc 0.8125, learning_rate 0.000403896\n","2023-06-13T17:00:26.841168: step 5263, loss 0.748755, acc 0.78125, learning_rate 0.000403749\n","2023-06-13T17:00:28.291012: step 5264, loss 0.571296, acc 0.78125, learning_rate 0.000403601\n","2023-06-13T17:00:29.792434: step 5265, loss 0.366835, acc 0.875, learning_rate 0.000403454\n","2023-06-13T17:00:31.240098: step 5266, loss 0.415753, acc 0.875, learning_rate 0.000403307\n","2023-06-13T17:00:33.068324: step 5267, loss 0.877742, acc 0.625, learning_rate 0.00040316\n","2023-06-13T17:00:35.725434: step 5268, loss 0.717518, acc 0.78125, learning_rate 0.000403013\n","2023-06-13T17:00:38.132525: step 5269, loss 0.566775, acc 0.875, learning_rate 0.000402866\n","2023-06-13T17:00:40.584001: step 5270, loss 0.392922, acc 0.84375, learning_rate 0.000402719\n","2023-06-13T17:00:42.699747: step 5271, loss 0.839221, acc 0.71875, learning_rate 0.000402572\n","2023-06-13T17:00:44.169539: step 5272, loss 0.734147, acc 0.75, learning_rate 0.000402426\n","2023-06-13T17:00:45.628104: step 5273, loss 0.53969, acc 0.75, learning_rate 0.000402279\n","2023-06-13T17:00:47.064638: step 5274, loss 0.691436, acc 0.78125, learning_rate 0.000402133\n","2023-06-13T17:00:48.508523: step 5275, loss 0.364337, acc 0.875, learning_rate 0.000401986\n","2023-06-13T17:00:49.960059: step 5276, loss 0.799725, acc 0.75, learning_rate 0.00040184\n","2023-06-13T17:00:51.388664: step 5277, loss 0.772679, acc 0.78125, learning_rate 0.000401693\n","2023-06-13T17:00:53.296512: step 5278, loss 0.331748, acc 0.875, learning_rate 0.000401547\n","2023-06-13T17:00:55.877225: step 5279, loss 0.698896, acc 0.8125, learning_rate 0.000401401\n","2023-06-13T17:00:58.422763: step 5280, loss 0.667634, acc 0.8125, learning_rate 0.000401255\n","2023-06-13T17:01:00.823270: step 5281, loss 0.267418, acc 0.96875, learning_rate 0.000401109\n","2023-06-13T17:01:02.906618: step 5282, loss 0.886515, acc 0.71875, learning_rate 0.000400963\n","2023-06-13T17:01:04.352504: step 5283, loss 0.759689, acc 0.78125, learning_rate 0.000400817\n","2023-06-13T17:01:05.794308: step 5284, loss 0.731884, acc 0.78125, learning_rate 0.000400671\n","2023-06-13T17:01:07.247194: step 5285, loss 0.795174, acc 0.75, learning_rate 0.000400525\n","2023-06-13T17:01:08.703981: step 5286, loss 0.242281, acc 0.96875, learning_rate 0.000400379\n","2023-06-13T17:01:10.186125: step 5287, loss 0.749096, acc 0.65625, learning_rate 0.000400234\n","2023-06-13T17:01:11.637895: step 5288, loss 0.573818, acc 0.78125, learning_rate 0.000400088\n","2023-06-13T17:01:13.524239: step 5289, loss 0.499179, acc 0.78125, learning_rate 0.000399943\n","2023-06-13T17:01:16.177914: step 5290, loss 0.476656, acc 0.84375, learning_rate 0.000399797\n","2023-06-13T17:01:18.502821: step 5291, loss 0.590849, acc 0.78125, learning_rate 0.000399652\n","2023-06-13T17:01:20.838583: step 5292, loss 0.590477, acc 0.78125, learning_rate 0.000399506\n","2023-06-13T17:01:23.092187: step 5293, loss 0.430527, acc 0.84375, learning_rate 0.000399361\n","2023-06-13T17:01:25.435993: step 5294, loss 0.621267, acc 0.8125, learning_rate 0.000399216\n","2023-06-13T17:01:27.902979: step 5295, loss 0.812446, acc 0.8125, learning_rate 0.000399071\n","2023-06-13T17:01:30.346036: step 5296, loss 0.575273, acc 0.75, learning_rate 0.000398926\n","2023-06-13T17:01:32.650117: step 5297, loss 0.40551, acc 0.875, learning_rate 0.000398781\n","2023-06-13T17:01:35.303004: step 5298, loss 0.523935, acc 0.8125, learning_rate 0.000398636\n","2023-06-13T17:01:37.849116: step 5299, loss 0.758506, acc 0.78125, learning_rate 0.000398491\n","\n","Evaluation:\n","2023-06-13T17:02:08.627682: step 5300, loss 2.62895, acc 0.385571\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5300\n","\n","2023-06-13T17:02:10.314381: step 5300, loss 0.441102, acc 0.84375, learning_rate 0.000398347\n","2023-06-13T17:02:11.762838: step 5301, loss 0.571283, acc 0.8125, learning_rate 0.000398202\n","2023-06-13T17:02:13.236920: step 5302, loss 0.502244, acc 0.8125, learning_rate 0.000398057\n","2023-06-13T17:02:14.662440: step 5303, loss 0.685709, acc 0.6875, learning_rate 0.000397913\n","2023-06-13T17:02:16.796952: step 5304, loss 0.594486, acc 0.75, learning_rate 0.000397768\n","2023-06-13T17:02:19.483018: step 5305, loss 0.515069, acc 0.75, learning_rate 0.000397624\n","2023-06-13T17:02:21.815613: step 5306, loss 0.52612, acc 0.8125, learning_rate 0.00039748\n","2023-06-13T17:02:24.189929: step 5307, loss 0.54002, acc 0.875, learning_rate 0.000397335\n","2023-06-13T17:02:26.346266: step 5308, loss 0.325355, acc 0.9375, learning_rate 0.000397191\n","2023-06-13T17:02:27.838730: step 5309, loss 0.50565, acc 0.84375, learning_rate 0.000397047\n","2023-06-13T17:02:29.327877: step 5310, loss 0.634385, acc 0.8125, learning_rate 0.000396903\n","2023-06-13T17:02:30.830583: step 5311, loss 0.592566, acc 0.84375, learning_rate 0.000396759\n","2023-06-13T17:02:32.329306: step 5312, loss 0.258828, acc 0.9375, learning_rate 0.000396615\n","2023-06-13T17:02:33.825723: step 5313, loss 0.638906, acc 0.875, learning_rate 0.000396471\n","2023-06-13T17:02:35.290983: step 5314, loss 0.451843, acc 0.8125, learning_rate 0.000396328\n","2023-06-13T17:02:37.289542: step 5315, loss 0.746513, acc 0.625, learning_rate 0.000396184\n","2023-06-13T17:02:39.834601: step 5316, loss 0.567217, acc 0.84375, learning_rate 0.00039604\n","2023-06-13T17:02:42.230589: step 5317, loss 0.635974, acc 0.78125, learning_rate 0.000395897\n","2023-06-13T17:02:44.593191: step 5318, loss 0.677361, acc 0.78125, learning_rate 0.000395753\n","2023-06-13T17:02:46.905553: step 5319, loss 0.486198, acc 0.84375, learning_rate 0.00039561\n","2023-06-13T17:02:48.424526: step 5320, loss 0.791715, acc 0.75, learning_rate 0.000395467\n","2023-06-13T17:02:49.896345: step 5321, loss 0.683089, acc 0.78125, learning_rate 0.000395323\n","2023-06-13T17:02:51.367154: step 5322, loss 0.402908, acc 0.90625, learning_rate 0.00039518\n","2023-06-13T17:02:52.827618: step 5323, loss 0.582805, acc 0.84375, learning_rate 0.000395037\n","2023-06-13T17:02:54.248123: step 5324, loss 0.590067, acc 0.75, learning_rate 0.000394894\n","2023-06-13T17:02:55.684540: step 5325, loss 0.542702, acc 0.875, learning_rate 0.000394751\n","2023-06-13T17:02:57.497515: step 5326, loss 0.627465, acc 0.78125, learning_rate 0.000394608\n","2023-06-13T17:03:00.074130: step 5327, loss 0.65578, acc 0.84375, learning_rate 0.000394465\n","2023-06-13T17:03:02.411889: step 5328, loss 0.57843, acc 0.8125, learning_rate 0.000394322\n","2023-06-13T17:03:04.716326: step 5329, loss 0.686089, acc 0.84375, learning_rate 0.00039418\n","2023-06-13T17:03:07.227071: step 5330, loss 0.526926, acc 0.875, learning_rate 0.000394037\n","2023-06-13T17:03:08.756329: step 5331, loss 0.684181, acc 0.75, learning_rate 0.000393895\n","2023-06-13T17:03:10.231953: step 5332, loss 0.338537, acc 0.875, learning_rate 0.000393752\n","2023-06-13T17:03:11.678123: step 5333, loss 0.387033, acc 0.875, learning_rate 0.00039361\n","2023-06-13T17:03:13.147773: step 5334, loss 0.60037, acc 0.8125, learning_rate 0.000393467\n","2023-06-13T17:03:14.563247: step 5335, loss 0.721306, acc 0.65625, learning_rate 0.000393325\n","2023-06-13T17:03:16.005503: step 5336, loss 0.637252, acc 0.78125, learning_rate 0.000393183\n","2023-06-13T17:03:17.442538: step 5337, loss 0.592106, acc 0.78125, learning_rate 0.000393041\n","2023-06-13T17:03:19.915649: step 5338, loss 0.64351, acc 0.84375, learning_rate 0.000392898\n","2023-06-13T17:03:22.400771: step 5339, loss 0.595876, acc 0.75, learning_rate 0.000392756\n","2023-06-13T17:03:24.826965: step 5340, loss 0.649601, acc 0.8125, learning_rate 0.000392615\n","2023-06-13T17:03:27.416999: step 5341, loss 0.5436, acc 0.84375, learning_rate 0.000392473\n","2023-06-13T17:03:29.047484: step 5342, loss 0.872202, acc 0.75, learning_rate 0.000392331\n","2023-06-13T17:03:30.535271: step 5343, loss 0.92743, acc 0.71875, learning_rate 0.000392189\n","2023-06-13T17:03:32.038351: step 5344, loss 0.633858, acc 0.84375, learning_rate 0.000392047\n","2023-06-13T17:03:33.461850: step 5345, loss 0.900378, acc 0.75, learning_rate 0.000391906\n","2023-06-13T17:03:34.924645: step 5346, loss 0.870409, acc 0.8125, learning_rate 0.000391764\n","2023-06-13T17:03:36.352404: step 5347, loss 0.850473, acc 0.78125, learning_rate 0.000391623\n","2023-06-13T17:03:37.875022: step 5348, loss 0.746006, acc 0.78125, learning_rate 0.000391481\n","2023-06-13T17:03:40.444152: step 5349, loss 0.60206, acc 0.78125, learning_rate 0.00039134\n","2023-06-13T17:03:42.995617: step 5350, loss 0.312391, acc 0.90625, learning_rate 0.000391199\n","2023-06-13T17:03:45.303737: step 5351, loss 0.425165, acc 0.90625, learning_rate 0.000391058\n","2023-06-13T17:03:47.729212: step 5352, loss 0.820772, acc 0.75, learning_rate 0.000390916\n","2023-06-13T17:03:49.359974: step 5353, loss 0.689105, acc 0.75, learning_rate 0.000390775\n","2023-06-13T17:03:50.828499: step 5354, loss 0.842221, acc 0.8125, learning_rate 0.000390634\n","2023-06-13T17:03:52.302891: step 5355, loss 0.632683, acc 0.84375, learning_rate 0.000390493\n","2023-06-13T17:03:53.761927: step 5356, loss 0.515106, acc 0.78125, learning_rate 0.000390353\n","2023-06-13T17:03:55.199543: step 5357, loss 0.674031, acc 0.8125, learning_rate 0.000390212\n","2023-06-13T17:03:56.649819: step 5358, loss 0.554564, acc 0.8125, learning_rate 0.000390071\n","2023-06-13T17:03:58.117694: step 5359, loss 0.599977, acc 0.84375, learning_rate 0.00038993\n","2023-06-13T17:04:00.672971: step 5360, loss 0.143485, acc 0.96875, learning_rate 0.00038979\n","2023-06-13T17:04:03.171595: step 5361, loss 0.70054, acc 0.75, learning_rate 0.000389649\n","2023-06-13T17:04:05.484887: step 5362, loss 0.833197, acc 0.75, learning_rate 0.000389509\n","2023-06-13T17:04:07.922953: step 5363, loss 0.651266, acc 0.71875, learning_rate 0.000389369\n","2023-06-13T17:04:09.674882: step 5364, loss 0.480438, acc 0.84375, learning_rate 0.000389228\n","2023-06-13T17:04:11.147319: step 5365, loss 0.438039, acc 0.875, learning_rate 0.000389088\n","2023-06-13T17:04:12.594101: step 5366, loss 0.636711, acc 0.78125, learning_rate 0.000388948\n","2023-06-13T17:04:14.031152: step 5367, loss 0.778605, acc 0.6875, learning_rate 0.000388808\n","2023-06-13T17:04:15.481177: step 5368, loss 0.748463, acc 0.65625, learning_rate 0.000388668\n","2023-06-13T17:04:16.944446: step 5369, loss 0.51804, acc 0.8125, learning_rate 0.000388528\n","2023-06-13T17:04:18.420015: step 5370, loss 0.818185, acc 0.71875, learning_rate 0.000388388\n","2023-06-13T17:04:20.747262: step 5371, loss 0.182543, acc 0.96875, learning_rate 0.000388248\n","2023-06-13T17:04:23.382964: step 5372, loss 0.570884, acc 0.84375, learning_rate 0.000388108\n","2023-06-13T17:04:25.702060: step 5373, loss 0.539289, acc 0.84375, learning_rate 0.000387969\n","2023-06-13T17:04:28.148634: step 5374, loss 0.408275, acc 0.9375, learning_rate 0.000387829\n","2023-06-13T17:04:30.117443: step 5375, loss 0.574894, acc 0.78125, learning_rate 0.000387689\n","2023-06-13T17:04:31.615307: step 5376, loss 0.620624, acc 0.8125, learning_rate 0.00038755\n","2023-06-13T17:04:33.056736: step 5377, loss 0.525344, acc 0.78125, learning_rate 0.00038741\n","2023-06-13T17:04:34.511515: step 5378, loss 0.978556, acc 0.65625, learning_rate 0.000387271\n","2023-06-13T17:04:35.994222: step 5379, loss 0.453178, acc 0.875, learning_rate 0.000387132\n","2023-06-13T17:04:37.489124: step 5380, loss 0.60403, acc 0.8125, learning_rate 0.000386993\n","2023-06-13T17:04:38.963172: step 5381, loss 0.595673, acc 0.8125, learning_rate 0.000386853\n","2023-06-13T17:04:41.058178: step 5382, loss 0.62294, acc 0.84375, learning_rate 0.000386714\n","2023-06-13T17:04:43.589216: step 5383, loss 0.520397, acc 0.96875, learning_rate 0.000386575\n","2023-06-13T17:04:45.911222: step 5384, loss 0.416117, acc 0.84375, learning_rate 0.000386436\n","2023-06-13T17:04:48.278290: step 5385, loss 0.40217, acc 0.84375, learning_rate 0.000386297\n","2023-06-13T17:04:50.391567: step 5386, loss 0.342484, acc 0.875, learning_rate 0.000386159\n","2023-06-13T17:04:51.832598: step 5387, loss 0.698359, acc 0.75, learning_rate 0.00038602\n","2023-06-13T17:04:53.306501: step 5388, loss 0.597968, acc 0.78125, learning_rate 0.000385881\n","2023-06-13T17:04:54.768675: step 5389, loss 0.725402, acc 0.75, learning_rate 0.000385743\n","2023-06-13T17:04:56.229868: step 5390, loss 0.345393, acc 0.875, learning_rate 0.000385604\n","2023-06-13T17:04:57.700117: step 5391, loss 0.451991, acc 0.875, learning_rate 0.000385466\n","2023-06-13T17:04:59.187404: step 5392, loss 0.419344, acc 0.8125, learning_rate 0.000385327\n","2023-06-13T17:05:01.278829: step 5393, loss 0.614184, acc 0.71875, learning_rate 0.000385189\n","2023-06-13T17:05:03.854142: step 5394, loss 0.31409, acc 0.90625, learning_rate 0.000385051\n","2023-06-13T17:05:06.137314: step 5395, loss 0.648838, acc 0.75, learning_rate 0.000384912\n","2023-06-13T17:05:08.561213: step 5396, loss 0.623284, acc 0.875, learning_rate 0.000384774\n","2023-06-13T17:05:10.725078: step 5397, loss 0.426133, acc 0.78125, learning_rate 0.000384636\n","2023-06-13T17:05:12.222664: step 5398, loss 0.823643, acc 0.78125, learning_rate 0.000384498\n","2023-06-13T17:05:13.700847: step 5399, loss 0.461595, acc 0.84375, learning_rate 0.00038436\n","\n","Evaluation:\n","2023-06-13T17:05:42.211448: step 5400, loss 2.64528, acc 0.384359\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5400\n","\n","2023-06-13T17:05:44.911333: step 5400, loss 0.441644, acc 0.8125, learning_rate 0.000384222\n","2023-06-13T17:05:47.452707: step 5401, loss 0.572814, acc 0.8125, learning_rate 0.000384084\n","2023-06-13T17:05:49.941132: step 5402, loss 0.694898, acc 0.65625, learning_rate 0.000383947\n","2023-06-13T17:05:52.440210: step 5403, loss 0.233435, acc 0.9375, learning_rate 0.000383809\n","2023-06-13T17:05:54.065244: step 5404, loss 0.590356, acc 0.8125, learning_rate 0.000383671\n","2023-06-13T17:05:55.553132: step 5405, loss 0.621441, acc 0.75, learning_rate 0.000383534\n","2023-06-13T17:05:57.019846: step 5406, loss 0.793548, acc 0.75, learning_rate 0.000383396\n","2023-06-13T17:05:58.485383: step 5407, loss 0.534581, acc 0.8125, learning_rate 0.000383259\n","2023-06-13T17:05:59.992381: step 5408, loss 0.7854, acc 0.75, learning_rate 0.000383122\n","2023-06-13T17:06:01.468078: step 5409, loss 0.51288, acc 0.875, learning_rate 0.000382984\n","2023-06-13T17:06:03.026037: step 5410, loss 0.544698, acc 0.875, learning_rate 0.000382847\n","2023-06-13T17:06:05.666677: step 5411, loss 0.36755, acc 0.875, learning_rate 0.00038271\n","2023-06-13T17:06:08.036840: step 5412, loss 0.314182, acc 0.90625, learning_rate 0.000382573\n","2023-06-13T17:06:10.474680: step 5413, loss 0.891955, acc 0.78125, learning_rate 0.000382436\n","2023-06-13T17:06:12.911427: step 5414, loss 0.804464, acc 0.71875, learning_rate 0.000382299\n","2023-06-13T17:06:14.463444: step 5415, loss 0.462282, acc 0.84375, learning_rate 0.000382162\n","2023-06-13T17:06:15.906743: step 5416, loss 0.81349, acc 0.6875, learning_rate 0.000382025\n","2023-06-13T17:06:17.349998: step 5417, loss 0.657853, acc 0.78125, learning_rate 0.000381888\n","2023-06-13T17:06:18.785007: step 5418, loss 0.847335, acc 0.75, learning_rate 0.000381752\n","2023-06-13T17:06:20.261591: step 5419, loss 0.538366, acc 0.875, learning_rate 0.000381615\n","2023-06-13T17:06:21.742997: step 5420, loss 0.407835, acc 0.90625, learning_rate 0.000381479\n","2023-06-13T17:06:23.271580: step 5421, loss 0.701336, acc 0.75, learning_rate 0.000381342\n","2023-06-13T17:06:25.910253: step 5422, loss 0.34276, acc 0.875, learning_rate 0.000381206\n","2023-06-13T17:06:28.384232: step 5423, loss 0.632834, acc 0.75, learning_rate 0.000381069\n","2023-06-13T17:06:30.850990: step 5424, loss 0.464229, acc 0.875, learning_rate 0.000380933\n","2023-06-13T17:06:33.292944: step 5425, loss 0.327904, acc 0.90625, learning_rate 0.000380797\n","2023-06-13T17:06:34.865028: step 5426, loss 0.384388, acc 0.9375, learning_rate 0.000380661\n","2023-06-13T17:06:36.343333: step 5427, loss 0.795755, acc 0.65625, learning_rate 0.000380525\n","2023-06-13T17:06:37.823326: step 5428, loss 1.11959, acc 0.6875, learning_rate 0.000380389\n","2023-06-13T17:06:39.310868: step 5429, loss 0.624037, acc 0.78125, learning_rate 0.000380253\n","2023-06-13T17:06:40.778482: step 5430, loss 0.670681, acc 0.8125, learning_rate 0.000380117\n","2023-06-13T17:06:42.237839: step 5431, loss 0.444279, acc 0.84375, learning_rate 0.000379981\n","2023-06-13T17:06:43.779631: step 5432, loss 0.478178, acc 0.875, learning_rate 0.000379845\n","2023-06-13T17:06:46.309031: step 5433, loss 0.610988, acc 0.8125, learning_rate 0.000379709\n","2023-06-13T17:06:48.744078: step 5434, loss 0.64531, acc 0.75, learning_rate 0.000379574\n","2023-06-13T17:06:51.180916: step 5435, loss 0.508004, acc 0.78125, learning_rate 0.000379438\n","2023-06-13T17:06:53.584170: step 5436, loss 0.623974, acc 0.8125, learning_rate 0.000379303\n","2023-06-13T17:06:55.356526: step 5437, loss 0.726639, acc 0.8125, learning_rate 0.000379167\n","2023-06-13T17:06:56.828329: step 5438, loss 0.845833, acc 0.8125, learning_rate 0.000379032\n","2023-06-13T17:06:58.314052: step 5439, loss 1.04244, acc 0.6875, learning_rate 0.000378897\n","2023-06-13T17:06:59.826541: step 5440, loss 0.497767, acc 0.8125, learning_rate 0.000378761\n","2023-06-13T17:07:01.301312: step 5441, loss 0.667497, acc 0.78125, learning_rate 0.000378626\n","2023-06-13T17:07:02.794595: step 5442, loss 0.480187, acc 0.8125, learning_rate 0.000378491\n","2023-06-13T17:07:04.308981: step 5443, loss 1.28518, acc 0.65625, learning_rate 0.000378356\n","2023-06-13T17:07:06.830902: step 5444, loss 0.726915, acc 0.71875, learning_rate 0.000378221\n","2023-06-13T17:07:09.423451: step 5445, loss 0.780328, acc 0.75, learning_rate 0.000378086\n","2023-06-13T17:07:11.989232: step 5446, loss 0.885058, acc 0.71875, learning_rate 0.000377951\n","2023-06-13T17:07:14.442609: step 5447, loss 0.43298, acc 0.84375, learning_rate 0.000377817\n","2023-06-13T17:07:15.936198: step 5448, loss 0.598248, acc 0.78125, learning_rate 0.000377682\n","2023-06-13T17:07:17.410560: step 5449, loss 0.247178, acc 0.90625, learning_rate 0.000377547\n","2023-06-13T17:07:18.886070: step 5450, loss 0.780395, acc 0.71875, learning_rate 0.000377413\n","2023-06-13T17:07:20.350412: step 5451, loss 0.310258, acc 0.875, learning_rate 0.000377278\n","2023-06-13T17:07:21.803343: step 5452, loss 0.569579, acc 0.8125, learning_rate 0.000377144\n","2023-06-13T17:07:23.265676: step 5453, loss 0.328388, acc 0.84375, learning_rate 0.000377009\n","2023-06-13T17:07:24.954842: step 5454, loss 0.494978, acc 0.875, learning_rate 0.000376875\n","2023-06-13T17:07:27.548533: step 5455, loss 0.667396, acc 0.78125, learning_rate 0.000376741\n","2023-06-13T17:07:30.076726: step 5456, loss 0.657146, acc 0.8125, learning_rate 0.000376607\n","2023-06-13T17:07:32.490478: step 5457, loss 0.723928, acc 0.6875, learning_rate 0.000376473\n","2023-06-13T17:07:34.854419: step 5458, loss 0.469852, acc 0.875, learning_rate 0.000376339\n","2023-06-13T17:07:36.312523: step 5459, loss 0.798946, acc 0.6875, learning_rate 0.000376205\n","2023-06-13T17:07:37.813362: step 5460, loss 0.419653, acc 0.84375, learning_rate 0.000376071\n","2023-06-13T17:07:39.290530: step 5461, loss 0.719457, acc 0.6875, learning_rate 0.000375937\n","2023-06-13T17:07:40.758238: step 5462, loss 0.361171, acc 0.90625, learning_rate 0.000375803\n","2023-06-13T17:07:42.256506: step 5463, loss 0.451129, acc 0.875, learning_rate 0.000375669\n","2023-06-13T17:07:43.732900: step 5464, loss 0.649303, acc 0.8125, learning_rate 0.000375536\n","2023-06-13T17:07:45.485490: step 5465, loss 0.599215, acc 0.875, learning_rate 0.000375402\n","2023-06-13T17:07:48.107267: step 5466, loss 0.694146, acc 0.71875, learning_rate 0.000375268\n","2023-06-13T17:07:50.504462: step 5467, loss 0.72902, acc 0.6875, learning_rate 0.000375135\n","2023-06-13T17:07:52.895154: step 5468, loss 0.667884, acc 0.8125, learning_rate 0.000375002\n","2023-06-13T17:07:55.203689: step 5469, loss 0.257382, acc 0.90625, learning_rate 0.000374868\n","2023-06-13T17:07:56.744456: step 5470, loss 0.655883, acc 0.78125, learning_rate 0.000374735\n","2023-06-13T17:07:58.264283: step 5471, loss 0.534578, acc 0.8125, learning_rate 0.000374602\n","2023-06-13T17:07:59.734888: step 5472, loss 0.750384, acc 0.75, learning_rate 0.000374469\n","2023-06-13T17:08:01.251227: step 5473, loss 0.2588, acc 0.9375, learning_rate 0.000374335\n","2023-06-13T17:08:02.720537: step 5474, loss 0.47167, acc 0.84375, learning_rate 0.000374202\n","2023-06-13T17:08:04.175887: step 5475, loss 0.803697, acc 0.75, learning_rate 0.00037407\n","2023-06-13T17:08:05.707735: step 5476, loss 0.489169, acc 0.78125, learning_rate 0.000373937\n","2023-06-13T17:08:08.413320: step 5477, loss 0.752133, acc 0.75, learning_rate 0.000373804\n","2023-06-13T17:08:10.795421: step 5478, loss 0.449017, acc 0.84375, learning_rate 0.000373671\n","2023-06-13T17:08:13.238823: step 5479, loss 0.654382, acc 0.8125, learning_rate 0.000373538\n","2023-06-13T17:08:15.617691: step 5480, loss 0.711581, acc 0.78125, learning_rate 0.000373406\n","2023-06-13T17:08:17.136783: step 5481, loss 0.835858, acc 0.71875, learning_rate 0.000373273\n","2023-06-13T17:08:18.600312: step 5482, loss 0.669597, acc 0.71875, learning_rate 0.000373141\n","2023-06-13T17:08:20.072907: step 5483, loss 0.534411, acc 0.84375, learning_rate 0.000373008\n","2023-06-13T17:08:21.554109: step 5484, loss 0.760654, acc 0.75, learning_rate 0.000372876\n","2023-06-13T17:08:23.013705: step 5485, loss 0.405193, acc 0.90625, learning_rate 0.000372743\n","2023-06-13T17:08:24.473754: step 5486, loss 0.866222, acc 0.75, learning_rate 0.000372611\n","2023-06-13T17:08:26.100893: step 5487, loss 0.818238, acc 0.84375, learning_rate 0.000372479\n","2023-06-13T17:08:28.692109: step 5488, loss 0.902422, acc 0.71875, learning_rate 0.000372347\n","2023-06-13T17:08:31.144385: step 5489, loss 0.310842, acc 0.90625, learning_rate 0.000372215\n","2023-06-13T17:08:33.463838: step 5490, loss 0.630639, acc 0.84375, learning_rate 0.000372083\n","2023-06-13T17:08:35.800986: step 5491, loss 0.57979, acc 0.8125, learning_rate 0.000371951\n","2023-06-13T17:08:37.469695: step 5492, loss 0.526262, acc 0.8125, learning_rate 0.000371819\n","2023-06-13T17:08:38.953613: step 5493, loss 0.635162, acc 0.8125, learning_rate 0.000371687\n","2023-06-13T17:08:40.390354: step 5494, loss 0.49708, acc 0.8125, learning_rate 0.000371556\n","2023-06-13T17:08:41.811420: step 5495, loss 0.975298, acc 0.75, learning_rate 0.000371424\n","2023-06-13T17:08:43.218316: step 5496, loss 0.743484, acc 0.75, learning_rate 0.000371292\n","2023-06-13T17:08:44.605613: step 5497, loss 0.563634, acc 0.8125, learning_rate 0.000371161\n","2023-06-13T17:08:46.043543: step 5498, loss 0.447801, acc 0.875, learning_rate 0.000371029\n","2023-06-13T17:08:48.149269: step 5499, loss 0.875377, acc 0.71875, learning_rate 0.000370898\n","\n","Evaluation:\n","2023-06-13T17:09:20.051557: step 5500, loss 2.64479, acc 0.388906\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5500\n","\n","2023-06-13T17:09:21.672483: step 5500, loss 0.551557, acc 0.875, learning_rate 0.000370767\n","2023-06-13T17:09:23.088526: step 5501, loss 0.643132, acc 0.78125, learning_rate 0.000370635\n","2023-06-13T17:09:24.503085: step 5502, loss 0.596199, acc 0.84375, learning_rate 0.000370504\n","2023-06-13T17:09:25.976855: step 5503, loss 0.487903, acc 0.8125, learning_rate 0.000370373\n","2023-06-13T17:09:27.478941: step 5504, loss 0.446867, acc 0.8125, learning_rate 0.000370242\n","2023-06-13T17:09:29.051596: step 5505, loss 0.742768, acc 0.75, learning_rate 0.000370111\n","2023-06-13T17:09:31.573086: step 5506, loss 0.681079, acc 0.75, learning_rate 0.00036998\n","2023-06-13T17:09:34.078866: step 5507, loss 0.331458, acc 0.875, learning_rate 0.000369849\n","2023-06-13T17:09:36.397027: step 5508, loss 0.381807, acc 0.90625, learning_rate 0.000369718\n","2023-06-13T17:09:38.777386: step 5509, loss 0.552904, acc 0.8125, learning_rate 0.000369587\n","2023-06-13T17:09:40.419950: step 5510, loss 0.558917, acc 0.8125, learning_rate 0.000369456\n","2023-06-13T17:09:41.874341: step 5511, loss 0.745475, acc 0.75, learning_rate 0.000369326\n","2023-06-13T17:09:43.321488: step 5512, loss 0.2208, acc 0.9375, learning_rate 0.000369195\n","2023-06-13T17:09:44.752861: step 5513, loss 0.72757, acc 0.71875, learning_rate 0.000369065\n","2023-06-13T17:09:46.213018: step 5514, loss 1.14837, acc 0.6875, learning_rate 0.000368934\n","2023-06-13T17:09:47.671539: step 5515, loss 0.392772, acc 0.84375, learning_rate 0.000368804\n","2023-06-13T17:09:49.138916: step 5516, loss 0.476383, acc 0.90625, learning_rate 0.000368674\n","2023-06-13T17:09:51.651725: step 5517, loss 0.64839, acc 0.78125, learning_rate 0.000368543\n","2023-06-13T17:09:54.172574: step 5518, loss 0.478634, acc 0.78125, learning_rate 0.000368413\n","2023-06-13T17:09:56.484960: step 5519, loss 0.270421, acc 1, learning_rate 0.000368283\n","2023-06-13T17:09:58.824516: step 5520, loss 0.414573, acc 0.84375, learning_rate 0.000368153\n","2023-06-13T17:10:00.508823: step 5521, loss 0.692851, acc 0.6875, learning_rate 0.000368023\n","2023-06-13T17:10:01.970709: step 5522, loss 0.625107, acc 0.78125, learning_rate 0.000367893\n","2023-06-13T17:10:03.432279: step 5523, loss 0.764683, acc 0.6875, learning_rate 0.000367763\n","2023-06-13T17:10:04.865309: step 5524, loss 0.442918, acc 0.78125, learning_rate 0.000367633\n","2023-06-13T17:10:06.291321: step 5525, loss 0.46116, acc 0.8125, learning_rate 0.000367503\n","2023-06-13T17:10:07.729593: step 5526, loss 0.914267, acc 0.8125, learning_rate 0.000367374\n","2023-06-13T17:10:09.171035: step 5527, loss 0.372937, acc 0.90625, learning_rate 0.000367244\n","2023-06-13T17:10:11.315773: step 5528, loss 0.618756, acc 0.84375, learning_rate 0.000367114\n","2023-06-13T17:10:13.840024: step 5529, loss 0.455961, acc 0.875, learning_rate 0.000366985\n","2023-06-13T17:10:16.243694: step 5530, loss 0.364309, acc 0.875, learning_rate 0.000366855\n","2023-06-13T17:10:18.421092: step 5531, loss 0.456241, acc 0.78125, learning_rate 0.000366726\n","2023-06-13T17:10:20.453711: step 5532, loss 0.543091, acc 0.84375, learning_rate 0.000366597\n","2023-06-13T17:10:21.909562: step 5533, loss 1.04701, acc 0.625, learning_rate 0.000366467\n","2023-06-13T17:10:23.334296: step 5534, loss 0.76727, acc 0.71875, learning_rate 0.000366338\n","2023-06-13T17:10:24.769828: step 5535, loss 0.41911, acc 0.875, learning_rate 0.000366209\n","2023-06-13T17:10:26.250807: step 5536, loss 0.336463, acc 0.84375, learning_rate 0.00036608\n","2023-06-13T17:10:27.726529: step 5537, loss 0.532998, acc 0.84375, learning_rate 0.000365951\n","2023-06-13T17:10:29.215091: step 5538, loss 0.542112, acc 0.84375, learning_rate 0.000365822\n","2023-06-13T17:10:31.128844: step 5539, loss 0.432239, acc 0.875, learning_rate 0.000365693\n","2023-06-13T17:10:33.729626: step 5540, loss 0.643799, acc 0.75, learning_rate 0.000365564\n","2023-06-13T17:10:35.976152: step 5541, loss 0.519447, acc 0.8125, learning_rate 0.000365436\n","2023-06-13T17:10:38.279411: step 5542, loss 0.280068, acc 0.875, learning_rate 0.000365307\n","2023-06-13T17:10:40.541477: step 5543, loss 0.409693, acc 0.84375, learning_rate 0.000365178\n","2023-06-13T17:10:42.069901: step 5544, loss 0.728147, acc 0.84375, learning_rate 0.00036505\n","2023-06-13T17:10:43.500459: step 5545, loss 0.526711, acc 0.8125, learning_rate 0.000364921\n","2023-06-13T17:10:44.947344: step 5546, loss 0.482438, acc 0.875, learning_rate 0.000364793\n","2023-06-13T17:10:46.388385: step 5547, loss 0.75753, acc 0.78125, learning_rate 0.000364664\n","2023-06-13T17:10:47.826901: step 5548, loss 0.939962, acc 0.65625, learning_rate 0.000364536\n","2023-06-13T17:10:49.264320: step 5549, loss 0.62079, acc 0.78125, learning_rate 0.000364408\n","2023-06-13T17:10:50.684523: step 5550, loss 0.574713, acc 0.75, learning_rate 0.000364279\n","2023-06-13T17:10:53.258607: step 5551, loss 0.575771, acc 0.78125, learning_rate 0.000364151\n","2023-06-13T17:10:55.664088: step 5552, loss 0.441383, acc 0.875, learning_rate 0.000364023\n","2023-06-13T17:10:58.106647: step 5553, loss 0.459732, acc 0.84375, learning_rate 0.000363895\n","2023-06-13T17:11:00.432840: step 5554, loss 0.582996, acc 0.8125, learning_rate 0.000363767\n","2023-06-13T17:11:02.042839: step 5555, loss 0.725692, acc 0.75, learning_rate 0.000363639\n","2023-06-13T17:11:03.518175: step 5556, loss 0.356945, acc 0.84375, learning_rate 0.000363512\n","2023-06-13T17:11:04.963688: step 5557, loss 0.584084, acc 0.78125, learning_rate 0.000363384\n","2023-06-13T17:11:06.410966: step 5558, loss 0.48775, acc 0.75, learning_rate 0.000363256\n","2023-06-13T17:11:07.876321: step 5559, loss 0.878838, acc 0.75, learning_rate 0.000363128\n","2023-06-13T17:11:09.311488: step 5560, loss 0.586802, acc 0.8125, learning_rate 0.000363001\n","2023-06-13T17:11:10.752729: step 5561, loss 0.438524, acc 0.875, learning_rate 0.000362873\n","2023-06-13T17:11:13.136116: step 5562, loss 0.764982, acc 0.78125, learning_rate 0.000362746\n","2023-06-13T17:11:15.626379: step 5563, loss 0.422815, acc 0.875, learning_rate 0.000362618\n","2023-06-13T17:11:17.964576: step 5564, loss 0.781794, acc 0.71875, learning_rate 0.000362491\n","2023-06-13T17:11:20.293543: step 5565, loss 0.908945, acc 0.6875, learning_rate 0.000362364\n","2023-06-13T17:11:22.068634: step 5566, loss 0.607426, acc 0.75, learning_rate 0.000362237\n","2023-06-13T17:11:23.492659: step 5567, loss 0.404693, acc 0.84375, learning_rate 0.000362109\n","2023-06-13T17:11:24.917422: step 5568, loss 0.589447, acc 0.78125, learning_rate 0.000361982\n","2023-06-13T17:11:26.389756: step 5569, loss 0.644637, acc 0.78125, learning_rate 0.000361855\n","2023-06-13T17:11:27.836146: step 5570, loss 0.332023, acc 0.8125, learning_rate 0.000361728\n","2023-06-13T17:11:29.247830: step 5571, loss 0.538921, acc 0.8125, learning_rate 0.000361601\n","2023-06-13T17:11:30.697230: step 5572, loss 0.620171, acc 0.8125, learning_rate 0.000361475\n","2023-06-13T17:11:32.720073: step 5573, loss 0.908679, acc 0.71875, learning_rate 0.000361348\n","2023-06-13T17:11:35.311517: step 5574, loss 0.42532, acc 0.84375, learning_rate 0.000361221\n","2023-06-13T17:11:37.585392: step 5575, loss 0.758774, acc 0.8125, learning_rate 0.000361094\n","2023-06-13T17:11:39.896895: step 5576, loss 0.568719, acc 0.75, learning_rate 0.000360968\n","2023-06-13T17:11:42.065531: step 5577, loss 0.536512, acc 0.8125, learning_rate 0.000360841\n","2023-06-13T17:11:43.506744: step 5578, loss 0.936999, acc 0.71875, learning_rate 0.000360715\n","2023-06-13T17:11:44.924860: step 5579, loss 0.568013, acc 0.75, learning_rate 0.000360588\n","2023-06-13T17:11:46.335217: step 5580, loss 0.318778, acc 0.9375, learning_rate 0.000360462\n","2023-06-13T17:11:47.806335: step 5581, loss 0.499986, acc 0.84375, learning_rate 0.000360336\n","2023-06-13T17:11:49.251970: step 5582, loss 0.525294, acc 0.8125, learning_rate 0.00036021\n","2023-06-13T17:11:50.699983: step 5583, loss 0.446056, acc 0.875, learning_rate 0.000360083\n","2023-06-13T17:11:52.258356: step 5584, loss 0.435862, acc 0.84375, learning_rate 0.000359957\n","2023-06-13T17:11:54.698554: step 5585, loss 0.364642, acc 0.9375, learning_rate 0.000359831\n","2023-06-13T17:11:57.232643: step 5586, loss 0.75396, acc 0.78125, learning_rate 0.000359705\n","2023-06-13T17:11:59.657287: step 5587, loss 0.559239, acc 0.8125, learning_rate 0.000359579\n","2023-06-13T17:12:01.993070: step 5588, loss 0.259287, acc 0.9375, learning_rate 0.000359453\n","2023-06-13T17:12:03.582116: step 5589, loss 0.643914, acc 0.78125, learning_rate 0.000359328\n","2023-06-13T17:12:05.079122: step 5590, loss 0.655655, acc 0.78125, learning_rate 0.000359202\n","2023-06-13T17:12:06.523275: step 5591, loss 0.52156, acc 0.8125, learning_rate 0.000359076\n","2023-06-13T17:12:07.992254: step 5592, loss 0.456746, acc 0.84375, learning_rate 0.000358951\n","2023-06-13T17:12:09.403591: step 5593, loss 0.679954, acc 0.75, learning_rate 0.000358825\n","2023-06-13T17:12:10.841622: step 5594, loss 0.375681, acc 0.84375, learning_rate 0.0003587\n","2023-06-13T17:12:12.287437: step 5595, loss 1.44827, acc 0.6875, learning_rate 0.000358574\n","2023-06-13T17:12:14.654188: step 5596, loss 0.742483, acc 0.6875, learning_rate 0.000358449\n","2023-06-13T17:12:17.053627: step 5597, loss 0.710648, acc 0.71875, learning_rate 0.000358323\n","2023-06-13T17:12:19.427952: step 5598, loss 0.772648, acc 0.78125, learning_rate 0.000358198\n","2023-06-13T17:12:21.744341: step 5599, loss 1.02869, acc 0.65625, learning_rate 0.000358073\n","\n","Evaluation:\n","2023-06-13T17:12:50.122640: step 5600, loss 2.68038, acc 0.384662\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5600\n","\n","2023-06-13T17:12:51.735275: step 5600, loss 0.397607, acc 0.875, learning_rate 0.000357948\n","2023-06-13T17:12:53.186683: step 5601, loss 0.96833, acc 0.59375, learning_rate 0.000357823\n","2023-06-13T17:12:55.113223: step 5602, loss 0.535018, acc 0.78125, learning_rate 0.000357698\n","2023-06-13T17:12:57.860877: step 5603, loss 1.08488, acc 0.6875, learning_rate 0.000357573\n","2023-06-13T17:13:00.092897: step 5604, loss 0.489109, acc 0.8125, learning_rate 0.000357448\n","2023-06-13T17:13:02.466298: step 5605, loss 0.561902, acc 0.84375, learning_rate 0.000357323\n","2023-06-13T17:13:04.673131: step 5606, loss 0.702641, acc 0.75, learning_rate 0.000357198\n","2023-06-13T17:13:06.096665: step 5607, loss 0.700704, acc 0.78125, learning_rate 0.000357074\n","2023-06-13T17:13:07.547811: step 5608, loss 0.774229, acc 0.625, learning_rate 0.000356949\n","2023-06-13T17:13:09.017770: step 5609, loss 0.463325, acc 0.875, learning_rate 0.000356824\n","2023-06-13T17:13:10.461918: step 5610, loss 0.550013, acc 0.78125, learning_rate 0.0003567\n","2023-06-13T17:13:11.913256: step 5611, loss 0.598241, acc 0.84375, learning_rate 0.000356575\n","2023-06-13T17:13:13.353060: step 5612, loss 0.563461, acc 0.8125, learning_rate 0.000356451\n","2023-06-13T17:13:15.000941: step 5613, loss 0.656187, acc 0.8125, learning_rate 0.000356327\n","2023-06-13T17:13:17.493596: step 5614, loss 0.717476, acc 0.6875, learning_rate 0.000356202\n","2023-06-13T17:13:19.895709: step 5615, loss 0.822159, acc 0.71875, learning_rate 0.000356078\n","2023-06-13T17:13:22.266438: step 5616, loss 0.38465, acc 0.90625, learning_rate 0.000355954\n","2023-06-13T17:13:24.557269: step 5617, loss 0.730661, acc 0.78125, learning_rate 0.00035583\n","2023-06-13T17:13:26.069419: step 5618, loss 0.647991, acc 0.75, learning_rate 0.000355706\n","2023-06-13T17:13:27.515993: step 5619, loss 0.940269, acc 0.5625, learning_rate 0.000355582\n","2023-06-13T17:13:28.969665: step 5620, loss 1.00268, acc 0.71875, learning_rate 0.000355458\n","2023-06-13T17:13:30.432784: step 5621, loss 0.508994, acc 0.8125, learning_rate 0.000355334\n","2023-06-13T17:13:31.926655: step 5622, loss 0.49032, acc 0.84375, learning_rate 0.00035521\n","2023-06-13T17:13:33.373472: step 5623, loss 0.777079, acc 0.71875, learning_rate 0.000355086\n","2023-06-13T17:13:35.039416: step 5624, loss 0.76731, acc 0.71875, learning_rate 0.000354963\n","2023-06-13T17:13:37.648916: step 5625, loss 0.561627, acc 0.78125, learning_rate 0.000354839\n","2023-06-13T17:13:40.017258: step 5626, loss 0.386509, acc 0.90625, learning_rate 0.000354716\n","2023-06-13T17:13:42.385126: step 5627, loss 0.626813, acc 0.6875, learning_rate 0.000354592\n","2023-06-13T17:13:44.762164: step 5628, loss 0.554523, acc 0.84375, learning_rate 0.000354469\n","2023-06-13T17:13:46.333579: step 5629, loss 0.802662, acc 0.75, learning_rate 0.000354345\n","2023-06-13T17:13:47.799952: step 5630, loss 0.662103, acc 0.78125, learning_rate 0.000354222\n","2023-06-13T17:13:49.262989: step 5631, loss 0.47634, acc 0.875, learning_rate 0.000354099\n","2023-06-13T17:13:50.721227: step 5632, loss 0.431489, acc 0.84375, learning_rate 0.000353975\n","2023-06-13T17:13:52.177741: step 5633, loss 1.07543, acc 0.6875, learning_rate 0.000353852\n","2023-06-13T17:13:53.614652: step 5634, loss 0.682496, acc 0.71875, learning_rate 0.000353729\n","2023-06-13T17:13:55.053500: step 5635, loss 0.698023, acc 0.8125, learning_rate 0.000353606\n","2023-06-13T17:13:57.497929: step 5636, loss 0.669468, acc 0.84375, learning_rate 0.000353483\n","2023-06-13T17:13:59.935805: step 5637, loss 0.512927, acc 0.84375, learning_rate 0.00035336\n","2023-06-13T17:14:02.366556: step 5638, loss 0.608408, acc 0.8125, learning_rate 0.000353237\n","2023-06-13T17:14:04.760969: step 5639, loss 0.918873, acc 0.6875, learning_rate 0.000353115\n","2023-06-13T17:14:06.525853: step 5640, loss 0.407413, acc 0.90625, learning_rate 0.000352992\n","2023-06-13T17:14:08.002048: step 5641, loss 0.670536, acc 0.8125, learning_rate 0.000352869\n","2023-06-13T17:14:10.096415: step 5642, loss 0.715216, acc 0.78125, learning_rate 0.000352747\n","2023-06-13T17:14:12.728793: step 5643, loss 0.500951, acc 0.84375, learning_rate 0.000352624\n","2023-06-13T17:14:15.009307: step 5644, loss 0.505419, acc 0.8125, learning_rate 0.000352502\n","2023-06-13T17:14:17.680949: step 5645, loss 0.565752, acc 0.8125, learning_rate 0.000352379\n","2023-06-13T17:14:20.359069: step 5646, loss 0.462825, acc 0.8125, learning_rate 0.000352257\n","2023-06-13T17:14:22.947278: step 5647, loss 0.562439, acc 0.84375, learning_rate 0.000352134\n","2023-06-13T17:14:25.343679: step 5648, loss 0.616565, acc 0.8125, learning_rate 0.000352012\n","2023-06-13T17:14:27.691484: step 5649, loss 0.561495, acc 0.8125, learning_rate 0.00035189\n","2023-06-13T17:14:29.420604: step 5650, loss 0.587639, acc 0.78125, learning_rate 0.000351768\n","2023-06-13T17:14:30.837302: step 5651, loss 0.638972, acc 0.75, learning_rate 0.000351646\n","2023-06-13T17:14:32.282696: step 5652, loss 0.697186, acc 0.78125, learning_rate 0.000351524\n","2023-06-13T17:14:33.692499: step 5653, loss 0.722246, acc 0.65625, learning_rate 0.000351402\n","2023-06-13T17:14:35.112543: step 5654, loss 0.482159, acc 0.84375, learning_rate 0.00035128\n","2023-06-13T17:14:36.538698: step 5655, loss 0.526434, acc 0.78125, learning_rate 0.000351158\n","2023-06-13T17:14:38.012965: step 5656, loss 0.554884, acc 0.78125, learning_rate 0.000351036\n","2023-06-13T17:14:40.219923: step 5657, loss 0.439286, acc 0.90625, learning_rate 0.000350915\n","2023-06-13T17:14:42.746229: step 5658, loss 0.875079, acc 0.71875, learning_rate 0.000350793\n","2023-06-13T17:14:45.121126: step 5659, loss 0.910332, acc 0.6875, learning_rate 0.000350671\n","2023-06-13T17:14:47.514957: step 5660, loss 0.68735, acc 0.71875, learning_rate 0.00035055\n","2023-06-13T17:14:49.416904: step 5661, loss 0.663052, acc 0.71875, learning_rate 0.000350428\n","2023-06-13T17:14:50.844807: step 5662, loss 0.777347, acc 0.6875, learning_rate 0.000350307\n","2023-06-13T17:14:52.304147: step 5663, loss 0.602717, acc 0.78125, learning_rate 0.000350185\n","2023-06-13T17:14:53.702692: step 5664, loss 0.551462, acc 0.78125, learning_rate 0.000350064\n","2023-06-13T17:14:55.171086: step 5665, loss 0.491177, acc 0.84375, learning_rate 0.000349943\n","2023-06-13T17:14:56.591839: step 5666, loss 0.84919, acc 0.75, learning_rate 0.000349822\n","2023-06-13T17:14:58.038184: step 5667, loss 0.558542, acc 0.78125, learning_rate 0.000349701\n","2023-06-13T17:14:59.960587: step 5668, loss 0.734013, acc 0.78125, learning_rate 0.00034958\n","2023-06-13T17:15:02.488910: step 5669, loss 0.619316, acc 0.84375, learning_rate 0.000349459\n","2023-06-13T17:15:04.814686: step 5670, loss 1.46784, acc 0.59375, learning_rate 0.000349338\n","2023-06-13T17:15:07.191406: step 5671, loss 0.691615, acc 0.6875, learning_rate 0.000349217\n","2023-06-13T17:15:09.458007: step 5672, loss 0.514617, acc 0.78125, learning_rate 0.000349096\n","2023-06-13T17:15:10.909660: step 5673, loss 0.684445, acc 0.71875, learning_rate 0.000348975\n","2023-06-13T17:15:12.384874: step 5674, loss 0.606959, acc 0.84375, learning_rate 0.000348854\n","2023-06-13T17:15:13.840482: step 5675, loss 0.510628, acc 0.875, learning_rate 0.000348734\n","2023-06-13T17:15:15.273404: step 5676, loss 0.699125, acc 0.71875, learning_rate 0.000348613\n","2023-06-13T17:15:16.688970: step 5677, loss 0.501734, acc 0.875, learning_rate 0.000348492\n","2023-06-13T17:15:18.120970: step 5678, loss 0.564144, acc 0.84375, learning_rate 0.000348372\n","2023-06-13T17:15:19.673734: step 5679, loss 0.596551, acc 0.8125, learning_rate 0.000348252\n","2023-06-13T17:15:22.244322: step 5680, loss 0.754106, acc 0.6875, learning_rate 0.000348131\n","2023-06-13T17:15:24.617989: step 5681, loss 0.981005, acc 0.625, learning_rate 0.000348011\n","2023-06-13T17:15:26.977476: step 5682, loss 0.551338, acc 0.90625, learning_rate 0.000347891\n","2023-06-13T17:15:29.295486: step 5683, loss 0.994846, acc 0.65625, learning_rate 0.00034777\n","2023-06-13T17:15:30.869610: step 5684, loss 0.460805, acc 0.84375, learning_rate 0.00034765\n","2023-06-13T17:15:32.320701: step 5685, loss 0.647011, acc 0.6875, learning_rate 0.00034753\n","2023-06-13T17:15:33.725027: step 5686, loss 0.803846, acc 0.6875, learning_rate 0.00034741\n","2023-06-13T17:15:35.132430: step 5687, loss 0.565474, acc 0.8125, learning_rate 0.00034729\n","2023-06-13T17:15:36.561827: step 5688, loss 0.603894, acc 0.78125, learning_rate 0.00034717\n","2023-06-13T17:15:38.021172: step 5689, loss 0.466057, acc 0.875, learning_rate 0.00034705\n","2023-06-13T17:15:39.441586: step 5690, loss 0.363039, acc 0.875, learning_rate 0.000346931\n","2023-06-13T17:15:41.839840: step 5691, loss 0.476235, acc 0.84375, learning_rate 0.000346811\n","2023-06-13T17:15:44.367388: step 5692, loss 0.72944, acc 0.71875, learning_rate 0.000346691\n","2023-06-13T17:15:46.654064: step 5693, loss 0.598791, acc 0.78125, learning_rate 0.000346572\n","2023-06-13T17:15:49.020502: step 5694, loss 0.645922, acc 0.75, learning_rate 0.000346452\n","2023-06-13T17:15:50.728589: step 5695, loss 0.382727, acc 0.875, learning_rate 0.000346333\n","2023-06-13T17:15:52.192125: step 5696, loss 0.739199, acc 0.78125, learning_rate 0.000346213\n","2023-06-13T17:15:53.620772: step 5697, loss 0.727045, acc 0.71875, learning_rate 0.000346094\n","2023-06-13T17:15:55.053612: step 5698, loss 0.435078, acc 0.84375, learning_rate 0.000345974\n","2023-06-13T17:15:56.484329: step 5699, loss 0.808793, acc 0.71875, learning_rate 0.000345855\n","\n","Evaluation:\n","2023-06-13T17:16:27.502389: step 5700, loss 2.67285, acc 0.385268\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5700\n","\n","2023-06-13T17:16:30.221490: step 5700, loss 0.375497, acc 0.9375, learning_rate 0.000345736\n","2023-06-13T17:16:32.042213: step 5701, loss 0.789334, acc 0.8125, learning_rate 0.000345617\n","2023-06-13T17:16:33.497936: step 5702, loss 0.492915, acc 0.84375, learning_rate 0.000345498\n","2023-06-13T17:16:34.951485: step 5703, loss 0.475618, acc 0.875, learning_rate 0.000345379\n","2023-06-13T17:16:36.401188: step 5704, loss 0.565256, acc 0.875, learning_rate 0.00034526\n","2023-06-13T17:16:37.926567: step 5705, loss 0.675904, acc 0.71875, learning_rate 0.000345141\n","2023-06-13T17:16:39.376682: step 5706, loss 0.639049, acc 0.71875, learning_rate 0.000345022\n","2023-06-13T17:16:40.829921: step 5707, loss 0.51848, acc 0.875, learning_rate 0.000344903\n","2023-06-13T17:16:43.219160: step 5708, loss 0.687464, acc 0.78125, learning_rate 0.000344784\n","2023-06-13T17:16:45.784286: step 5709, loss 0.708479, acc 0.78125, learning_rate 0.000344666\n","2023-06-13T17:16:48.159385: step 5710, loss 0.558499, acc 0.75, learning_rate 0.000344547\n","2023-06-13T17:16:50.525609: step 5711, loss 0.390554, acc 0.8125, learning_rate 0.000344428\n","2023-06-13T17:16:52.160893: step 5712, loss 0.489581, acc 0.84375, learning_rate 0.00034431\n","2023-06-13T17:16:53.575850: step 5713, loss 0.618493, acc 0.90625, learning_rate 0.000344192\n","2023-06-13T17:16:55.006312: step 5714, loss 0.66164, acc 0.78125, learning_rate 0.000344073\n","2023-06-13T17:16:56.459795: step 5715, loss 0.765341, acc 0.6875, learning_rate 0.000343955\n","2023-06-13T17:16:57.923040: step 5716, loss 0.646492, acc 0.75, learning_rate 0.000343836\n","2023-06-13T17:16:59.346752: step 5717, loss 0.39964, acc 0.9375, learning_rate 0.000343718\n","2023-06-13T17:17:00.808378: step 5718, loss 0.590599, acc 0.75, learning_rate 0.0003436\n","2023-06-13T17:17:03.103938: step 5719, loss 0.405042, acc 0.875, learning_rate 0.000343482\n","2023-06-13T17:17:05.571244: step 5720, loss 0.556905, acc 0.8125, learning_rate 0.000343364\n","2023-06-13T17:17:08.017880: step 5721, loss 0.798915, acc 0.75, learning_rate 0.000343246\n","2023-06-13T17:17:10.318400: step 5722, loss 0.974326, acc 0.75, learning_rate 0.000343128\n","2023-06-13T17:17:12.141399: step 5723, loss 0.808685, acc 0.71875, learning_rate 0.00034301\n","2023-06-13T17:17:13.593476: step 5724, loss 0.599692, acc 0.78125, learning_rate 0.000342892\n","2023-06-13T17:17:15.076528: step 5725, loss 0.689422, acc 0.8125, learning_rate 0.000342774\n","2023-06-13T17:17:16.529650: step 5726, loss 0.480832, acc 0.84375, learning_rate 0.000342657\n","2023-06-13T17:17:18.019013: step 5727, loss 0.652158, acc 0.78125, learning_rate 0.000342539\n","2023-06-13T17:17:19.465712: step 5728, loss 0.621219, acc 0.75, learning_rate 0.000342421\n","2023-06-13T17:17:20.943984: step 5729, loss 0.626735, acc 0.78125, learning_rate 0.000342304\n","2023-06-13T17:17:23.131387: step 5730, loss 0.597991, acc 0.8125, learning_rate 0.000342186\n","2023-06-13T17:17:25.674623: step 5731, loss 1.10748, acc 0.65625, learning_rate 0.000342069\n","2023-06-13T17:17:28.144063: step 5732, loss 0.461819, acc 0.84375, learning_rate 0.000341952\n","2023-06-13T17:17:30.581309: step 5733, loss 0.505542, acc 0.84375, learning_rate 0.000341834\n","2023-06-13T17:17:32.544562: step 5734, loss 0.360081, acc 0.90625, learning_rate 0.000341717\n","2023-06-13T17:17:33.983715: step 5735, loss 0.500496, acc 0.84375, learning_rate 0.0003416\n","2023-06-13T17:17:35.431057: step 5736, loss 0.56662, acc 0.78125, learning_rate 0.000341483\n","2023-06-13T17:17:36.925700: step 5737, loss 0.663425, acc 0.84375, learning_rate 0.000341366\n","2023-06-13T17:17:38.348714: step 5738, loss 0.811034, acc 0.78125, learning_rate 0.000341249\n","2023-06-13T17:17:39.781449: step 5739, loss 0.685016, acc 0.78125, learning_rate 0.000341132\n","2023-06-13T17:17:41.233187: step 5740, loss 0.822521, acc 0.6875, learning_rate 0.000341015\n","2023-06-13T17:17:43.394569: step 5741, loss 0.513274, acc 0.875, learning_rate 0.000340898\n","2023-06-13T17:17:45.971981: step 5742, loss 0.572048, acc 0.78125, learning_rate 0.000340781\n","2023-06-13T17:17:48.506293: step 5743, loss 0.67827, acc 0.8125, learning_rate 0.000340664\n","2023-06-13T17:17:50.987028: step 5744, loss 0.749615, acc 0.75, learning_rate 0.000340548\n","2023-06-13T17:17:52.759386: step 5745, loss 0.712543, acc 0.75, learning_rate 0.000340431\n","2023-06-13T17:17:54.240567: step 5746, loss 0.543058, acc 0.8125, learning_rate 0.000340314\n","2023-06-13T17:17:55.699059: step 5747, loss 0.675551, acc 0.78125, learning_rate 0.000340198\n","2023-06-13T17:17:57.173683: step 5748, loss 0.691967, acc 0.75, learning_rate 0.000340081\n","2023-06-13T17:17:58.640085: step 5749, loss 0.662885, acc 0.8125, learning_rate 0.000339965\n","2023-06-13T17:18:00.097204: step 5750, loss 0.791157, acc 0.75, learning_rate 0.000339849\n","2023-06-13T17:18:01.561904: step 5751, loss 0.544124, acc 0.84375, learning_rate 0.000339732\n","2023-06-13T17:18:03.945413: step 5752, loss 0.693131, acc 0.8125, learning_rate 0.000339616\n","2023-06-13T17:18:06.546351: step 5753, loss 0.439784, acc 0.84375, learning_rate 0.0003395\n","2023-06-13T17:18:08.961087: step 5754, loss 0.727847, acc 0.78125, learning_rate 0.000339384\n","2023-06-13T17:18:11.386082: step 5755, loss 0.496911, acc 0.90625, learning_rate 0.000339268\n","2023-06-13T17:18:13.130691: step 5756, loss 0.663519, acc 0.8125, learning_rate 0.000339152\n","2023-06-13T17:18:14.573339: step 5757, loss 0.722425, acc 0.71875, learning_rate 0.000339036\n","2023-06-13T17:18:16.057357: step 5758, loss 0.601154, acc 0.78125, learning_rate 0.00033892\n","2023-06-13T17:18:17.527692: step 5759, loss 0.981582, acc 0.6875, learning_rate 0.000338804\n","2023-06-13T17:18:19.033187: step 5760, loss 0.66773, acc 0.8125, learning_rate 0.000338688\n","2023-06-13T17:18:20.485900: step 5761, loss 0.679887, acc 0.65625, learning_rate 0.000338572\n","2023-06-13T17:18:21.967013: step 5762, loss 0.914174, acc 0.6875, learning_rate 0.000338457\n","2023-06-13T17:18:24.518746: step 5763, loss 0.444124, acc 0.90625, learning_rate 0.000338341\n","2023-06-13T17:18:27.145643: step 5764, loss 0.551187, acc 0.75, learning_rate 0.000338226\n","2023-06-13T17:18:29.563261: step 5765, loss 0.787823, acc 0.75, learning_rate 0.00033811\n","2023-06-13T17:18:32.071628: step 5766, loss 0.449861, acc 0.84375, learning_rate 0.000337995\n","2023-06-13T17:18:33.608396: step 5767, loss 0.529767, acc 0.8125, learning_rate 0.000337879\n","2023-06-13T17:18:35.113477: step 5768, loss 0.587801, acc 0.75, learning_rate 0.000337764\n","2023-06-13T17:18:36.558826: step 5769, loss 0.722179, acc 0.71875, learning_rate 0.000337649\n","2023-06-13T17:18:38.023988: step 5770, loss 0.353841, acc 0.8125, learning_rate 0.000337533\n","2023-06-13T17:18:39.476182: step 5771, loss 0.759213, acc 0.75, learning_rate 0.000337418\n","2023-06-13T17:18:40.931608: step 5772, loss 0.451264, acc 0.84375, learning_rate 0.000337303\n","2023-06-13T17:18:42.512664: step 5773, loss 0.745692, acc 0.8125, learning_rate 0.000337188\n","2023-06-13T17:18:44.565580: step 5774, loss 0.71889, acc 0.708333, learning_rate 0.000337073\n","2023-06-13T17:18:47.162413: step 5775, loss 0.466835, acc 0.84375, learning_rate 0.000336958\n","2023-06-13T17:18:49.528908: step 5776, loss 0.598309, acc 0.84375, learning_rate 0.000336843\n","2023-06-13T17:18:51.893021: step 5777, loss 0.668089, acc 0.8125, learning_rate 0.000336728\n","2023-06-13T17:18:54.136020: step 5778, loss 0.512156, acc 0.90625, learning_rate 0.000336614\n","2023-06-13T17:18:56.736859: step 5779, loss 0.553879, acc 0.90625, learning_rate 0.000336499\n","2023-06-13T17:18:59.228261: step 5780, loss 0.448537, acc 0.875, learning_rate 0.000336384\n","2023-06-13T17:19:01.681888: step 5781, loss 0.527443, acc 0.78125, learning_rate 0.00033627\n","2023-06-13T17:19:04.302677: step 5782, loss 0.668451, acc 0.71875, learning_rate 0.000336155\n","2023-06-13T17:19:06.878898: step 5783, loss 0.762144, acc 0.75, learning_rate 0.00033604\n","2023-06-13T17:19:09.366726: step 5784, loss 0.666516, acc 0.84375, learning_rate 0.000335926\n","2023-06-13T17:19:11.804812: step 5785, loss 0.444932, acc 0.875, learning_rate 0.000335812\n","2023-06-13T17:19:13.885943: step 5786, loss 0.891671, acc 0.6875, learning_rate 0.000335697\n","2023-06-13T17:19:15.337746: step 5787, loss 0.479794, acc 0.78125, learning_rate 0.000335583\n","2023-06-13T17:19:16.820122: step 5788, loss 0.448573, acc 0.84375, learning_rate 0.000335469\n","2023-06-13T17:19:18.276735: step 5789, loss 0.433625, acc 0.84375, learning_rate 0.000335355\n","2023-06-13T17:19:19.770963: step 5790, loss 0.550888, acc 0.78125, learning_rate 0.00033524\n","2023-06-13T17:19:21.232222: step 5791, loss 0.617474, acc 0.75, learning_rate 0.000335126\n","2023-06-13T17:19:22.700284: step 5792, loss 0.519548, acc 0.84375, learning_rate 0.000335012\n","2023-06-13T17:19:24.735452: step 5793, loss 0.44906, acc 0.84375, learning_rate 0.000334898\n","2023-06-13T17:19:27.383938: step 5794, loss 0.253496, acc 0.90625, learning_rate 0.000334785\n","2023-06-13T17:19:29.887831: step 5795, loss 0.41927, acc 0.875, learning_rate 0.000334671\n","2023-06-13T17:19:32.321286: step 5796, loss 0.672949, acc 0.8125, learning_rate 0.000334557\n","2023-06-13T17:19:34.323565: step 5797, loss 0.584219, acc 0.8125, learning_rate 0.000334443\n","2023-06-13T17:19:35.782981: step 5798, loss 0.52992, acc 0.75, learning_rate 0.000334329\n","2023-06-13T17:19:37.253590: step 5799, loss 0.358896, acc 0.90625, learning_rate 0.000334216\n","\n","Evaluation:\n","2023-06-13T17:20:06.812420: step 5800, loss 2.69018, acc 0.384965\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5800\n","\n","2023-06-13T17:20:09.538839: step 5800, loss 0.433609, acc 0.8125, learning_rate 0.000334102\n","2023-06-13T17:20:11.985054: step 5801, loss 0.483057, acc 0.84375, learning_rate 0.000333989\n","2023-06-13T17:20:14.399333: step 5802, loss 0.47633, acc 0.875, learning_rate 0.000333875\n","2023-06-13T17:20:16.095840: step 5803, loss 0.588819, acc 0.78125, learning_rate 0.000333762\n","2023-06-13T17:20:17.671069: step 5804, loss 0.536519, acc 0.78125, learning_rate 0.000333649\n","2023-06-13T17:20:19.162454: step 5805, loss 0.360448, acc 0.875, learning_rate 0.000333535\n","2023-06-13T17:20:20.642460: step 5806, loss 0.793569, acc 0.75, learning_rate 0.000333422\n","2023-06-13T17:20:22.138492: step 5807, loss 0.663207, acc 0.78125, learning_rate 0.000333309\n","2023-06-13T17:20:23.618838: step 5808, loss 0.661647, acc 0.71875, learning_rate 0.000333196\n","2023-06-13T17:20:25.189211: step 5809, loss 0.640467, acc 0.78125, learning_rate 0.000333083\n","2023-06-13T17:20:27.792291: step 5810, loss 0.530972, acc 0.84375, learning_rate 0.00033297\n","2023-06-13T17:20:30.399154: step 5811, loss 0.733149, acc 0.8125, learning_rate 0.000332857\n","2023-06-13T17:20:32.860577: step 5812, loss 0.607355, acc 0.8125, learning_rate 0.000332744\n","2023-06-13T17:20:35.247616: step 5813, loss 0.763239, acc 0.75, learning_rate 0.000332631\n","2023-06-13T17:20:36.721712: step 5814, loss 1.00779, acc 0.5625, learning_rate 0.000332518\n","2023-06-13T17:20:38.191859: step 5815, loss 0.377672, acc 0.8125, learning_rate 0.000332405\n","2023-06-13T17:20:39.638632: step 5816, loss 0.49367, acc 0.875, learning_rate 0.000332293\n","2023-06-13T17:20:41.085782: step 5817, loss 0.396667, acc 0.90625, learning_rate 0.00033218\n","2023-06-13T17:20:42.539188: step 5818, loss 0.542813, acc 0.78125, learning_rate 0.000332068\n","2023-06-13T17:20:43.991663: step 5819, loss 0.538399, acc 0.78125, learning_rate 0.000331955\n","2023-06-13T17:20:45.591460: step 5820, loss 0.579483, acc 0.8125, learning_rate 0.000331843\n","2023-06-13T17:20:48.234291: step 5821, loss 0.697573, acc 0.75, learning_rate 0.00033173\n","2023-06-13T17:20:50.751595: step 5822, loss 0.322987, acc 0.90625, learning_rate 0.000331618\n","2023-06-13T17:20:53.082616: step 5823, loss 0.627874, acc 0.8125, learning_rate 0.000331505\n","2023-06-13T17:20:55.474404: step 5824, loss 0.528812, acc 0.8125, learning_rate 0.000331393\n","2023-06-13T17:20:56.948265: step 5825, loss 0.830417, acc 0.6875, learning_rate 0.000331281\n","2023-06-13T17:20:58.401351: step 5826, loss 1.28699, acc 0.65625, learning_rate 0.000331169\n","2023-06-13T17:20:59.881756: step 5827, loss 0.391718, acc 0.90625, learning_rate 0.000331057\n","2023-06-13T17:21:01.327531: step 5828, loss 0.608727, acc 0.78125, learning_rate 0.000330945\n","2023-06-13T17:21:02.805445: step 5829, loss 0.733237, acc 0.8125, learning_rate 0.000330833\n","2023-06-13T17:21:04.322391: step 5830, loss 0.626018, acc 0.78125, learning_rate 0.000330721\n","2023-06-13T17:21:05.944966: step 5831, loss 0.457668, acc 0.8125, learning_rate 0.000330609\n","2023-06-13T17:21:08.486301: step 5832, loss 0.568913, acc 0.8125, learning_rate 0.000330497\n","2023-06-13T17:21:10.880364: step 5833, loss 1.1038, acc 0.59375, learning_rate 0.000330385\n","2023-06-13T17:21:13.327748: step 5834, loss 0.527969, acc 0.8125, learning_rate 0.000330274\n","2023-06-13T17:21:15.617405: step 5835, loss 0.563524, acc 0.84375, learning_rate 0.000330162\n","2023-06-13T17:21:17.297855: step 5836, loss 0.598467, acc 0.8125, learning_rate 0.00033005\n","2023-06-13T17:21:18.763477: step 5837, loss 0.411372, acc 0.90625, learning_rate 0.000329939\n","2023-06-13T17:21:20.246995: step 5838, loss 0.454438, acc 0.78125, learning_rate 0.000329827\n","2023-06-13T17:21:21.716336: step 5839, loss 0.675166, acc 0.8125, learning_rate 0.000329716\n","2023-06-13T17:21:24.025305: step 5840, loss 0.624749, acc 0.84375, learning_rate 0.000329605\n","2023-06-13T17:21:26.711973: step 5841, loss 0.605665, acc 0.84375, learning_rate 0.000329493\n","2023-06-13T17:21:29.571684: step 5842, loss 0.575634, acc 0.78125, learning_rate 0.000329382\n","2023-06-13T17:21:32.404308: step 5843, loss 0.639109, acc 0.71875, learning_rate 0.000329271\n","2023-06-13T17:21:34.991514: step 5844, loss 0.602237, acc 0.78125, learning_rate 0.00032916\n","2023-06-13T17:21:37.566526: step 5845, loss 0.554538, acc 0.84375, learning_rate 0.000329048\n","2023-06-13T17:21:39.995305: step 5846, loss 0.969566, acc 0.6875, learning_rate 0.000328937\n","2023-06-13T17:21:41.684018: step 5847, loss 0.356288, acc 0.875, learning_rate 0.000328826\n","2023-06-13T17:21:43.163726: step 5848, loss 0.58265, acc 0.84375, learning_rate 0.000328715\n","2023-06-13T17:21:44.621900: step 5849, loss 0.330852, acc 0.875, learning_rate 0.000328604\n","2023-06-13T17:21:46.124081: step 5850, loss 0.522809, acc 0.75, learning_rate 0.000328494\n","2023-06-13T17:21:47.594238: step 5851, loss 0.384162, acc 0.9375, learning_rate 0.000328383\n","2023-06-13T17:21:49.066357: step 5852, loss 0.510471, acc 0.84375, learning_rate 0.000328272\n","2023-06-13T17:21:50.557073: step 5853, loss 0.478158, acc 0.78125, learning_rate 0.000328161\n","2023-06-13T17:21:53.060238: step 5854, loss 0.447653, acc 0.875, learning_rate 0.000328051\n","2023-06-13T17:21:55.488214: step 5855, loss 0.483972, acc 0.84375, learning_rate 0.00032794\n","2023-06-13T17:21:57.977922: step 5856, loss 0.45871, acc 0.8125, learning_rate 0.00032783\n","2023-06-13T17:22:00.581224: step 5857, loss 0.298363, acc 0.9375, learning_rate 0.000327719\n","2023-06-13T17:22:02.066752: step 5858, loss 0.821808, acc 0.71875, learning_rate 0.000327609\n","2023-06-13T17:22:03.592019: step 5859, loss 0.448931, acc 0.90625, learning_rate 0.000327498\n","2023-06-13T17:22:05.076011: step 5860, loss 0.629216, acc 0.84375, learning_rate 0.000327388\n","2023-06-13T17:22:06.543191: step 5861, loss 0.551628, acc 0.78125, learning_rate 0.000327278\n","2023-06-13T17:22:08.044366: step 5862, loss 0.637909, acc 0.78125, learning_rate 0.000327168\n","2023-06-13T17:22:09.519245: step 5863, loss 0.765885, acc 0.71875, learning_rate 0.000327058\n","2023-06-13T17:22:11.301563: step 5864, loss 0.484121, acc 0.8125, learning_rate 0.000326947\n","2023-06-13T17:22:13.917033: step 5865, loss 0.743007, acc 0.6875, learning_rate 0.000326837\n","2023-06-13T17:22:16.505718: step 5866, loss 0.576808, acc 0.8125, learning_rate 0.000326727\n","2023-06-13T17:22:18.898121: step 5867, loss 0.250891, acc 0.9375, learning_rate 0.000326617\n","2023-06-13T17:22:21.131177: step 5868, loss 0.615843, acc 0.875, learning_rate 0.000326508\n","2023-06-13T17:22:22.599502: step 5869, loss 0.368521, acc 0.8125, learning_rate 0.000326398\n","2023-06-13T17:22:24.063252: step 5870, loss 0.691796, acc 0.71875, learning_rate 0.000326288\n","2023-06-13T17:22:25.571078: step 5871, loss 0.956489, acc 0.75, learning_rate 0.000326178\n","2023-06-13T17:22:27.104486: step 5872, loss 0.456465, acc 0.875, learning_rate 0.000326069\n","2023-06-13T17:22:28.583655: step 5873, loss 0.716726, acc 0.75, learning_rate 0.000325959\n","2023-06-13T17:22:30.045376: step 5874, loss 0.648497, acc 0.8125, learning_rate 0.000325849\n","2023-06-13T17:22:32.045222: step 5875, loss 0.39002, acc 0.875, learning_rate 0.00032574\n","2023-06-13T17:22:34.573538: step 5876, loss 0.54124, acc 0.78125, learning_rate 0.00032563\n","2023-06-13T17:22:36.992236: step 5877, loss 0.722642, acc 0.78125, learning_rate 0.000325521\n","2023-06-13T17:22:39.429336: step 5878, loss 0.666291, acc 0.75, learning_rate 0.000325412\n","2023-06-13T17:22:41.619719: step 5879, loss 0.591794, acc 0.78125, learning_rate 0.000325302\n","2023-06-13T17:22:43.101829: step 5880, loss 0.597849, acc 0.8125, learning_rate 0.000325193\n","2023-06-13T17:22:44.555300: step 5881, loss 0.718972, acc 0.75, learning_rate 0.000325084\n","2023-06-13T17:22:46.036328: step 5882, loss 0.504329, acc 0.78125, learning_rate 0.000324975\n","2023-06-13T17:22:47.488480: step 5883, loss 0.4925, acc 0.8125, learning_rate 0.000324866\n","2023-06-13T17:22:48.974785: step 5884, loss 0.404713, acc 0.8125, learning_rate 0.000324757\n","2023-06-13T17:22:50.430895: step 5885, loss 0.345345, acc 0.90625, learning_rate 0.000324648\n","2023-06-13T17:22:52.346937: step 5886, loss 0.473617, acc 0.8125, learning_rate 0.000324539\n","2023-06-13T17:22:54.973943: step 5887, loss 0.432354, acc 0.875, learning_rate 0.00032443\n","2023-06-13T17:22:57.435951: step 5888, loss 0.781002, acc 0.75, learning_rate 0.000324321\n","2023-06-13T17:22:59.905173: step 5889, loss 0.391634, acc 0.90625, learning_rate 0.000324212\n","2023-06-13T17:23:02.058742: step 5890, loss 0.511521, acc 0.75, learning_rate 0.000324104\n","2023-06-13T17:23:03.550656: step 5891, loss 0.685566, acc 0.6875, learning_rate 0.000323995\n","2023-06-13T17:23:05.071535: step 5892, loss 0.660288, acc 0.71875, learning_rate 0.000323886\n","2023-06-13T17:23:06.592168: step 5893, loss 0.575443, acc 0.8125, learning_rate 0.000323778\n","2023-06-13T17:23:08.077085: step 5894, loss 0.437191, acc 0.8125, learning_rate 0.000323669\n","2023-06-13T17:23:09.523130: step 5895, loss 0.597544, acc 0.875, learning_rate 0.000323561\n","2023-06-13T17:23:10.983688: step 5896, loss 0.615867, acc 0.71875, learning_rate 0.000323452\n","2023-06-13T17:23:13.099885: step 5897, loss 0.882478, acc 0.78125, learning_rate 0.000323344\n","2023-06-13T17:23:15.668400: step 5898, loss 0.434225, acc 0.875, learning_rate 0.000323236\n","2023-06-13T17:23:18.028217: step 5899, loss 0.446263, acc 0.9375, learning_rate 0.000323128\n","\n","Evaluation:\n","2023-06-13T17:23:48.348291: step 5900, loss 2.73277, acc 0.382085\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-5900\n","\n","2023-06-13T17:23:50.025695: step 5900, loss 0.547419, acc 0.84375, learning_rate 0.000323019\n","2023-06-13T17:23:51.479551: step 5901, loss 0.565795, acc 0.8125, learning_rate 0.000322911\n","2023-06-13T17:23:53.039860: step 5902, loss 0.62247, acc 0.75, learning_rate 0.000322803\n","2023-06-13T17:23:55.610407: step 5903, loss 0.555657, acc 0.8125, learning_rate 0.000322695\n","2023-06-13T17:23:58.161739: step 5904, loss 0.510786, acc 0.875, learning_rate 0.000322587\n","2023-06-13T17:24:00.558151: step 5905, loss 0.539129, acc 0.875, learning_rate 0.000322479\n","2023-06-13T17:24:03.016688: step 5906, loss 0.307088, acc 0.9375, learning_rate 0.000322371\n","2023-06-13T17:24:04.657351: step 5907, loss 0.747858, acc 0.75, learning_rate 0.000322264\n","2023-06-13T17:24:06.112915: step 5908, loss 0.383055, acc 0.875, learning_rate 0.000322156\n","2023-06-13T17:24:07.604638: step 5909, loss 0.511699, acc 0.875, learning_rate 0.000322048\n","2023-06-13T17:24:09.076452: step 5910, loss 0.57758, acc 0.75, learning_rate 0.00032194\n","2023-06-13T17:24:10.605153: step 5911, loss 0.486035, acc 0.84375, learning_rate 0.000321833\n","2023-06-13T17:24:12.099209: step 5912, loss 0.367523, acc 0.875, learning_rate 0.000321725\n","2023-06-13T17:24:13.654570: step 5913, loss 0.527523, acc 0.875, learning_rate 0.000321618\n","2023-06-13T17:24:16.218977: step 5914, loss 0.4275, acc 0.875, learning_rate 0.00032151\n","2023-06-13T17:24:18.641546: step 5915, loss 0.556508, acc 0.84375, learning_rate 0.000321403\n","2023-06-13T17:24:21.171436: step 5916, loss 0.597927, acc 0.8125, learning_rate 0.000321295\n","2023-06-13T17:24:23.620626: step 5917, loss 0.894911, acc 0.75, learning_rate 0.000321188\n","2023-06-13T17:24:25.135076: step 5918, loss 0.791395, acc 0.65625, learning_rate 0.000321081\n","2023-06-13T17:24:26.626446: step 5919, loss 0.675644, acc 0.71875, learning_rate 0.000320974\n","2023-06-13T17:24:28.117936: step 5920, loss 0.624125, acc 0.8125, learning_rate 0.000320867\n","2023-06-13T17:24:29.607310: step 5921, loss 0.523741, acc 0.78125, learning_rate 0.000320759\n","2023-06-13T17:24:31.077661: step 5922, loss 0.542558, acc 0.78125, learning_rate 0.000320652\n","2023-06-13T17:24:32.537539: step 5923, loss 0.721151, acc 0.75, learning_rate 0.000320545\n","2023-06-13T17:24:34.088380: step 5924, loss 0.729136, acc 0.8125, learning_rate 0.000320439\n","2023-06-13T17:24:36.581192: step 5925, loss 0.764157, acc 0.71875, learning_rate 0.000320332\n","2023-06-13T17:24:39.025368: step 5926, loss 0.461762, acc 0.84375, learning_rate 0.000320225\n","2023-06-13T17:24:41.502496: step 5927, loss 0.783143, acc 0.75, learning_rate 0.000320118\n","2023-06-13T17:24:43.932146: step 5928, loss 0.357533, acc 0.875, learning_rate 0.000320011\n","2023-06-13T17:24:45.594332: step 5929, loss 0.492567, acc 0.84375, learning_rate 0.000319905\n","2023-06-13T17:24:47.057297: step 5930, loss 0.313962, acc 0.875, learning_rate 0.000319798\n","2023-06-13T17:24:48.510462: step 5931, loss 0.542367, acc 0.84375, learning_rate 0.000319691\n","2023-06-13T17:24:49.970314: step 5932, loss 0.271854, acc 0.9375, learning_rate 0.000319585\n","2023-06-13T17:24:51.478470: step 5933, loss 0.369628, acc 0.875, learning_rate 0.000319478\n","2023-06-13T17:24:52.935710: step 5934, loss 0.591004, acc 0.875, learning_rate 0.000319372\n","2023-06-13T17:24:54.434917: step 5935, loss 0.268468, acc 0.9375, learning_rate 0.000319266\n","2023-06-13T17:24:56.948415: step 5936, loss 0.486527, acc 0.90625, learning_rate 0.000319159\n","2023-06-13T17:24:59.491068: step 5937, loss 0.425825, acc 0.84375, learning_rate 0.000319053\n","2023-06-13T17:25:01.758201: step 5938, loss 0.58864, acc 0.78125, learning_rate 0.000318947\n","2023-06-13T17:25:04.064931: step 5939, loss 0.357225, acc 0.84375, learning_rate 0.000318841\n","2023-06-13T17:25:05.516121: step 5940, loss 0.374481, acc 0.84375, learning_rate 0.000318735\n","2023-06-13T17:25:06.981933: step 5941, loss 0.862622, acc 0.625, learning_rate 0.000318628\n","2023-06-13T17:25:08.489263: step 5942, loss 0.546816, acc 0.84375, learning_rate 0.000318522\n","2023-06-13T17:25:09.990497: step 5943, loss 0.39306, acc 0.90625, learning_rate 0.000318417\n","2023-06-13T17:25:11.455372: step 5944, loss 0.505127, acc 0.75, learning_rate 0.000318311\n","2023-06-13T17:25:12.917702: step 5945, loss 0.517276, acc 0.875, learning_rate 0.000318205\n","2023-06-13T17:25:14.781843: step 5946, loss 0.429481, acc 0.8125, learning_rate 0.000318099\n","2023-06-13T17:25:17.211281: step 5947, loss 0.644439, acc 0.78125, learning_rate 0.000317993\n","2023-06-13T17:25:19.487888: step 5948, loss 0.657076, acc 0.75, learning_rate 0.000317888\n","2023-06-13T17:25:21.866777: step 5949, loss 0.590097, acc 0.84375, learning_rate 0.000317782\n","2023-06-13T17:25:23.971722: step 5950, loss 0.592786, acc 0.8125, learning_rate 0.000317676\n","2023-06-13T17:25:25.424995: step 5951, loss 0.314335, acc 0.9375, learning_rate 0.000317571\n","2023-06-13T17:25:26.895174: step 5952, loss 0.451544, acc 0.8125, learning_rate 0.000317465\n","2023-06-13T17:25:28.368406: step 5953, loss 0.390787, acc 0.8125, learning_rate 0.00031736\n","2023-06-13T17:25:29.816623: step 5954, loss 0.405789, acc 0.875, learning_rate 0.000317254\n","2023-06-13T17:25:31.251841: step 5955, loss 0.822833, acc 0.6875, learning_rate 0.000317149\n","2023-06-13T17:25:32.745455: step 5956, loss 0.694582, acc 0.84375, learning_rate 0.000317044\n","2023-06-13T17:25:34.627878: step 5957, loss 0.836245, acc 0.71875, learning_rate 0.000316939\n","2023-06-13T17:25:37.225503: step 5958, loss 0.781022, acc 0.75, learning_rate 0.000316833\n","2023-06-13T17:25:39.617443: step 5959, loss 0.551186, acc 0.84375, learning_rate 0.000316728\n","2023-06-13T17:25:42.043410: step 5960, loss 0.741132, acc 0.78125, learning_rate 0.000316623\n","2023-06-13T17:25:43.828478: step 5961, loss 0.398939, acc 0.84375, learning_rate 0.000316518\n","2023-06-13T17:25:45.310326: step 5962, loss 0.679268, acc 0.71875, learning_rate 0.000316413\n","2023-06-13T17:25:46.776235: step 5963, loss 0.424413, acc 0.875, learning_rate 0.000316308\n","2023-06-13T17:25:48.236359: step 5964, loss 0.403359, acc 0.875, learning_rate 0.000316203\n","2023-06-13T17:25:49.697912: step 5965, loss 0.61423, acc 0.875, learning_rate 0.000316098\n","2023-06-13T17:25:51.180878: step 5966, loss 0.648934, acc 0.84375, learning_rate 0.000315994\n","2023-06-13T17:25:52.646709: step 5967, loss 0.353923, acc 0.875, learning_rate 0.000315889\n","2023-06-13T17:25:54.908232: step 5968, loss 0.634617, acc 0.71875, learning_rate 0.000315784\n","2023-06-13T17:25:57.415905: step 5969, loss 0.607203, acc 0.71875, learning_rate 0.00031568\n","2023-06-13T17:25:59.668980: step 5970, loss 0.736255, acc 0.8125, learning_rate 0.000315575\n","2023-06-13T17:26:02.205015: step 5971, loss 0.487276, acc 0.8125, learning_rate 0.00031547\n","2023-06-13T17:26:04.036587: step 5972, loss 0.407459, acc 0.84375, learning_rate 0.000315366\n","2023-06-13T17:26:06.636532: step 5973, loss 0.309428, acc 0.9375, learning_rate 0.000315262\n","2023-06-13T17:26:09.057364: step 5974, loss 0.540181, acc 0.875, learning_rate 0.000315157\n","2023-06-13T17:26:11.295496: step 5975, loss 0.542346, acc 0.8125, learning_rate 0.000315053\n","2023-06-13T17:26:13.873293: step 5976, loss 0.536242, acc 0.8125, learning_rate 0.000314949\n","2023-06-13T17:26:16.285767: step 5977, loss 0.510321, acc 0.8125, learning_rate 0.000314844\n","2023-06-13T17:26:18.455307: step 5978, loss 1.08188, acc 0.5625, learning_rate 0.00031474\n","2023-06-13T17:26:21.058235: step 5979, loss 0.475061, acc 0.8125, learning_rate 0.000314636\n","2023-06-13T17:26:23.208700: step 5980, loss 0.532269, acc 0.875, learning_rate 0.000314532\n","2023-06-13T17:26:24.688559: step 5981, loss 0.563484, acc 0.84375, learning_rate 0.000314428\n","2023-06-13T17:26:26.136824: step 5982, loss 0.602104, acc 0.84375, learning_rate 0.000314324\n","2023-06-13T17:26:27.615780: step 5983, loss 0.473684, acc 0.8125, learning_rate 0.00031422\n","2023-06-13T17:26:29.069744: step 5984, loss 0.710516, acc 0.75, learning_rate 0.000314116\n","2023-06-13T17:26:30.520438: step 5985, loss 0.433319, acc 0.84375, learning_rate 0.000314012\n","2023-06-13T17:26:31.963525: step 5986, loss 0.34067, acc 0.90625, learning_rate 0.000313909\n","2023-06-13T17:26:33.834656: step 5987, loss 0.950923, acc 0.84375, learning_rate 0.000313805\n","2023-06-13T17:26:36.389495: step 5988, loss 0.448658, acc 0.875, learning_rate 0.000313701\n","2023-06-13T17:26:38.823922: step 5989, loss 0.224775, acc 0.9375, learning_rate 0.000313598\n","2023-06-13T17:26:41.224414: step 5990, loss 0.767421, acc 0.78125, learning_rate 0.000313494\n","2023-06-13T17:26:43.041839: step 5991, loss 0.288932, acc 0.90625, learning_rate 0.000313391\n","2023-06-13T17:26:44.518520: step 5992, loss 0.432365, acc 0.84375, learning_rate 0.000313287\n","2023-06-13T17:26:45.993534: step 5993, loss 0.495208, acc 0.8125, learning_rate 0.000313184\n","2023-06-13T17:26:47.431990: step 5994, loss 0.435339, acc 0.875, learning_rate 0.00031308\n","2023-06-13T17:26:48.872472: step 5995, loss 0.5628, acc 0.8125, learning_rate 0.000312977\n","2023-06-13T17:26:50.316360: step 5996, loss 0.576525, acc 0.78125, learning_rate 0.000312874\n","2023-06-13T17:26:51.749999: step 5997, loss 0.824321, acc 0.71875, learning_rate 0.00031277\n","2023-06-13T17:26:53.987949: step 5998, loss 0.624985, acc 0.84375, learning_rate 0.000312667\n","2023-06-13T17:26:56.462466: step 5999, loss 0.408426, acc 0.875, learning_rate 0.000312564\n","\n","Evaluation:\n","2023-06-13T17:27:26.625227: step 6000, loss 2.74121, acc 0.385117\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6000\n","\n","2023-06-13T17:27:28.300980: step 6000, loss 0.760816, acc 0.71875, learning_rate 0.000312461\n","2023-06-13T17:27:29.746737: step 6001, loss 0.887117, acc 0.75, learning_rate 0.000312358\n","2023-06-13T17:27:31.186864: step 6002, loss 0.476274, acc 0.8125, learning_rate 0.000312255\n","2023-06-13T17:27:32.746074: step 6003, loss 0.399528, acc 0.90625, learning_rate 0.000312152\n","2023-06-13T17:27:35.298561: step 6004, loss 0.304407, acc 0.90625, learning_rate 0.000312049\n","2023-06-13T17:27:37.741608: step 6005, loss 0.416188, acc 0.875, learning_rate 0.000311947\n","2023-06-13T17:27:40.055862: step 6006, loss 0.449536, acc 0.84375, learning_rate 0.000311844\n","2023-06-13T17:27:42.497485: step 6007, loss 0.787176, acc 0.65625, learning_rate 0.000311741\n","2023-06-13T17:27:43.995718: step 6008, loss 0.328188, acc 0.90625, learning_rate 0.000311638\n","2023-06-13T17:27:45.478430: step 6009, loss 0.548174, acc 0.8125, learning_rate 0.000311536\n","2023-06-13T17:27:46.949253: step 6010, loss 0.409523, acc 0.875, learning_rate 0.000311433\n","2023-06-13T17:27:48.412480: step 6011, loss 0.615864, acc 0.8125, learning_rate 0.000311331\n","2023-06-13T17:27:49.864195: step 6012, loss 0.693343, acc 0.8125, learning_rate 0.000311228\n","2023-06-13T17:27:51.306865: step 6013, loss 0.45284, acc 0.8125, learning_rate 0.000311126\n","2023-06-13T17:27:53.131241: step 6014, loss 0.628307, acc 0.78125, learning_rate 0.000311023\n","2023-06-13T17:27:55.728160: step 6015, loss 0.47438, acc 0.8125, learning_rate 0.000310921\n","2023-06-13T17:27:58.136450: step 6016, loss 0.557635, acc 0.71875, learning_rate 0.000310819\n","2023-06-13T17:28:00.669724: step 6017, loss 0.564592, acc 0.78125, learning_rate 0.000310717\n","2023-06-13T17:28:02.575226: step 6018, loss 0.568408, acc 0.78125, learning_rate 0.000310614\n","2023-06-13T17:28:04.016847: step 6019, loss 0.911704, acc 0.84375, learning_rate 0.000310512\n","2023-06-13T17:28:05.518937: step 6020, loss 0.60229, acc 0.78125, learning_rate 0.00031041\n","2023-06-13T17:28:06.978050: step 6021, loss 0.551584, acc 0.875, learning_rate 0.000310308\n","2023-06-13T17:28:08.446745: step 6022, loss 0.532698, acc 0.75, learning_rate 0.000310206\n","2023-06-13T17:28:09.895375: step 6023, loss 0.54333, acc 0.8125, learning_rate 0.000310104\n","2023-06-13T17:28:11.358965: step 6024, loss 0.518787, acc 0.8125, learning_rate 0.000310002\n","2023-06-13T17:28:13.532037: step 6025, loss 0.352704, acc 0.875, learning_rate 0.000309901\n","2023-06-13T17:28:15.980480: step 6026, loss 0.786501, acc 0.6875, learning_rate 0.000309799\n","2023-06-13T17:28:18.390100: step 6027, loss 0.450167, acc 0.84375, learning_rate 0.000309697\n","2023-06-13T17:28:20.837111: step 6028, loss 0.583991, acc 0.875, learning_rate 0.000309595\n","2023-06-13T17:28:22.492995: step 6029, loss 0.424672, acc 0.84375, learning_rate 0.000309494\n","2023-06-13T17:28:23.941957: step 6030, loss 0.42964, acc 0.84375, learning_rate 0.000309392\n","2023-06-13T17:28:25.475039: step 6031, loss 0.523782, acc 0.875, learning_rate 0.000309291\n","2023-06-13T17:28:26.944491: step 6032, loss 0.661479, acc 0.8125, learning_rate 0.000309189\n","2023-06-13T17:28:28.405109: step 6033, loss 0.461224, acc 0.875, learning_rate 0.000309088\n","2023-06-13T17:28:29.872041: step 6034, loss 0.692524, acc 0.8125, learning_rate 0.000308986\n","2023-06-13T17:28:31.422706: step 6035, loss 0.857355, acc 0.78125, learning_rate 0.000308885\n","2023-06-13T17:28:33.942854: step 6036, loss 0.125449, acc 0.96875, learning_rate 0.000308784\n","2023-06-13T17:28:36.368114: step 6037, loss 0.450148, acc 0.84375, learning_rate 0.000308683\n","2023-06-13T17:28:38.757379: step 6038, loss 0.691526, acc 0.84375, learning_rate 0.000308581\n","2023-06-13T17:28:41.084986: step 6039, loss 0.708896, acc 0.8125, learning_rate 0.00030848\n","2023-06-13T17:28:42.580834: step 6040, loss 0.42904, acc 0.84375, learning_rate 0.000308379\n","2023-06-13T17:28:44.052845: step 6041, loss 0.398629, acc 0.84375, learning_rate 0.000308278\n","2023-06-13T17:28:45.507379: step 6042, loss 0.602222, acc 0.78125, learning_rate 0.000308177\n","2023-06-13T17:28:46.976035: step 6043, loss 0.805617, acc 0.78125, learning_rate 0.000308076\n","2023-06-13T17:28:48.417299: step 6044, loss 0.303116, acc 0.9375, learning_rate 0.000307975\n","2023-06-13T17:28:49.888216: step 6045, loss 0.59093, acc 0.84375, learning_rate 0.000307874\n","2023-06-13T17:28:51.544160: step 6046, loss 0.690216, acc 0.71875, learning_rate 0.000307774\n","2023-06-13T17:28:54.209496: step 6047, loss 0.581716, acc 0.8125, learning_rate 0.000307673\n","2023-06-13T17:28:56.562696: step 6048, loss 0.872129, acc 0.6875, learning_rate 0.000307572\n","2023-06-13T17:28:58.903675: step 6049, loss 0.506234, acc 0.78125, learning_rate 0.000307472\n","2023-06-13T17:29:01.038711: step 6050, loss 0.642669, acc 0.75, learning_rate 0.000307371\n","2023-06-13T17:29:02.524734: step 6051, loss 0.323468, acc 0.90625, learning_rate 0.00030727\n","2023-06-13T17:29:04.018087: step 6052, loss 0.382329, acc 0.84375, learning_rate 0.00030717\n","2023-06-13T17:29:05.538539: step 6053, loss 0.516582, acc 0.84375, learning_rate 0.000307069\n","2023-06-13T17:29:07.049447: step 6054, loss 0.572442, acc 0.875, learning_rate 0.000306969\n","2023-06-13T17:29:08.575636: step 6055, loss 0.529855, acc 0.875, learning_rate 0.000306869\n","2023-06-13T17:29:10.075718: step 6056, loss 0.669482, acc 0.8125, learning_rate 0.000306768\n","2023-06-13T17:29:12.247504: step 6057, loss 0.760953, acc 0.8125, learning_rate 0.000306668\n","2023-06-13T17:29:14.824203: step 6058, loss 0.477121, acc 0.8125, learning_rate 0.000306568\n","2023-06-13T17:29:17.290355: step 6059, loss 0.774169, acc 0.6875, learning_rate 0.000306468\n","2023-06-13T17:29:19.705361: step 6060, loss 0.351235, acc 0.90625, learning_rate 0.000306368\n","2023-06-13T17:29:21.234562: step 6061, loss 0.493608, acc 0.84375, learning_rate 0.000306268\n","2023-06-13T17:29:22.693007: step 6062, loss 0.465486, acc 0.875, learning_rate 0.000306168\n","2023-06-13T17:29:24.138269: step 6063, loss 0.508455, acc 0.84375, learning_rate 0.000306068\n","2023-06-13T17:29:25.612046: step 6064, loss 0.594584, acc 0.78125, learning_rate 0.000305968\n","2023-06-13T17:29:27.068898: step 6065, loss 0.503692, acc 0.8125, learning_rate 0.000305868\n","2023-06-13T17:29:28.548832: step 6066, loss 0.407131, acc 0.90625, learning_rate 0.000305768\n","2023-06-13T17:29:30.053760: step 6067, loss 0.321466, acc 0.875, learning_rate 0.000305668\n","2023-06-13T17:29:32.658657: step 6068, loss 0.449097, acc 0.84375, learning_rate 0.000305569\n","2023-06-13T17:29:35.055737: step 6069, loss 0.877614, acc 0.65625, learning_rate 0.000305469\n","2023-06-13T17:29:37.339738: step 6070, loss 0.474638, acc 0.84375, learning_rate 0.000305369\n","2023-06-13T17:29:39.565777: step 6071, loss 0.48244, acc 0.84375, learning_rate 0.00030527\n","2023-06-13T17:29:41.091201: step 6072, loss 0.450543, acc 0.8125, learning_rate 0.00030517\n","2023-06-13T17:29:42.548608: step 6073, loss 0.370355, acc 0.90625, learning_rate 0.000305071\n","2023-06-13T17:29:44.037880: step 6074, loss 0.644345, acc 0.71875, learning_rate 0.000304971\n","2023-06-13T17:29:45.498025: step 6075, loss 0.988696, acc 0.71875, learning_rate 0.000304872\n","2023-06-13T17:29:46.979925: step 6076, loss 0.606387, acc 0.75, learning_rate 0.000304772\n","2023-06-13T17:29:48.451419: step 6077, loss 0.639916, acc 0.78125, learning_rate 0.000304673\n","2023-06-13T17:29:49.996306: step 6078, loss 0.669546, acc 0.75, learning_rate 0.000304574\n","2023-06-13T17:29:52.679801: step 6079, loss 0.386378, acc 0.90625, learning_rate 0.000304475\n","2023-06-13T17:29:55.072667: step 6080, loss 0.751497, acc 0.6875, learning_rate 0.000304376\n","2023-06-13T17:29:57.420740: step 6081, loss 0.613556, acc 0.8125, learning_rate 0.000304277\n","2023-06-13T17:29:59.744056: step 6082, loss 0.70264, acc 0.75, learning_rate 0.000304177\n","2023-06-13T17:30:01.218499: step 6083, loss 0.438703, acc 0.84375, learning_rate 0.000304078\n","2023-06-13T17:30:02.751201: step 6084, loss 0.455995, acc 0.84375, learning_rate 0.000303979\n","2023-06-13T17:30:04.223813: step 6085, loss 0.764818, acc 0.6875, learning_rate 0.000303881\n","2023-06-13T17:30:05.729288: step 6086, loss 1.05655, acc 0.625, learning_rate 0.000303782\n","2023-06-13T17:30:07.196633: step 6087, loss 0.437994, acc 0.875, learning_rate 0.000303683\n","2023-06-13T17:30:08.699260: step 6088, loss 0.367184, acc 0.875, learning_rate 0.000303584\n","2023-06-13T17:30:10.471092: step 6089, loss 0.375112, acc 0.875, learning_rate 0.000303485\n","2023-06-13T17:30:13.022389: step 6090, loss 0.596005, acc 0.78125, learning_rate 0.000303387\n","2023-06-13T17:30:15.335208: step 6091, loss 0.858313, acc 0.78125, learning_rate 0.000303288\n","2023-06-13T17:30:17.769420: step 6092, loss 0.297984, acc 0.875, learning_rate 0.00030319\n","2023-06-13T17:30:19.932276: step 6093, loss 0.76821, acc 0.75, learning_rate 0.000303091\n","2023-06-13T17:30:21.390258: step 6094, loss 0.46347, acc 0.84375, learning_rate 0.000302993\n","2023-06-13T17:30:22.881365: step 6095, loss 0.694562, acc 0.78125, learning_rate 0.000302894\n","2023-06-13T17:30:24.347305: step 6096, loss 0.567174, acc 0.78125, learning_rate 0.000302796\n","2023-06-13T17:30:25.860213: step 6097, loss 0.608496, acc 0.75, learning_rate 0.000302697\n","2023-06-13T17:30:27.362421: step 6098, loss 0.358754, acc 0.9375, learning_rate 0.000302599\n","2023-06-13T17:30:28.828554: step 6099, loss 0.348789, acc 0.875, learning_rate 0.000302501\n","\n","Evaluation:\n","2023-06-13T17:31:05.556254: step 6100, loss 2.78011, acc 0.384207\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6100\n","\n","2023-06-13T17:31:07.262705: step 6100, loss 0.533899, acc 0.84375, learning_rate 0.000302403\n","2023-06-13T17:31:08.778053: step 6101, loss 0.854818, acc 0.6875, learning_rate 0.000302305\n","2023-06-13T17:31:10.227356: step 6102, loss 0.433562, acc 0.84375, learning_rate 0.000302207\n","2023-06-13T17:31:11.672292: step 6103, loss 0.475153, acc 0.84375, learning_rate 0.000302108\n","2023-06-13T17:31:13.237227: step 6104, loss 0.483609, acc 0.8125, learning_rate 0.00030201\n","2023-06-13T17:31:14.741388: step 6105, loss 0.148122, acc 0.96875, learning_rate 0.000301913\n","2023-06-13T17:31:17.326511: step 6106, loss 0.572364, acc 0.78125, learning_rate 0.000301815\n","2023-06-13T17:31:19.761119: step 6107, loss 0.438685, acc 0.90625, learning_rate 0.000301717\n","2023-06-13T17:31:22.231198: step 6108, loss 0.547658, acc 0.875, learning_rate 0.000301619\n","2023-06-13T17:31:24.336673: step 6109, loss 0.472158, acc 0.8125, learning_rate 0.000301521\n","2023-06-13T17:31:25.813043: step 6110, loss 0.819582, acc 0.75, learning_rate 0.000301423\n","2023-06-13T17:31:27.329784: step 6111, loss 0.568924, acc 0.8125, learning_rate 0.000301326\n","2023-06-13T17:31:28.824713: step 6112, loss 0.528179, acc 0.8125, learning_rate 0.000301228\n","2023-06-13T17:31:30.276503: step 6113, loss 0.454779, acc 0.8125, learning_rate 0.000301131\n","2023-06-13T17:31:31.740328: step 6114, loss 1.00748, acc 0.625, learning_rate 0.000301033\n","2023-06-13T17:31:33.187454: step 6115, loss 0.367405, acc 0.90625, learning_rate 0.000300936\n","2023-06-13T17:31:35.266007: step 6116, loss 0.454989, acc 0.8125, learning_rate 0.000300838\n","2023-06-13T17:31:37.738641: step 6117, loss 0.490509, acc 0.8125, learning_rate 0.000300741\n","2023-06-13T17:31:40.257362: step 6118, loss 0.798442, acc 0.625, learning_rate 0.000300643\n","2023-06-13T17:31:42.745991: step 6119, loss 0.812917, acc 0.6875, learning_rate 0.000300546\n","2023-06-13T17:31:44.387132: step 6120, loss 0.487498, acc 0.84375, learning_rate 0.000300449\n","2023-06-13T17:31:45.873437: step 6121, loss 0.302332, acc 0.875, learning_rate 0.000300352\n","2023-06-13T17:31:47.354441: step 6122, loss 1.27199, acc 0.625, learning_rate 0.000300255\n","2023-06-13T17:31:48.834609: step 6123, loss 0.447376, acc 0.8125, learning_rate 0.000300158\n","2023-06-13T17:31:50.321316: step 6124, loss 0.461659, acc 0.84375, learning_rate 0.00030006\n","2023-06-13T17:31:51.781655: step 6125, loss 0.232579, acc 0.96875, learning_rate 0.000299963\n","2023-06-13T17:31:53.261047: step 6126, loss 0.695205, acc 0.78125, learning_rate 0.000299867\n","2023-06-13T17:31:55.838354: step 6127, loss 0.692954, acc 0.8125, learning_rate 0.00029977\n","2023-06-13T17:31:58.268648: step 6128, loss 0.381002, acc 0.9375, learning_rate 0.000299673\n","2023-06-13T17:32:00.662676: step 6129, loss 0.772084, acc 0.75, learning_rate 0.000299576\n","2023-06-13T17:32:02.907510: step 6130, loss 0.609296, acc 0.71875, learning_rate 0.000299479\n","2023-06-13T17:32:04.355087: step 6131, loss 0.482815, acc 0.8125, learning_rate 0.000299382\n","2023-06-13T17:32:05.869827: step 6132, loss 0.319851, acc 0.90625, learning_rate 0.000299286\n","2023-06-13T17:32:07.328572: step 6133, loss 0.557207, acc 0.75, learning_rate 0.000299189\n","2023-06-13T17:32:08.787470: step 6134, loss 0.372257, acc 0.875, learning_rate 0.000299093\n","2023-06-13T17:32:10.232042: step 6135, loss 0.637993, acc 0.78125, learning_rate 0.000298996\n","2023-06-13T17:32:11.685832: step 6136, loss 0.493937, acc 0.84375, learning_rate 0.0002989\n","2023-06-13T17:32:13.447448: step 6137, loss 0.609964, acc 0.8125, learning_rate 0.000298803\n","2023-06-13T17:32:16.055282: step 6138, loss 0.318771, acc 0.90625, learning_rate 0.000298707\n","2023-06-13T17:32:18.472748: step 6139, loss 0.860788, acc 0.71875, learning_rate 0.00029861\n","2023-06-13T17:32:20.929308: step 6140, loss 0.419833, acc 0.90625, learning_rate 0.000298514\n","2023-06-13T17:32:22.796511: step 6141, loss 0.342387, acc 0.875, learning_rate 0.000298418\n","2023-06-13T17:32:24.295427: step 6142, loss 0.337025, acc 0.875, learning_rate 0.000298322\n","2023-06-13T17:32:25.778814: step 6143, loss 0.625167, acc 0.84375, learning_rate 0.000298225\n","2023-06-13T17:32:27.257906: step 6144, loss 0.273443, acc 0.90625, learning_rate 0.000298129\n","2023-06-13T17:32:28.730250: step 6145, loss 0.64723, acc 0.75, learning_rate 0.000298033\n","2023-06-13T17:32:30.197426: step 6146, loss 0.619173, acc 0.78125, learning_rate 0.000297937\n","2023-06-13T17:32:31.661080: step 6147, loss 0.59798, acc 0.84375, learning_rate 0.000297841\n","2023-06-13T17:32:33.866697: step 6148, loss 0.681798, acc 0.75, learning_rate 0.000297745\n","2023-06-13T17:32:36.418989: step 6149, loss 0.548228, acc 0.90625, learning_rate 0.000297649\n","2023-06-13T17:32:38.813743: step 6150, loss 0.612124, acc 0.8125, learning_rate 0.000297554\n","2023-06-13T17:32:41.335354: step 6151, loss 0.556019, acc 0.84375, learning_rate 0.000297458\n","2023-06-13T17:32:42.810158: step 6152, loss 0.71191, acc 0.75, learning_rate 0.000297362\n","2023-06-13T17:32:44.257428: step 6153, loss 0.368598, acc 0.84375, learning_rate 0.000297266\n","2023-06-13T17:32:45.699700: step 6154, loss 0.736058, acc 0.71875, learning_rate 0.000297171\n","2023-06-13T17:32:47.184514: step 6155, loss 0.627056, acc 0.8125, learning_rate 0.000297075\n","2023-06-13T17:32:48.641900: step 6156, loss 0.384171, acc 0.84375, learning_rate 0.00029698\n","2023-06-13T17:32:50.109877: step 6157, loss 0.403124, acc 0.875, learning_rate 0.000296884\n","2023-06-13T17:32:51.695657: step 6158, loss 0.638091, acc 0.75, learning_rate 0.000296789\n","2023-06-13T17:32:54.299657: step 6159, loss 0.615445, acc 0.875, learning_rate 0.000296693\n","2023-06-13T17:32:56.728277: step 6160, loss 0.588298, acc 0.78125, learning_rate 0.000296598\n","2023-06-13T17:32:59.216415: step 6161, loss 0.523318, acc 0.8125, learning_rate 0.000296502\n","2023-06-13T17:33:01.169601: step 6162, loss 0.458538, acc 0.8125, learning_rate 0.000296407\n","2023-06-13T17:33:02.614643: step 6163, loss 0.598429, acc 0.75, learning_rate 0.000296312\n","2023-06-13T17:33:04.041821: step 6164, loss 0.66744, acc 0.71875, learning_rate 0.000296217\n","2023-06-13T17:33:05.491565: step 6165, loss 0.823287, acc 0.6875, learning_rate 0.000296122\n","2023-06-13T17:33:06.938864: step 6166, loss 0.603761, acc 0.8125, learning_rate 0.000296027\n","2023-06-13T17:33:08.409211: step 6167, loss 0.721122, acc 0.84375, learning_rate 0.000295931\n","2023-06-13T17:33:09.859679: step 6168, loss 0.76932, acc 0.75, learning_rate 0.000295836\n","2023-06-13T17:33:12.019578: step 6169, loss 0.408464, acc 0.90625, learning_rate 0.000295741\n","2023-06-13T17:33:14.578546: step 6170, loss 0.544409, acc 0.78125, learning_rate 0.000295647\n","2023-06-13T17:33:17.095695: step 6171, loss 0.522984, acc 0.8125, learning_rate 0.000295552\n","2023-06-13T17:33:19.443394: step 6172, loss 0.789701, acc 0.78125, learning_rate 0.000295457\n","2023-06-13T17:33:20.925530: step 6173, loss 0.393256, acc 0.90625, learning_rate 0.000295362\n","2023-06-13T17:33:22.398669: step 6174, loss 0.700709, acc 0.6875, learning_rate 0.000295267\n","2023-06-13T17:33:23.842831: step 6175, loss 0.20429, acc 1, learning_rate 0.000295173\n","2023-06-13T17:33:25.301494: step 6176, loss 0.42649, acc 0.875, learning_rate 0.000295078\n","2023-06-13T17:33:26.777464: step 6177, loss 0.700942, acc 0.71875, learning_rate 0.000294983\n","2023-06-13T17:33:28.280163: step 6178, loss 0.334199, acc 0.8125, learning_rate 0.000294889\n","2023-06-13T17:33:30.128798: step 6179, loss 0.637068, acc 0.78125, learning_rate 0.000294794\n","2023-06-13T17:33:32.682372: step 6180, loss 0.145153, acc 1, learning_rate 0.0002947\n","2023-06-13T17:33:35.019905: step 6181, loss 0.166741, acc 0.96875, learning_rate 0.000294606\n","2023-06-13T17:33:37.456360: step 6182, loss 0.906826, acc 0.6875, learning_rate 0.000294511\n","2023-06-13T17:33:39.457997: step 6183, loss 0.664275, acc 0.78125, learning_rate 0.000294417\n","2023-06-13T17:33:40.914852: step 6184, loss 0.558027, acc 0.8125, learning_rate 0.000294323\n","2023-06-13T17:33:42.392872: step 6185, loss 0.657579, acc 0.75, learning_rate 0.000294228\n","2023-06-13T17:33:43.868566: step 6186, loss 0.522847, acc 0.90625, learning_rate 0.000294134\n","2023-06-13T17:33:45.354010: step 6187, loss 0.776997, acc 0.78125, learning_rate 0.00029404\n","2023-06-13T17:33:46.822051: step 6188, loss 0.501785, acc 0.84375, learning_rate 0.000293946\n","2023-06-13T17:33:48.270921: step 6189, loss 0.476249, acc 0.84375, learning_rate 0.000293852\n","2023-06-13T17:33:50.232823: step 6190, loss 0.444613, acc 0.8125, learning_rate 0.000293758\n","2023-06-13T17:33:52.690779: step 6191, loss 0.421372, acc 0.84375, learning_rate 0.000293664\n","2023-06-13T17:33:55.133376: step 6192, loss 0.565338, acc 0.78125, learning_rate 0.00029357\n","2023-06-13T17:33:57.560363: step 6193, loss 0.566989, acc 0.84375, learning_rate 0.000293476\n","2023-06-13T17:33:59.382267: step 6194, loss 0.5419, acc 0.84375, learning_rate 0.000293382\n","2023-06-13T17:34:00.856898: step 6195, loss 0.691012, acc 0.75, learning_rate 0.000293289\n","2023-06-13T17:34:02.343489: step 6196, loss 0.248808, acc 0.9375, learning_rate 0.000293195\n","2023-06-13T17:34:03.786313: step 6197, loss 0.626738, acc 0.8125, learning_rate 0.000293101\n","2023-06-13T17:34:05.266308: step 6198, loss 0.695349, acc 0.78125, learning_rate 0.000293008\n","2023-06-13T17:34:06.719431: step 6199, loss 0.727801, acc 0.875, learning_rate 0.000292914\n","\n","Evaluation:\n","2023-06-13T17:34:38.820860: step 6200, loss 2.80398, acc 0.386632\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6200\n","\n","2023-06-13T17:34:40.434712: step 6200, loss 0.180923, acc 0.96875, learning_rate 0.000292821\n","2023-06-13T17:34:41.904488: step 6201, loss 0.303548, acc 0.9375, learning_rate 0.000292727\n","2023-06-13T17:34:43.362176: step 6202, loss 0.483513, acc 0.9375, learning_rate 0.000292634\n","2023-06-13T17:34:44.827431: step 6203, loss 0.776161, acc 0.78125, learning_rate 0.00029254\n","2023-06-13T17:34:46.280836: step 6204, loss 0.435803, acc 0.84375, learning_rate 0.000292447\n","2023-06-13T17:34:47.788467: step 6205, loss 0.531944, acc 0.84375, learning_rate 0.000292354\n","2023-06-13T17:34:49.583702: step 6206, loss 0.706239, acc 0.8125, learning_rate 0.00029226\n","2023-06-13T17:34:52.111668: step 6207, loss 0.408837, acc 0.84375, learning_rate 0.000292167\n","2023-06-13T17:34:54.522861: step 6208, loss 0.594716, acc 0.875, learning_rate 0.000292074\n","2023-06-13T17:34:56.938124: step 6209, loss 0.498409, acc 0.78125, learning_rate 0.000291981\n","2023-06-13T17:34:58.863066: step 6210, loss 0.646421, acc 0.78125, learning_rate 0.000291888\n","2023-06-13T17:35:00.295691: step 6211, loss 0.476281, acc 0.84375, learning_rate 0.000291795\n","2023-06-13T17:35:01.769212: step 6212, loss 0.388405, acc 0.8125, learning_rate 0.000291702\n","2023-06-13T17:35:03.254083: step 6213, loss 0.566041, acc 0.78125, learning_rate 0.000291609\n","2023-06-13T17:35:04.707569: step 6214, loss 0.498229, acc 0.84375, learning_rate 0.000291516\n","2023-06-13T17:35:06.247629: step 6215, loss 0.525962, acc 0.8125, learning_rate 0.000291423\n","2023-06-13T17:35:07.725420: step 6216, loss 0.516382, acc 0.8125, learning_rate 0.00029133\n","2023-06-13T17:35:09.951700: step 6217, loss 0.461552, acc 0.78125, learning_rate 0.000291237\n","2023-06-13T17:35:12.457463: step 6218, loss 0.320356, acc 0.9375, learning_rate 0.000291145\n","2023-06-13T17:35:14.709339: step 6219, loss 0.525382, acc 0.78125, learning_rate 0.000291052\n","2023-06-13T17:35:17.047696: step 6220, loss 0.458804, acc 0.8125, learning_rate 0.000290959\n","2023-06-13T17:35:18.776438: step 6221, loss 0.610274, acc 0.8125, learning_rate 0.000290867\n","2023-06-13T17:35:20.267137: step 6222, loss 0.387348, acc 0.90625, learning_rate 0.000290774\n","2023-06-13T17:35:21.731711: step 6223, loss 0.53459, acc 0.84375, learning_rate 0.000290682\n","2023-06-13T17:35:23.208904: step 6224, loss 0.366348, acc 0.875, learning_rate 0.000290589\n","2023-06-13T17:35:24.726028: step 6225, loss 0.587209, acc 0.84375, learning_rate 0.000290497\n","2023-06-13T17:35:27.238692: step 6226, loss 0.60107, acc 0.8125, learning_rate 0.000290404\n","2023-06-13T17:35:29.998162: step 6227, loss 0.792441, acc 0.75, learning_rate 0.000290312\n","2023-06-13T17:35:32.696900: step 6228, loss 0.707323, acc 0.71875, learning_rate 0.00029022\n","2023-06-13T17:35:35.369983: step 6229, loss 0.655764, acc 0.78125, learning_rate 0.000290128\n","2023-06-13T17:35:37.991842: step 6230, loss 0.435465, acc 0.875, learning_rate 0.000290035\n","2023-06-13T17:35:40.438293: step 6231, loss 0.444161, acc 0.8125, learning_rate 0.000289943\n","2023-06-13T17:35:42.377963: step 6232, loss 0.546041, acc 0.875, learning_rate 0.000289851\n","2023-06-13T17:35:43.859849: step 6233, loss 0.511199, acc 0.75, learning_rate 0.000289759\n","2023-06-13T17:35:45.283183: step 6234, loss 0.57052, acc 0.75, learning_rate 0.000289667\n","2023-06-13T17:35:46.739310: step 6235, loss 0.588264, acc 0.78125, learning_rate 0.000289575\n","2023-06-13T17:35:48.170187: step 6236, loss 0.268606, acc 0.90625, learning_rate 0.000289483\n","2023-06-13T17:35:49.618327: step 6237, loss 0.567223, acc 0.875, learning_rate 0.000289391\n","2023-06-13T17:35:51.082364: step 6238, loss 0.616942, acc 0.875, learning_rate 0.000289299\n","2023-06-13T17:35:53.101625: step 6239, loss 0.82434, acc 0.78125, learning_rate 0.000289208\n","2023-06-13T17:35:55.652287: step 6240, loss 0.556366, acc 0.84375, learning_rate 0.000289116\n","2023-06-13T17:35:58.103459: step 6241, loss 0.868831, acc 0.78125, learning_rate 0.000289024\n","2023-06-13T17:36:00.510568: step 6242, loss 0.778462, acc 0.75, learning_rate 0.000288933\n","2023-06-13T17:36:02.102857: step 6243, loss 0.562343, acc 0.78125, learning_rate 0.000288841\n","2023-06-13T17:36:03.577183: step 6244, loss 0.398381, acc 0.8125, learning_rate 0.000288749\n","2023-06-13T17:36:05.033106: step 6245, loss 0.673392, acc 0.75, learning_rate 0.000288658\n","2023-06-13T17:36:06.522798: step 6246, loss 0.638494, acc 0.71875, learning_rate 0.000288566\n","2023-06-13T17:36:08.002102: step 6247, loss 0.46645, acc 0.8125, learning_rate 0.000288475\n","2023-06-13T17:36:09.456053: step 6248, loss 0.953333, acc 0.65625, learning_rate 0.000288384\n","2023-06-13T17:36:10.895340: step 6249, loss 0.891244, acc 0.8125, learning_rate 0.000288292\n","2023-06-13T17:36:13.288284: step 6250, loss 0.765728, acc 0.6875, learning_rate 0.000288201\n","2023-06-13T17:36:15.577178: step 6251, loss 0.68788, acc 0.75, learning_rate 0.00028811\n","2023-06-13T17:36:18.071362: step 6252, loss 0.62904, acc 0.6875, learning_rate 0.000288018\n","2023-06-13T17:36:20.465629: step 6253, loss 0.615871, acc 0.71875, learning_rate 0.000287927\n","2023-06-13T17:36:22.030283: step 6254, loss 0.541534, acc 0.8125, learning_rate 0.000287836\n","2023-06-13T17:36:23.473073: step 6255, loss 0.639508, acc 0.78125, learning_rate 0.000287745\n","2023-06-13T17:36:24.953084: step 6256, loss 0.400083, acc 0.875, learning_rate 0.000287654\n","2023-06-13T17:36:26.409992: step 6257, loss 0.745155, acc 0.875, learning_rate 0.000287563\n","2023-06-13T17:36:27.894282: step 6258, loss 1.16869, acc 0.6875, learning_rate 0.000287472\n","2023-06-13T17:36:29.357701: step 6259, loss 0.666689, acc 0.75, learning_rate 0.000287381\n","2023-06-13T17:36:30.852239: step 6260, loss 0.808497, acc 0.8125, learning_rate 0.00028729\n","2023-06-13T17:36:33.423430: step 6261, loss 0.453228, acc 0.875, learning_rate 0.0002872\n","2023-06-13T17:36:35.878351: step 6262, loss 0.654697, acc 0.875, learning_rate 0.000287109\n","2023-06-13T17:36:38.316531: step 6263, loss 0.341262, acc 0.90625, learning_rate 0.000287018\n","2023-06-13T17:36:40.583675: step 6264, loss 0.157827, acc 0.96875, learning_rate 0.000286927\n","2023-06-13T17:36:42.055137: step 6265, loss 0.680978, acc 0.78125, learning_rate 0.000286837\n","2023-06-13T17:36:43.522722: step 6266, loss 0.407027, acc 0.90625, learning_rate 0.000286746\n","2023-06-13T17:36:44.968847: step 6267, loss 0.682748, acc 0.78125, learning_rate 0.000286656\n","2023-06-13T17:36:46.443112: step 6268, loss 0.373744, acc 0.875, learning_rate 0.000286565\n","2023-06-13T17:36:47.933508: step 6269, loss 0.577318, acc 0.84375, learning_rate 0.000286475\n","2023-06-13T17:36:49.400580: step 6270, loss 0.856723, acc 0.78125, learning_rate 0.000286384\n","2023-06-13T17:36:51.110163: step 6271, loss 0.86801, acc 0.78125, learning_rate 0.000286294\n","2023-06-13T17:36:53.676038: step 6272, loss 0.43684, acc 0.84375, learning_rate 0.000286204\n","2023-06-13T17:36:56.068715: step 6273, loss 0.668861, acc 0.78125, learning_rate 0.000286113\n","2023-06-13T17:36:58.498367: step 6274, loss 0.329421, acc 0.9375, learning_rate 0.000286023\n","2023-06-13T17:37:00.829644: step 6275, loss 0.532896, acc 0.84375, learning_rate 0.000285933\n","2023-06-13T17:37:03.358667: step 6276, loss 0.44447, acc 0.84375, learning_rate 0.000285843\n","2023-06-13T17:37:05.687818: step 6277, loss 0.735327, acc 0.75, learning_rate 0.000285753\n","2023-06-13T17:37:08.063972: step 6278, loss 0.534104, acc 0.84375, learning_rate 0.000285662\n","2023-06-13T17:37:10.308411: step 6279, loss 0.35644, acc 0.875, learning_rate 0.000285572\n","2023-06-13T17:37:12.888833: step 6280, loss 0.379775, acc 0.875, learning_rate 0.000285482\n","2023-06-13T17:37:15.365364: step 6281, loss 0.65202, acc 0.65625, learning_rate 0.000285393\n","2023-06-13T17:37:17.688472: step 6282, loss 0.489831, acc 0.90625, learning_rate 0.000285303\n","2023-06-13T17:37:19.740720: step 6283, loss 0.459294, acc 0.8125, learning_rate 0.000285213\n","2023-06-13T17:37:21.213315: step 6284, loss 0.32236, acc 0.90625, learning_rate 0.000285123\n","2023-06-13T17:37:22.662821: step 6285, loss 0.539977, acc 0.75, learning_rate 0.000285033\n","2023-06-13T17:37:24.128919: step 6286, loss 1.19664, acc 0.625, learning_rate 0.000284944\n","2023-06-13T17:37:25.574992: step 6287, loss 0.308186, acc 0.90625, learning_rate 0.000284854\n","2023-06-13T17:37:27.083561: step 6288, loss 0.460192, acc 0.84375, learning_rate 0.000284764\n","2023-06-13T17:37:28.581991: step 6289, loss 0.453342, acc 0.8125, learning_rate 0.000284675\n","2023-06-13T17:37:30.605953: step 6290, loss 0.525503, acc 0.8125, learning_rate 0.000284585\n","2023-06-13T17:37:33.078133: step 6291, loss 0.269718, acc 0.90625, learning_rate 0.000284496\n","2023-06-13T17:37:35.517756: step 6292, loss 0.718111, acc 0.8125, learning_rate 0.000284406\n","2023-06-13T17:37:37.877187: step 6293, loss 0.518218, acc 0.875, learning_rate 0.000284317\n","2023-06-13T17:37:39.852335: step 6294, loss 0.660681, acc 0.75, learning_rate 0.000284227\n","2023-06-13T17:37:41.294527: step 6295, loss 0.609424, acc 0.78125, learning_rate 0.000284138\n","2023-06-13T17:37:42.765757: step 6296, loss 0.831088, acc 0.8125, learning_rate 0.000284049\n","2023-06-13T17:37:44.227227: step 6297, loss 0.702448, acc 0.71875, learning_rate 0.000283959\n","2023-06-13T17:37:45.669938: step 6298, loss 0.672552, acc 0.75, learning_rate 0.00028387\n","2023-06-13T17:37:47.171618: step 6299, loss 0.643532, acc 0.71875, learning_rate 0.000283781\n","\n","Evaluation:\n","2023-06-13T17:38:19.113493: step 6300, loss 2.82267, acc 0.384056\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6300\n","\n","2023-06-13T17:38:20.863909: step 6300, loss 0.33016, acc 0.875, learning_rate 0.000283692\n","2023-06-13T17:38:22.327301: step 6301, loss 0.440247, acc 0.90625, learning_rate 0.000283603\n","2023-06-13T17:38:23.757809: step 6302, loss 0.926839, acc 0.71875, learning_rate 0.000283514\n","2023-06-13T17:38:25.202006: step 6303, loss 0.367612, acc 0.90625, learning_rate 0.000283425\n","2023-06-13T17:38:26.700206: step 6304, loss 0.425798, acc 0.78125, learning_rate 0.000283336\n","2023-06-13T17:38:28.308086: step 6305, loss 0.750472, acc 0.75, learning_rate 0.000283247\n","2023-06-13T17:38:30.150545: step 6306, loss 0.303681, acc 0.90625, learning_rate 0.000283158\n","2023-06-13T17:38:32.632467: step 6307, loss 0.630611, acc 0.71875, learning_rate 0.000283069\n","2023-06-13T17:38:34.940458: step 6308, loss 0.520524, acc 0.8125, learning_rate 0.000282981\n","2023-06-13T17:38:37.340709: step 6309, loss 0.528432, acc 0.8125, learning_rate 0.000282892\n","2023-06-13T17:38:39.513479: step 6310, loss 0.80233, acc 0.6875, learning_rate 0.000282803\n","2023-06-13T17:38:41.109974: step 6311, loss 0.683336, acc 0.78125, learning_rate 0.000282715\n","2023-06-13T17:38:42.578904: step 6312, loss 0.737917, acc 0.8125, learning_rate 0.000282626\n","2023-06-13T17:38:44.022716: step 6313, loss 0.507559, acc 0.84375, learning_rate 0.000282537\n","2023-06-13T17:38:45.476258: step 6314, loss 0.389643, acc 0.8125, learning_rate 0.000282449\n","2023-06-13T17:38:46.920095: step 6315, loss 0.854262, acc 0.6875, learning_rate 0.000282361\n","2023-06-13T17:38:48.407881: step 6316, loss 0.383519, acc 0.875, learning_rate 0.000282272\n","2023-06-13T17:38:49.941508: step 6317, loss 0.716659, acc 0.71875, learning_rate 0.000282184\n","2023-06-13T17:38:52.508055: step 6318, loss 0.431777, acc 0.84375, learning_rate 0.000282095\n","2023-06-13T17:38:54.971111: step 6319, loss 0.618064, acc 0.84375, learning_rate 0.000282007\n","2023-06-13T17:38:57.227398: step 6320, loss 0.605519, acc 0.84375, learning_rate 0.000281919\n","2023-06-13T17:38:59.513585: step 6321, loss 0.34071, acc 0.90625, learning_rate 0.000281831\n","2023-06-13T17:39:01.039719: step 6322, loss 0.455755, acc 0.90625, learning_rate 0.000281742\n","2023-06-13T17:39:02.525943: step 6323, loss 0.439424, acc 0.875, learning_rate 0.000281654\n","2023-06-13T17:39:03.974852: step 6324, loss 0.572898, acc 0.8125, learning_rate 0.000281566\n","2023-06-13T17:39:05.429746: step 6325, loss 0.6917, acc 0.6875, learning_rate 0.000281478\n","2023-06-13T17:39:06.908582: step 6326, loss 0.730946, acc 0.71875, learning_rate 0.00028139\n","2023-06-13T17:39:08.469567: step 6327, loss 0.586365, acc 0.84375, learning_rate 0.000281302\n","2023-06-13T17:39:10.141431: step 6328, loss 0.394013, acc 0.90625, learning_rate 0.000281214\n","2023-06-13T17:39:12.682390: step 6329, loss 0.379155, acc 0.84375, learning_rate 0.000281126\n","2023-06-13T17:39:15.032304: step 6330, loss 0.292992, acc 0.9375, learning_rate 0.000281039\n","2023-06-13T17:39:17.358783: step 6331, loss 0.839654, acc 0.625, learning_rate 0.000280951\n","2023-06-13T17:39:19.614781: step 6332, loss 0.599681, acc 0.75, learning_rate 0.000280863\n","2023-06-13T17:39:21.062706: step 6333, loss 0.39899, acc 0.84375, learning_rate 0.000280775\n","2023-06-13T17:39:22.550250: step 6334, loss 0.507181, acc 0.84375, learning_rate 0.000280688\n","2023-06-13T17:39:23.986155: step 6335, loss 0.662383, acc 0.6875, learning_rate 0.0002806\n","2023-06-13T17:39:25.447666: step 6336, loss 0.569928, acc 0.875, learning_rate 0.000280513\n","2023-06-13T17:39:26.923064: step 6337, loss 0.385874, acc 0.875, learning_rate 0.000280425\n","2023-06-13T17:39:28.407914: step 6338, loss 0.863987, acc 0.78125, learning_rate 0.000280338\n","2023-06-13T17:39:30.202034: step 6339, loss 0.667157, acc 0.78125, learning_rate 0.00028025\n","2023-06-13T17:39:32.754102: step 6340, loss 0.599342, acc 0.78125, learning_rate 0.000280163\n","2023-06-13T17:39:35.123344: step 6341, loss 0.514163, acc 0.84375, learning_rate 0.000280075\n","2023-06-13T17:39:37.515719: step 6342, loss 0.386694, acc 0.84375, learning_rate 0.000279988\n","2023-06-13T17:39:39.565317: step 6343, loss 1.23164, acc 0.65625, learning_rate 0.000279901\n","2023-06-13T17:39:41.005339: step 6344, loss 0.882271, acc 0.75, learning_rate 0.000279814\n","2023-06-13T17:39:42.477772: step 6345, loss 0.408311, acc 0.84375, learning_rate 0.000279726\n","2023-06-13T17:39:43.950659: step 6346, loss 0.653576, acc 0.875, learning_rate 0.000279639\n","2023-06-13T17:39:45.396670: step 6347, loss 0.555566, acc 0.84375, learning_rate 0.000279552\n","2023-06-13T17:39:46.859040: step 6348, loss 0.532916, acc 0.84375, learning_rate 0.000279465\n","2023-06-13T17:39:48.313248: step 6349, loss 0.509871, acc 0.84375, learning_rate 0.000279378\n","2023-06-13T17:39:50.142858: step 6350, loss 0.517136, acc 0.875, learning_rate 0.000279291\n","2023-06-13T17:39:52.718930: step 6351, loss 0.414434, acc 0.875, learning_rate 0.000279204\n","2023-06-13T17:39:55.103504: step 6352, loss 0.492944, acc 0.84375, learning_rate 0.000279117\n","2023-06-13T17:39:57.427306: step 6353, loss 0.842773, acc 0.71875, learning_rate 0.00027903\n","2023-06-13T17:39:59.273667: step 6354, loss 0.577839, acc 0.75, learning_rate 0.000278944\n","2023-06-13T17:40:00.699555: step 6355, loss 0.624652, acc 0.78125, learning_rate 0.000278857\n","2023-06-13T17:40:02.199657: step 6356, loss 0.488264, acc 0.84375, learning_rate 0.00027877\n","2023-06-13T17:40:04.576625: step 6357, loss 0.880315, acc 0.6875, learning_rate 0.000278683\n","2023-06-13T17:40:07.073913: step 6358, loss 0.514576, acc 0.84375, learning_rate 0.000278597\n","2023-06-13T17:40:09.559762: step 6359, loss 0.334226, acc 0.9375, learning_rate 0.00027851\n","2023-06-13T17:40:12.302956: step 6360, loss 0.292154, acc 0.875, learning_rate 0.000278424\n","2023-06-13T17:40:14.923488: step 6361, loss 0.331388, acc 0.90625, learning_rate 0.000278337\n","2023-06-13T17:40:17.354859: step 6362, loss 0.992691, acc 0.75, learning_rate 0.000278251\n","2023-06-13T17:40:19.521201: step 6363, loss 0.667135, acc 0.8125, learning_rate 0.000278164\n","2023-06-13T17:40:21.518259: step 6364, loss 0.326149, acc 0.875, learning_rate 0.000278078\n","2023-06-13T17:40:23.005837: step 6365, loss 0.805397, acc 0.78125, learning_rate 0.000277991\n","2023-06-13T17:40:24.444825: step 6366, loss 1.01228, acc 0.6875, learning_rate 0.000277905\n","2023-06-13T17:40:25.923851: step 6367, loss 0.437942, acc 0.84375, learning_rate 0.000277819\n","2023-06-13T17:40:27.377983: step 6368, loss 0.678778, acc 0.78125, learning_rate 0.000277733\n","2023-06-13T17:40:28.862503: step 6369, loss 0.517165, acc 0.8125, learning_rate 0.000277647\n","2023-06-13T17:40:30.320078: step 6370, loss 0.751022, acc 0.65625, learning_rate 0.00027756\n","2023-06-13T17:40:32.303119: step 6371, loss 0.513955, acc 0.8125, learning_rate 0.000277474\n","2023-06-13T17:40:34.823770: step 6372, loss 0.486125, acc 0.78125, learning_rate 0.000277388\n","2023-06-13T17:40:37.320608: step 6373, loss 0.649042, acc 0.75, learning_rate 0.000277302\n","2023-06-13T17:40:39.741390: step 6374, loss 0.421568, acc 0.90625, learning_rate 0.000277216\n","2023-06-13T17:40:41.460559: step 6375, loss 0.914262, acc 0.625, learning_rate 0.00027713\n","2023-06-13T17:40:42.914983: step 6376, loss 0.458341, acc 0.90625, learning_rate 0.000277044\n","2023-06-13T17:40:44.375117: step 6377, loss 0.268049, acc 0.90625, learning_rate 0.000276959\n","2023-06-13T17:40:45.849106: step 6378, loss 0.309147, acc 0.9375, learning_rate 0.000276873\n","2023-06-13T17:40:47.280833: step 6379, loss 0.234244, acc 0.9375, learning_rate 0.000276787\n","2023-06-13T17:40:48.730582: step 6380, loss 0.333621, acc 0.90625, learning_rate 0.000276701\n","2023-06-13T17:40:50.207597: step 6381, loss 0.637795, acc 0.78125, learning_rate 0.000276616\n","2023-06-13T17:40:52.444856: step 6382, loss 0.669444, acc 0.75, learning_rate 0.00027653\n","2023-06-13T17:40:54.869801: step 6383, loss 1.23354, acc 0.78125, learning_rate 0.000276444\n","2023-06-13T17:40:57.383969: step 6384, loss 0.545265, acc 0.78125, learning_rate 0.000276359\n","2023-06-13T17:40:59.796801: step 6385, loss 0.4783, acc 0.78125, learning_rate 0.000276273\n","2023-06-13T17:41:01.304807: step 6386, loss 0.473423, acc 0.84375, learning_rate 0.000276188\n","2023-06-13T17:41:02.773244: step 6387, loss 0.580011, acc 0.71875, learning_rate 0.000276102\n","2023-06-13T17:41:04.223892: step 6388, loss 0.826934, acc 0.65625, learning_rate 0.000276017\n","2023-06-13T17:41:05.682712: step 6389, loss 0.713008, acc 0.6875, learning_rate 0.000275932\n","2023-06-13T17:41:07.128781: step 6390, loss 0.731241, acc 0.78125, learning_rate 0.000275846\n","2023-06-13T17:41:08.631233: step 6391, loss 0.492008, acc 0.78125, learning_rate 0.000275761\n","2023-06-13T17:41:10.252650: step 6392, loss 0.54574, acc 0.875, learning_rate 0.000275676\n","2023-06-13T17:41:12.845115: step 6393, loss 0.389574, acc 0.875, learning_rate 0.000275591\n","2023-06-13T17:41:15.298623: step 6394, loss 0.479053, acc 0.8125, learning_rate 0.000275506\n","2023-06-13T17:41:17.670033: step 6395, loss 0.782453, acc 0.6875, learning_rate 0.00027542\n","2023-06-13T17:41:19.790012: step 6396, loss 0.502754, acc 0.8125, learning_rate 0.000275335\n","2023-06-13T17:41:21.242123: step 6397, loss 0.508594, acc 0.8125, learning_rate 0.00027525\n","2023-06-13T17:41:22.722563: step 6398, loss 1.02547, acc 0.625, learning_rate 0.000275165\n","2023-06-13T17:41:24.186284: step 6399, loss 1.09579, acc 0.75, learning_rate 0.000275081\n","\n","Evaluation:\n","2023-06-13T17:41:59.135977: step 6400, loss 2.83095, acc 0.38451\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6400\n","\n","2023-06-13T17:42:01.842396: step 6400, loss 0.270512, acc 0.96875, learning_rate 0.000274996\n","2023-06-13T17:42:03.430510: step 6401, loss 0.701241, acc 0.78125, learning_rate 0.000274911\n","2023-06-13T17:42:04.894529: step 6402, loss 0.520448, acc 0.84375, learning_rate 0.000274826\n","2023-06-13T17:42:06.337095: step 6403, loss 0.366717, acc 0.90625, learning_rate 0.000274741\n","2023-06-13T17:42:07.871424: step 6404, loss 0.349468, acc 0.9375, learning_rate 0.000274656\n","2023-06-13T17:42:09.380552: step 6405, loss 0.774944, acc 0.6875, learning_rate 0.000274572\n","2023-06-13T17:42:10.895170: step 6406, loss 0.50222, acc 0.78125, learning_rate 0.000274487\n","2023-06-13T17:42:12.536619: step 6407, loss 0.895599, acc 0.71875, learning_rate 0.000274403\n","2023-06-13T17:42:15.130325: step 6408, loss 0.690632, acc 0.75, learning_rate 0.000274318\n","2023-06-13T17:42:17.586033: step 6409, loss 0.748718, acc 0.78125, learning_rate 0.000274233\n","2023-06-13T17:42:19.998311: step 6410, loss 0.533519, acc 0.8125, learning_rate 0.000274149\n","2023-06-13T17:42:22.131013: step 6411, loss 0.502715, acc 0.8125, learning_rate 0.000274065\n","2023-06-13T17:42:23.585830: step 6412, loss 0.377517, acc 0.8125, learning_rate 0.00027398\n","2023-06-13T17:42:25.060904: step 6413, loss 0.572538, acc 0.8125, learning_rate 0.000273896\n","2023-06-13T17:42:26.554345: step 6414, loss 0.955033, acc 0.625, learning_rate 0.000273811\n","2023-06-13T17:42:28.019889: step 6415, loss 0.678589, acc 0.71875, learning_rate 0.000273727\n","2023-06-13T17:42:29.503131: step 6416, loss 0.666852, acc 0.78125, learning_rate 0.000273643\n","2023-06-13T17:42:30.982276: step 6417, loss 0.528488, acc 0.71875, learning_rate 0.000273559\n","2023-06-13T17:42:33.078657: step 6418, loss 0.551966, acc 0.8125, learning_rate 0.000273475\n","2023-06-13T17:42:35.555777: step 6419, loss 0.571582, acc 0.75, learning_rate 0.00027339\n","2023-06-13T17:42:38.134053: step 6420, loss 0.362733, acc 0.84375, learning_rate 0.000273306\n","2023-06-13T17:42:40.622567: step 6421, loss 0.730147, acc 0.6875, learning_rate 0.000273222\n","2023-06-13T17:42:42.246602: step 6422, loss 0.483774, acc 0.78125, learning_rate 0.000273138\n","2023-06-13T17:42:43.721838: step 6423, loss 0.516127, acc 0.84375, learning_rate 0.000273054\n","2023-06-13T17:42:45.160984: step 6424, loss 0.499836, acc 0.90625, learning_rate 0.00027297\n","2023-06-13T17:42:46.609235: step 6425, loss 0.840667, acc 0.75, learning_rate 0.000272887\n","2023-06-13T17:42:48.081682: step 6426, loss 0.566728, acc 0.8125, learning_rate 0.000272803\n","2023-06-13T17:42:49.548068: step 6427, loss 0.454135, acc 0.8125, learning_rate 0.000272719\n","2023-06-13T17:42:51.056721: step 6428, loss 0.544636, acc 0.78125, learning_rate 0.000272635\n","2023-06-13T17:42:53.636367: step 6429, loss 0.39625, acc 0.84375, learning_rate 0.000272552\n","2023-06-13T17:42:55.972192: step 6430, loss 0.668284, acc 0.78125, learning_rate 0.000272468\n","2023-06-13T17:42:58.484409: step 6431, loss 0.641453, acc 0.75, learning_rate 0.000272384\n","2023-06-13T17:43:00.748608: step 6432, loss 0.432811, acc 0.875, learning_rate 0.000272301\n","2023-06-13T17:43:02.230128: step 6433, loss 0.307751, acc 0.90625, learning_rate 0.000272217\n","2023-06-13T17:43:03.708426: step 6434, loss 0.72571, acc 0.71875, learning_rate 0.000272134\n","2023-06-13T17:43:05.175230: step 6435, loss 0.312331, acc 0.90625, learning_rate 0.00027205\n","2023-06-13T17:43:06.653946: step 6436, loss 0.817783, acc 0.75, learning_rate 0.000271967\n","2023-06-13T17:43:08.197492: step 6437, loss 0.716373, acc 0.75, learning_rate 0.000271883\n","2023-06-13T17:43:09.680356: step 6438, loss 0.497852, acc 0.875, learning_rate 0.0002718\n","2023-06-13T17:43:11.564358: step 6439, loss 0.572731, acc 0.6875, learning_rate 0.000271717\n","2023-06-13T17:43:14.205070: step 6440, loss 0.992663, acc 0.75, learning_rate 0.000271633\n","2023-06-13T17:43:16.781679: step 6441, loss 0.484013, acc 0.90625, learning_rate 0.00027155\n","2023-06-13T17:43:19.265467: step 6442, loss 0.594378, acc 0.84375, learning_rate 0.000271467\n","2023-06-13T17:43:21.156146: step 6443, loss 0.769851, acc 0.8125, learning_rate 0.000271384\n","2023-06-13T17:43:22.655070: step 6444, loss 0.479981, acc 0.84375, learning_rate 0.000271301\n","2023-06-13T17:43:24.148072: step 6445, loss 0.591658, acc 0.75, learning_rate 0.000271218\n","2023-06-13T17:43:25.652714: step 6446, loss 0.542031, acc 0.875, learning_rate 0.000271135\n","2023-06-13T17:43:27.140573: step 6447, loss 1.18738, acc 0.6875, learning_rate 0.000271052\n","2023-06-13T17:43:28.617398: step 6448, loss 0.304511, acc 0.96875, learning_rate 0.000270969\n","2023-06-13T17:43:30.103862: step 6449, loss 0.946742, acc 0.78125, learning_rate 0.000270886\n","2023-06-13T17:43:32.495634: step 6450, loss 0.555726, acc 0.75, learning_rate 0.000270803\n","2023-06-13T17:43:34.951770: step 6451, loss 0.373909, acc 0.90625, learning_rate 0.00027072\n","2023-06-13T17:43:37.402563: step 6452, loss 0.514881, acc 0.8125, learning_rate 0.000270637\n","2023-06-13T17:43:39.745959: step 6453, loss 0.614002, acc 0.8125, learning_rate 0.000270555\n","2023-06-13T17:43:41.197054: step 6454, loss 0.450566, acc 0.875, learning_rate 0.000270472\n","2023-06-13T17:43:42.640225: step 6455, loss 0.760488, acc 0.75, learning_rate 0.000270389\n","2023-06-13T17:43:44.134648: step 6456, loss 0.641186, acc 0.84375, learning_rate 0.000270307\n","2023-06-13T17:43:45.620584: step 6457, loss 0.561195, acc 0.78125, learning_rate 0.000270224\n","2023-06-13T17:43:47.088275: step 6458, loss 0.398429, acc 0.875, learning_rate 0.000270142\n","2023-06-13T17:43:48.548854: step 6459, loss 0.431051, acc 0.84375, learning_rate 0.000270059\n","2023-06-13T17:43:50.186596: step 6460, loss 0.396028, acc 0.9375, learning_rate 0.000269977\n","2023-06-13T17:43:52.837764: step 6461, loss 0.499202, acc 0.875, learning_rate 0.000269894\n","2023-06-13T17:43:55.203839: step 6462, loss 0.372056, acc 0.84375, learning_rate 0.000269812\n","2023-06-13T17:43:57.566078: step 6463, loss 0.643133, acc 0.75, learning_rate 0.00026973\n","2023-06-13T17:43:59.588422: step 6464, loss 0.628094, acc 0.8125, learning_rate 0.000269647\n","2023-06-13T17:44:01.044477: step 6465, loss 0.470497, acc 0.875, learning_rate 0.000269565\n","2023-06-13T17:44:02.523378: step 6466, loss 0.254408, acc 0.9375, learning_rate 0.000269483\n","2023-06-13T17:44:03.968192: step 6467, loss 0.363513, acc 0.875, learning_rate 0.000269401\n","2023-06-13T17:44:05.434681: step 6468, loss 0.437478, acc 0.84375, learning_rate 0.000269318\n","2023-06-13T17:44:06.902757: step 6469, loss 0.386558, acc 0.84375, learning_rate 0.000269236\n","2023-06-13T17:44:08.413913: step 6470, loss 0.516887, acc 0.875, learning_rate 0.000269154\n","2023-06-13T17:44:10.324058: step 6471, loss 0.836132, acc 0.71875, learning_rate 0.000269072\n","2023-06-13T17:44:12.871171: step 6472, loss 0.943647, acc 0.71875, learning_rate 0.00026899\n","2023-06-13T17:44:15.355821: step 6473, loss 0.589032, acc 0.84375, learning_rate 0.000268908\n","2023-06-13T17:44:17.676872: step 6474, loss 0.438165, acc 0.875, learning_rate 0.000268826\n","2023-06-13T17:44:19.493969: step 6475, loss 0.461029, acc 0.8125, learning_rate 0.000268745\n","2023-06-13T17:44:20.968757: step 6476, loss 0.794309, acc 0.75, learning_rate 0.000268663\n","2023-06-13T17:44:22.415086: step 6477, loss 0.662297, acc 0.75, learning_rate 0.000268581\n","2023-06-13T17:44:23.893485: step 6478, loss 0.560478, acc 0.875, learning_rate 0.000268499\n","2023-06-13T17:44:25.381611: step 6479, loss 0.72031, acc 0.75, learning_rate 0.000268418\n","2023-06-13T17:44:26.860772: step 6480, loss 0.871696, acc 0.75, learning_rate 0.000268336\n","2023-06-13T17:44:28.328282: step 6481, loss 0.565441, acc 0.8125, learning_rate 0.000268254\n","2023-06-13T17:44:30.593536: step 6482, loss 0.61771, acc 0.75, learning_rate 0.000268173\n","2023-06-13T17:44:33.119351: step 6483, loss 0.565406, acc 0.8125, learning_rate 0.000268091\n","2023-06-13T17:44:35.596150: step 6484, loss 0.55726, acc 0.78125, learning_rate 0.00026801\n","2023-06-13T17:44:38.337040: step 6485, loss 0.534719, acc 0.8125, learning_rate 0.000267928\n","2023-06-13T17:44:41.048168: step 6486, loss 0.476209, acc 0.84375, learning_rate 0.000267847\n","2023-06-13T17:44:43.535409: step 6487, loss 0.835968, acc 0.6875, learning_rate 0.000267765\n","2023-06-13T17:44:45.984414: step 6488, loss 0.704946, acc 0.75, learning_rate 0.000267684\n","2023-06-13T17:44:47.736220: step 6489, loss 0.484239, acc 0.8125, learning_rate 0.000267603\n","2023-06-13T17:44:49.241588: step 6490, loss 0.602483, acc 0.78125, learning_rate 0.000267521\n","2023-06-13T17:44:51.110207: step 6491, loss 0.537419, acc 0.78125, learning_rate 0.00026744\n","2023-06-13T17:44:53.664860: step 6492, loss 0.693003, acc 0.6875, learning_rate 0.000267359\n","2023-06-13T17:44:55.968365: step 6493, loss 0.50855, acc 0.875, learning_rate 0.000267278\n","2023-06-13T17:44:58.321056: step 6494, loss 0.706371, acc 0.8125, learning_rate 0.000267197\n","2023-06-13T17:45:00.198100: step 6495, loss 0.351877, acc 0.875, learning_rate 0.000267116\n","2023-06-13T17:45:01.643765: step 6496, loss 0.606662, acc 0.78125, learning_rate 0.000267035\n","2023-06-13T17:45:03.111085: step 6497, loss 0.589616, acc 0.75, learning_rate 0.000266954\n","2023-06-13T17:45:04.584330: step 6498, loss 0.399764, acc 0.84375, learning_rate 0.000266873\n","2023-06-13T17:45:06.006249: step 6499, loss 0.649074, acc 0.875, learning_rate 0.000266792\n","\n","Evaluation:\n","2023-06-13T17:45:37.403953: step 6500, loss 2.81478, acc 0.384056\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6500\n","\n","2023-06-13T17:45:39.677356: step 6500, loss 0.717917, acc 0.78125, learning_rate 0.000266711\n","2023-06-13T17:45:41.156191: step 6501, loss 0.56342, acc 0.84375, learning_rate 0.00026663\n","2023-06-13T17:45:42.622118: step 6502, loss 0.480056, acc 0.9375, learning_rate 0.000266549\n","2023-06-13T17:45:44.155909: step 6503, loss 0.661967, acc 0.84375, learning_rate 0.000266469\n","2023-06-13T17:45:45.643786: step 6504, loss 0.892731, acc 0.65625, learning_rate 0.000266388\n","2023-06-13T17:45:47.072580: step 6505, loss 0.353462, acc 0.875, learning_rate 0.000266307\n","2023-06-13T17:45:48.528436: step 6506, loss 0.539191, acc 0.78125, learning_rate 0.000266227\n","2023-06-13T17:45:50.587593: step 6507, loss 0.711659, acc 0.6875, learning_rate 0.000266146\n","2023-06-13T17:45:53.127416: step 6508, loss 0.47613, acc 0.84375, learning_rate 0.000266065\n","2023-06-13T17:45:55.582593: step 6509, loss 0.561309, acc 0.8125, learning_rate 0.000265985\n","2023-06-13T17:45:57.981992: step 6510, loss 0.82441, acc 0.71875, learning_rate 0.000265904\n","2023-06-13T17:45:59.618426: step 6511, loss 0.377008, acc 0.875, learning_rate 0.000265824\n","2023-06-13T17:46:01.094520: step 6512, loss 0.386178, acc 0.875, learning_rate 0.000265744\n","2023-06-13T17:46:02.559871: step 6513, loss 0.776667, acc 0.6875, learning_rate 0.000265663\n","2023-06-13T17:46:04.008501: step 6514, loss 0.706629, acc 0.71875, learning_rate 0.000265583\n","2023-06-13T17:46:05.443654: step 6515, loss 0.418888, acc 0.84375, learning_rate 0.000265503\n","2023-06-13T17:46:06.893988: step 6516, loss 0.634779, acc 0.84375, learning_rate 0.000265422\n","2023-06-13T17:46:08.385364: step 6517, loss 0.507088, acc 0.84375, learning_rate 0.000265342\n","2023-06-13T17:46:10.859626: step 6518, loss 0.73839, acc 0.71875, learning_rate 0.000265262\n","2023-06-13T17:46:13.418535: step 6519, loss 0.996918, acc 0.84375, learning_rate 0.000265182\n","2023-06-13T17:46:15.911009: step 6520, loss 0.436416, acc 0.84375, learning_rate 0.000265102\n","2023-06-13T17:46:18.699016: step 6521, loss 0.382551, acc 0.875, learning_rate 0.000265022\n","2023-06-13T17:46:21.270863: step 6522, loss 0.379684, acc 0.84375, learning_rate 0.000264942\n","2023-06-13T17:46:23.713021: step 6523, loss 0.463394, acc 0.875, learning_rate 0.000264862\n","2023-06-13T17:46:26.230062: step 6524, loss 0.480604, acc 0.875, learning_rate 0.000264782\n","2023-06-13T17:46:27.795180: step 6525, loss 0.495808, acc 0.875, learning_rate 0.000264702\n","2023-06-13T17:46:29.307539: step 6526, loss 0.490475, acc 0.875, learning_rate 0.000264622\n","2023-06-13T17:46:31.302938: step 6527, loss 0.572241, acc 0.78125, learning_rate 0.000264542\n","2023-06-13T17:46:33.751399: step 6528, loss 0.631054, acc 0.78125, learning_rate 0.000264462\n","2023-06-13T17:46:36.257759: step 6529, loss 0.418063, acc 0.875, learning_rate 0.000264383\n","2023-06-13T17:46:38.658656: step 6530, loss 0.519633, acc 0.84375, learning_rate 0.000264303\n","2023-06-13T17:46:40.466782: step 6531, loss 0.668143, acc 0.6875, learning_rate 0.000264223\n","2023-06-13T17:46:41.934254: step 6532, loss 0.373563, acc 0.875, learning_rate 0.000264144\n","2023-06-13T17:46:43.383937: step 6533, loss 0.274646, acc 0.9375, learning_rate 0.000264064\n","2023-06-13T17:46:44.805294: step 6534, loss 0.569012, acc 0.8125, learning_rate 0.000263984\n","2023-06-13T17:46:46.270650: step 6535, loss 0.504328, acc 0.84375, learning_rate 0.000263905\n","2023-06-13T17:46:47.729443: step 6536, loss 0.53022, acc 0.8125, learning_rate 0.000263825\n","2023-06-13T17:46:49.182323: step 6537, loss 0.656901, acc 0.71875, learning_rate 0.000263746\n","2023-06-13T17:46:51.272229: step 6538, loss 0.570102, acc 0.8125, learning_rate 0.000263667\n","2023-06-13T17:46:53.788408: step 6539, loss 0.30869, acc 0.875, learning_rate 0.000263587\n","2023-06-13T17:46:56.031377: step 6540, loss 0.428832, acc 0.84375, learning_rate 0.000263508\n","2023-06-13T17:46:58.351207: step 6541, loss 0.320655, acc 0.875, learning_rate 0.000263429\n","2023-06-13T17:47:00.217418: step 6542, loss 1.02396, acc 0.6875, learning_rate 0.000263349\n","2023-06-13T17:47:01.672350: step 6543, loss 0.696694, acc 0.78125, learning_rate 0.00026327\n","2023-06-13T17:47:03.085589: step 6544, loss 0.25503, acc 0.9375, learning_rate 0.000263191\n","2023-06-13T17:47:04.530905: step 6545, loss 0.431983, acc 0.875, learning_rate 0.000263112\n","2023-06-13T17:47:05.963315: step 6546, loss 1.10157, acc 0.59375, learning_rate 0.000263033\n","2023-06-13T17:47:07.435878: step 6547, loss 0.497991, acc 0.8125, learning_rate 0.000262954\n","2023-06-13T17:47:08.906269: step 6548, loss 0.683256, acc 0.78125, learning_rate 0.000262875\n","2023-06-13T17:47:11.074711: step 6549, loss 0.696485, acc 0.71875, learning_rate 0.000262796\n","2023-06-13T17:47:13.610066: step 6550, loss 0.573467, acc 0.78125, learning_rate 0.000262717\n","2023-06-13T17:47:15.966119: step 6551, loss 0.724159, acc 0.78125, learning_rate 0.000262638\n","2023-06-13T17:47:18.371630: step 6552, loss 0.788174, acc 0.6875, learning_rate 0.000262559\n","2023-06-13T17:47:20.063890: step 6553, loss 0.718691, acc 0.8125, learning_rate 0.00026248\n","2023-06-13T17:47:21.508324: step 6554, loss 0.291524, acc 0.875, learning_rate 0.000262401\n","2023-06-13T17:47:22.947705: step 6555, loss 0.838097, acc 0.71875, learning_rate 0.000262323\n","2023-06-13T17:47:24.396687: step 6556, loss 0.763522, acc 0.84375, learning_rate 0.000262244\n","2023-06-13T17:47:25.850669: step 6557, loss 0.460363, acc 0.875, learning_rate 0.000262165\n","2023-06-13T17:47:27.310580: step 6558, loss 0.835707, acc 0.75, learning_rate 0.000262087\n","2023-06-13T17:47:28.793922: step 6559, loss 0.497653, acc 0.875, learning_rate 0.000262008\n","2023-06-13T17:47:31.199240: step 6560, loss 0.675608, acc 0.78125, learning_rate 0.00026193\n","2023-06-13T17:47:33.661458: step 6561, loss 0.632864, acc 0.75, learning_rate 0.000261851\n","2023-06-13T17:47:36.076462: step 6562, loss 0.657885, acc 0.78125, learning_rate 0.000261773\n","2023-06-13T17:47:38.491864: step 6563, loss 0.321252, acc 0.9375, learning_rate 0.000261694\n","2023-06-13T17:47:39.970480: step 6564, loss 0.578473, acc 0.8125, learning_rate 0.000261616\n","2023-06-13T17:47:41.451412: step 6565, loss 0.775522, acc 0.78125, learning_rate 0.000261537\n","2023-06-13T17:47:42.890212: step 6566, loss 0.419947, acc 0.8125, learning_rate 0.000261459\n","2023-06-13T17:47:44.349841: step 6567, loss 0.452953, acc 0.8125, learning_rate 0.000261381\n","2023-06-13T17:47:45.796326: step 6568, loss 0.643251, acc 0.71875, learning_rate 0.000261303\n","2023-06-13T17:47:47.307654: step 6569, loss 0.403654, acc 0.875, learning_rate 0.000261224\n","2023-06-13T17:47:48.993654: step 6570, loss 0.429482, acc 0.84375, learning_rate 0.000261146\n","2023-06-13T17:47:51.508651: step 6571, loss 0.83515, acc 0.75, learning_rate 0.000261068\n","2023-06-13T17:47:53.967664: step 6572, loss 0.753715, acc 0.78125, learning_rate 0.00026099\n","2023-06-13T17:47:56.331555: step 6573, loss 0.691634, acc 0.6875, learning_rate 0.000260912\n","2023-06-13T17:47:58.298241: step 6574, loss 0.760925, acc 0.75, learning_rate 0.000260834\n","2023-06-13T17:47:59.765679: step 6575, loss 0.66569, acc 0.84375, learning_rate 0.000260756\n","2023-06-13T17:48:01.278059: step 6576, loss 0.694674, acc 0.75, learning_rate 0.000260678\n","2023-06-13T17:48:02.716024: step 6577, loss 0.44996, acc 0.90625, learning_rate 0.0002606\n","2023-06-13T17:48:04.163226: step 6578, loss 0.381455, acc 0.9375, learning_rate 0.000260522\n","2023-06-13T17:48:05.638554: step 6579, loss 0.455172, acc 0.84375, learning_rate 0.000260444\n","2023-06-13T17:48:07.149649: step 6580, loss 0.389478, acc 0.90625, learning_rate 0.000260366\n","2023-06-13T17:48:09.322655: step 6581, loss 0.586001, acc 0.78125, learning_rate 0.000260289\n","2023-06-13T17:48:11.928557: step 6582, loss 0.422555, acc 0.78125, learning_rate 0.000260211\n","2023-06-13T17:48:14.363241: step 6583, loss 0.81456, acc 0.6875, learning_rate 0.000260133\n","2023-06-13T17:48:16.722480: step 6584, loss 0.820242, acc 0.8125, learning_rate 0.000260056\n","2023-06-13T17:48:18.400685: step 6585, loss 0.991278, acc 0.71875, learning_rate 0.000259978\n","2023-06-13T17:48:19.892456: step 6586, loss 0.315907, acc 0.84375, learning_rate 0.000259901\n","2023-06-13T17:48:21.387769: step 6587, loss 0.564891, acc 0.75, learning_rate 0.000259823\n","2023-06-13T17:48:22.889847: step 6588, loss 0.555758, acc 0.8125, learning_rate 0.000259745\n","2023-06-13T17:48:24.370472: step 6589, loss 0.50621, acc 0.84375, learning_rate 0.000259668\n","2023-06-13T17:48:25.866036: step 6590, loss 0.601658, acc 0.8125, learning_rate 0.000259591\n","2023-06-13T17:48:27.519861: step 6591, loss 0.442841, acc 0.84375, learning_rate 0.000259513\n","2023-06-13T17:48:30.161070: step 6592, loss 0.559049, acc 0.8125, learning_rate 0.000259436\n","2023-06-13T17:48:32.631433: step 6593, loss 0.505207, acc 0.875, learning_rate 0.000259359\n","2023-06-13T17:48:35.006911: step 6594, loss 0.749299, acc 0.71875, learning_rate 0.000259281\n","2023-06-13T17:48:37.162496: step 6595, loss 0.561863, acc 0.78125, learning_rate 0.000259204\n","2023-06-13T17:48:38.643940: step 6596, loss 0.412082, acc 0.84375, learning_rate 0.000259127\n","2023-06-13T17:48:40.128920: step 6597, loss 0.576678, acc 0.71875, learning_rate 0.00025905\n","2023-06-13T17:48:41.599273: step 6598, loss 0.930819, acc 0.6875, learning_rate 0.000258973\n","2023-06-13T17:48:42.739639: step 6599, loss 0.48405, acc 0.833333, learning_rate 0.000258896\n","\n","Evaluation:\n","2023-06-13T17:49:14.564708: step 6600, loss 2.81246, acc 0.382692\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6600\n","\n","2023-06-13T17:49:17.480154: step 6600, loss 0.51667, acc 0.84375, learning_rate 0.000258818\n","2023-06-13T17:49:20.150951: step 6601, loss 0.423379, acc 0.84375, learning_rate 0.000258741\n","2023-06-13T17:49:22.726662: step 6602, loss 0.614328, acc 0.84375, learning_rate 0.000258664\n","2023-06-13T17:49:24.736996: step 6603, loss 0.605041, acc 0.8125, learning_rate 0.000258588\n","2023-06-13T17:49:26.230919: step 6604, loss 0.70211, acc 0.78125, learning_rate 0.000258511\n","2023-06-13T17:49:27.706287: step 6605, loss 0.556226, acc 0.75, learning_rate 0.000258434\n","2023-06-13T17:49:29.159289: step 6606, loss 0.757168, acc 0.84375, learning_rate 0.000258357\n","2023-06-13T17:49:30.633267: step 6607, loss 0.365605, acc 0.9375, learning_rate 0.00025828\n","2023-06-13T17:49:32.076651: step 6608, loss 0.642928, acc 0.75, learning_rate 0.000258203\n","2023-06-13T17:49:33.816634: step 6609, loss 0.801571, acc 0.78125, learning_rate 0.000258127\n","2023-06-13T17:49:36.363760: step 6610, loss 0.596254, acc 0.8125, learning_rate 0.00025805\n","2023-06-13T17:49:38.776318: step 6611, loss 0.55366, acc 0.78125, learning_rate 0.000257973\n","2023-06-13T17:49:41.179845: step 6612, loss 0.54118, acc 0.8125, learning_rate 0.000257897\n","2023-06-13T17:49:43.224448: step 6613, loss 0.421547, acc 0.8125, learning_rate 0.00025782\n","2023-06-13T17:49:44.670917: step 6614, loss 0.514262, acc 0.78125, learning_rate 0.000257744\n","2023-06-13T17:49:46.133885: step 6615, loss 0.710808, acc 0.78125, learning_rate 0.000257667\n","2023-06-13T17:49:47.577122: step 6616, loss 0.510367, acc 0.78125, learning_rate 0.000257591\n","2023-06-13T17:49:49.050683: step 6617, loss 0.336357, acc 0.875, learning_rate 0.000257514\n","2023-06-13T17:49:50.506873: step 6618, loss 0.500681, acc 0.75, learning_rate 0.000257438\n","2023-06-13T17:49:51.945352: step 6619, loss 0.52817, acc 0.90625, learning_rate 0.000257362\n","2023-06-13T17:49:53.802278: step 6620, loss 0.568374, acc 0.78125, learning_rate 0.000257285\n","2023-06-13T17:49:56.343032: step 6621, loss 0.755415, acc 0.6875, learning_rate 0.000257209\n","2023-06-13T17:49:58.771092: step 6622, loss 0.38925, acc 0.84375, learning_rate 0.000257133\n","2023-06-13T17:50:01.296768: step 6623, loss 0.707081, acc 0.78125, learning_rate 0.000257057\n","2023-06-13T17:50:03.073880: step 6624, loss 0.619654, acc 0.75, learning_rate 0.000256981\n","2023-06-13T17:50:04.575278: step 6625, loss 0.45962, acc 0.78125, learning_rate 0.000256904\n","2023-06-13T17:50:06.060301: step 6626, loss 0.381319, acc 0.84375, learning_rate 0.000256828\n","2023-06-13T17:50:07.539094: step 6627, loss 0.514525, acc 0.90625, learning_rate 0.000256752\n","2023-06-13T17:50:09.016779: step 6628, loss 0.701211, acc 0.8125, learning_rate 0.000256676\n","2023-06-13T17:50:10.541297: step 6629, loss 0.401037, acc 0.875, learning_rate 0.0002566\n","2023-06-13T17:50:11.992585: step 6630, loss 0.258594, acc 0.90625, learning_rate 0.000256524\n","2023-06-13T17:50:14.380032: step 6631, loss 0.4057, acc 0.84375, learning_rate 0.000256449\n","2023-06-13T17:50:16.804683: step 6632, loss 0.755201, acc 0.75, learning_rate 0.000256373\n","2023-06-13T17:50:19.326618: step 6633, loss 0.58248, acc 0.8125, learning_rate 0.000256297\n","2023-06-13T17:50:21.640113: step 6634, loss 0.686578, acc 0.78125, learning_rate 0.000256221\n","2023-06-13T17:50:23.092220: step 6635, loss 0.59856, acc 0.78125, learning_rate 0.000256145\n","2023-06-13T17:50:24.554707: step 6636, loss 0.282355, acc 0.9375, learning_rate 0.00025607\n","2023-06-13T17:50:26.018698: step 6637, loss 0.248244, acc 0.90625, learning_rate 0.000255994\n","2023-06-13T17:50:27.528492: step 6638, loss 0.600783, acc 0.8125, learning_rate 0.000255918\n","2023-06-13T17:50:29.011886: step 6639, loss 0.244032, acc 0.9375, learning_rate 0.000255843\n","2023-06-13T17:50:30.500893: step 6640, loss 0.807578, acc 0.6875, learning_rate 0.000255767\n","2023-06-13T17:50:32.313199: step 6641, loss 0.717222, acc 0.71875, learning_rate 0.000255692\n","2023-06-13T17:50:34.913059: step 6642, loss 0.570494, acc 0.78125, learning_rate 0.000255616\n","2023-06-13T17:50:37.188171: step 6643, loss 0.431147, acc 0.875, learning_rate 0.000255541\n","2023-06-13T17:50:39.543003: step 6644, loss 0.660544, acc 0.75, learning_rate 0.000255465\n","2023-06-13T17:50:41.565375: step 6645, loss 0.365881, acc 0.84375, learning_rate 0.00025539\n","2023-06-13T17:50:43.020764: step 6646, loss 0.494295, acc 0.875, learning_rate 0.000255314\n","2023-06-13T17:50:44.472401: step 6647, loss 0.540035, acc 0.78125, learning_rate 0.000255239\n","2023-06-13T17:50:45.935124: step 6648, loss 0.151807, acc 0.96875, learning_rate 0.000255164\n","2023-06-13T17:50:47.341324: step 6649, loss 0.582622, acc 0.8125, learning_rate 0.000255089\n","2023-06-13T17:50:48.785538: step 6650, loss 0.440909, acc 0.8125, learning_rate 0.000255013\n","2023-06-13T17:50:50.287302: step 6651, loss 0.658374, acc 0.78125, learning_rate 0.000254938\n","2023-06-13T17:50:52.125707: step 6652, loss 0.72348, acc 0.71875, learning_rate 0.000254863\n","2023-06-13T17:50:54.868941: step 6653, loss 0.743955, acc 0.75, learning_rate 0.000254788\n","2023-06-13T17:50:57.680416: step 6654, loss 0.479576, acc 0.875, learning_rate 0.000254713\n","2023-06-13T17:51:00.284485: step 6655, loss 0.739307, acc 0.78125, learning_rate 0.000254638\n","2023-06-13T17:51:02.875162: step 6656, loss 0.584789, acc 0.84375, learning_rate 0.000254563\n","2023-06-13T17:51:05.559188: step 6657, loss 0.600505, acc 0.78125, learning_rate 0.000254488\n","2023-06-13T17:51:07.948791: step 6658, loss 0.631889, acc 0.84375, learning_rate 0.000254413\n","2023-06-13T17:51:09.493412: step 6659, loss 0.720861, acc 0.78125, learning_rate 0.000254338\n","2023-06-13T17:51:11.008570: step 6660, loss 0.506452, acc 0.78125, learning_rate 0.000254263\n","2023-06-13T17:51:12.516164: step 6661, loss 0.426221, acc 0.90625, learning_rate 0.000254189\n","2023-06-13T17:51:14.021216: step 6662, loss 0.970314, acc 0.65625, learning_rate 0.000254114\n","2023-06-13T17:51:15.651378: step 6663, loss 0.48248, acc 0.8125, learning_rate 0.000254039\n","2023-06-13T17:51:18.197276: step 6664, loss 0.398074, acc 0.84375, learning_rate 0.000253965\n","2023-06-13T17:51:20.859120: step 6665, loss 0.403072, acc 0.90625, learning_rate 0.00025389\n","2023-06-13T17:51:23.262189: step 6666, loss 0.78274, acc 0.71875, learning_rate 0.000253815\n","2023-06-13T17:51:25.219574: step 6667, loss 0.258925, acc 0.9375, learning_rate 0.000253741\n","2023-06-13T17:51:26.659052: step 6668, loss 0.606998, acc 0.84375, learning_rate 0.000253666\n","2023-06-13T17:51:28.109606: step 6669, loss 0.494717, acc 0.84375, learning_rate 0.000253592\n","2023-06-13T17:51:29.558620: step 6670, loss 0.333211, acc 0.90625, learning_rate 0.000253517\n","2023-06-13T17:51:31.035097: step 6671, loss 0.555892, acc 0.8125, learning_rate 0.000253443\n","2023-06-13T17:51:32.490620: step 6672, loss 0.521636, acc 0.84375, learning_rate 0.000253368\n","2023-06-13T17:51:33.960358: step 6673, loss 0.457822, acc 0.84375, learning_rate 0.000253294\n","2023-06-13T17:51:35.835915: step 6674, loss 0.818055, acc 0.75, learning_rate 0.00025322\n","2023-06-13T17:51:38.485692: step 6675, loss 0.6409, acc 0.71875, learning_rate 0.000253145\n","2023-06-13T17:51:40.798885: step 6676, loss 0.508621, acc 0.875, learning_rate 0.000253071\n","2023-06-13T17:51:43.234072: step 6677, loss 0.285312, acc 0.9375, learning_rate 0.000252997\n","2023-06-13T17:51:45.116635: step 6678, loss 0.512787, acc 0.8125, learning_rate 0.000252923\n","2023-06-13T17:51:46.581444: step 6679, loss 0.373773, acc 0.875, learning_rate 0.000252848\n","2023-06-13T17:51:48.021953: step 6680, loss 0.751986, acc 0.78125, learning_rate 0.000252774\n","2023-06-13T17:51:49.475651: step 6681, loss 0.578277, acc 0.84375, learning_rate 0.0002527\n","2023-06-13T17:51:50.957718: step 6682, loss 0.23128, acc 0.90625, learning_rate 0.000252626\n","2023-06-13T17:51:52.456786: step 6683, loss 0.561127, acc 0.8125, learning_rate 0.000252552\n","2023-06-13T17:51:53.896830: step 6684, loss 0.320482, acc 0.84375, learning_rate 0.000252478\n","2023-06-13T17:51:55.993400: step 6685, loss 0.533181, acc 0.84375, learning_rate 0.000252404\n","2023-06-13T17:51:58.431781: step 6686, loss 0.635469, acc 0.78125, learning_rate 0.00025233\n","2023-06-13T17:52:00.839693: step 6687, loss 0.582246, acc 0.78125, learning_rate 0.000252257\n","2023-06-13T17:52:03.333500: step 6688, loss 0.491674, acc 0.8125, learning_rate 0.000252183\n","2023-06-13T17:52:04.910410: step 6689, loss 0.279994, acc 0.875, learning_rate 0.000252109\n","2023-06-13T17:52:06.346286: step 6690, loss 0.456928, acc 0.75, learning_rate 0.000252035\n","2023-06-13T17:52:07.776827: step 6691, loss 0.371525, acc 0.90625, learning_rate 0.000251962\n","2023-06-13T17:52:09.218394: step 6692, loss 0.515325, acc 0.84375, learning_rate 0.000251888\n","2023-06-13T17:52:10.680291: step 6693, loss 0.486276, acc 0.90625, learning_rate 0.000251814\n","2023-06-13T17:52:12.144461: step 6694, loss 0.870005, acc 0.71875, learning_rate 0.000251741\n","2023-06-13T17:52:13.586390: step 6695, loss 0.675525, acc 0.75, learning_rate 0.000251667\n","2023-06-13T17:52:16.111727: step 6696, loss 0.387136, acc 0.875, learning_rate 0.000251593\n","2023-06-13T17:52:18.399706: step 6697, loss 0.531105, acc 0.875, learning_rate 0.00025152\n","2023-06-13T17:52:20.713593: step 6698, loss 0.822744, acc 0.65625, learning_rate 0.000251446\n","2023-06-13T17:52:22.928603: step 6699, loss 0.532611, acc 0.78125, learning_rate 0.000251373\n","\n","Evaluation:\n","2023-06-13T17:52:50.537001: step 6700, loss 2.83629, acc 0.383904\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6700\n","\n","2023-06-13T17:52:52.148546: step 6700, loss 0.859376, acc 0.65625, learning_rate 0.0002513\n","2023-06-13T17:52:53.850087: step 6701, loss 0.349798, acc 0.875, learning_rate 0.000251226\n","2023-06-13T17:52:56.438635: step 6702, loss 0.35458, acc 0.9375, learning_rate 0.000251153\n","2023-06-13T17:52:58.890421: step 6703, loss 0.437737, acc 0.78125, learning_rate 0.00025108\n","2023-06-13T17:53:01.223849: step 6704, loss 0.480689, acc 0.875, learning_rate 0.000251006\n","2023-06-13T17:53:03.203173: step 6705, loss 1.16556, acc 0.75, learning_rate 0.000250933\n","2023-06-13T17:53:04.638256: step 6706, loss 0.432303, acc 0.84375, learning_rate 0.00025086\n","2023-06-13T17:53:06.082536: step 6707, loss 0.464776, acc 0.8125, learning_rate 0.000250787\n","2023-06-13T17:53:07.532852: step 6708, loss 0.524143, acc 0.78125, learning_rate 0.000250714\n","2023-06-13T17:53:09.016926: step 6709, loss 0.635498, acc 0.78125, learning_rate 0.000250641\n","2023-06-13T17:53:10.484063: step 6710, loss 0.537592, acc 0.8125, learning_rate 0.000250568\n","2023-06-13T17:53:11.940257: step 6711, loss 0.296151, acc 0.9375, learning_rate 0.000250495\n","2023-06-13T17:53:13.994397: step 6712, loss 0.407203, acc 0.875, learning_rate 0.000250422\n","2023-06-13T17:53:16.471881: step 6713, loss 0.705455, acc 0.8125, learning_rate 0.000250349\n","2023-06-13T17:53:18.829099: step 6714, loss 0.453142, acc 0.8125, learning_rate 0.000250276\n","2023-06-13T17:53:21.156902: step 6715, loss 0.325319, acc 0.9375, learning_rate 0.000250203\n","2023-06-13T17:53:22.855779: step 6716, loss 0.438902, acc 0.75, learning_rate 0.00025013\n","2023-06-13T17:53:24.288743: step 6717, loss 0.556013, acc 0.8125, learning_rate 0.000250057\n","2023-06-13T17:53:25.799759: step 6718, loss 0.2682, acc 0.9375, learning_rate 0.000249985\n","2023-06-13T17:53:27.265337: step 6719, loss 0.352398, acc 0.90625, learning_rate 0.000249912\n","2023-06-13T17:53:28.758614: step 6720, loss 0.643756, acc 0.75, learning_rate 0.000249839\n","2023-06-13T17:53:30.201496: step 6721, loss 0.48419, acc 0.78125, learning_rate 0.000249766\n","2023-06-13T17:53:31.625724: step 6722, loss 0.553131, acc 0.78125, learning_rate 0.000249694\n","2023-06-13T17:53:33.922579: step 6723, loss 0.680704, acc 0.71875, learning_rate 0.000249621\n","2023-06-13T17:53:36.312434: step 6724, loss 0.617281, acc 0.71875, learning_rate 0.000249549\n","2023-06-13T17:53:38.635135: step 6725, loss 0.478537, acc 0.875, learning_rate 0.000249476\n","2023-06-13T17:53:40.883536: step 6726, loss 0.791927, acc 0.71875, learning_rate 0.000249404\n","2023-06-13T17:53:43.312165: step 6727, loss 0.552712, acc 0.75, learning_rate 0.000249331\n","2023-06-13T17:53:45.708018: step 6728, loss 0.482081, acc 0.84375, learning_rate 0.000249259\n","2023-06-13T17:53:48.174030: step 6729, loss 0.698088, acc 0.78125, learning_rate 0.000249187\n","2023-06-13T17:53:50.590972: step 6730, loss 0.542635, acc 0.8125, learning_rate 0.000249114\n","2023-06-13T17:53:52.309919: step 6731, loss 0.518017, acc 0.875, learning_rate 0.000249042\n","2023-06-13T17:53:54.761008: step 6732, loss 0.688449, acc 0.78125, learning_rate 0.00024897\n","2023-06-13T17:53:57.279472: step 6733, loss 0.867902, acc 0.75, learning_rate 0.000248897\n","2023-06-13T17:53:59.733389: step 6734, loss 0.949836, acc 0.78125, learning_rate 0.000248825\n","2023-06-13T17:54:01.543197: step 6735, loss 0.67168, acc 0.75, learning_rate 0.000248753\n","2023-06-13T17:54:02.976664: step 6736, loss 0.581593, acc 0.8125, learning_rate 0.000248681\n","2023-06-13T17:54:04.413914: step 6737, loss 0.475504, acc 0.8125, learning_rate 0.000248609\n","2023-06-13T17:54:05.840131: step 6738, loss 0.305766, acc 0.90625, learning_rate 0.000248537\n","2023-06-13T17:54:07.251443: step 6739, loss 0.363155, acc 0.84375, learning_rate 0.000248465\n","2023-06-13T17:54:08.682596: step 6740, loss 0.597208, acc 0.78125, learning_rate 0.000248393\n","2023-06-13T17:54:10.102288: step 6741, loss 0.702622, acc 0.78125, learning_rate 0.000248321\n","2023-06-13T17:54:12.279608: step 6742, loss 0.549434, acc 0.8125, learning_rate 0.000248249\n","2023-06-13T17:54:14.711579: step 6743, loss 0.537607, acc 0.84375, learning_rate 0.000248177\n","2023-06-13T17:54:17.231463: step 6744, loss 0.637886, acc 0.75, learning_rate 0.000248105\n","2023-06-13T17:54:19.590512: step 6745, loss 0.581961, acc 0.84375, learning_rate 0.000248033\n","2023-06-13T17:54:21.016198: step 6746, loss 0.505616, acc 0.84375, learning_rate 0.000247962\n","2023-06-13T17:54:22.443213: step 6747, loss 0.693653, acc 0.71875, learning_rate 0.00024789\n","2023-06-13T17:54:23.859179: step 6748, loss 0.772624, acc 0.78125, learning_rate 0.000247818\n","2023-06-13T17:54:25.264158: step 6749, loss 0.518219, acc 0.84375, learning_rate 0.000247746\n","2023-06-13T17:54:26.742108: step 6750, loss 0.795137, acc 0.6875, learning_rate 0.000247675\n","2023-06-13T17:54:28.205569: step 6751, loss 0.804186, acc 0.71875, learning_rate 0.000247603\n","2023-06-13T17:54:29.652086: step 6752, loss 0.507275, acc 0.8125, learning_rate 0.000247532\n","2023-06-13T17:54:32.119206: step 6753, loss 0.521601, acc 0.8125, learning_rate 0.00024746\n","2023-06-13T17:54:34.592543: step 6754, loss 0.885337, acc 0.71875, learning_rate 0.000247389\n","2023-06-13T17:54:36.908690: step 6755, loss 0.559472, acc 0.84375, learning_rate 0.000247317\n","2023-06-13T17:54:39.207910: step 6756, loss 0.390813, acc 0.84375, learning_rate 0.000247246\n","2023-06-13T17:54:40.681293: step 6757, loss 0.273302, acc 0.875, learning_rate 0.000247174\n","2023-06-13T17:54:42.159776: step 6758, loss 0.380556, acc 0.90625, learning_rate 0.000247103\n","2023-06-13T17:54:43.626650: step 6759, loss 0.428753, acc 0.8125, learning_rate 0.000247032\n","2023-06-13T17:54:45.086677: step 6760, loss 0.569376, acc 0.75, learning_rate 0.00024696\n","2023-06-13T17:54:46.492752: step 6761, loss 0.85878, acc 0.8125, learning_rate 0.000246889\n","2023-06-13T17:54:47.929773: step 6762, loss 0.37234, acc 0.90625, learning_rate 0.000246818\n","2023-06-13T17:54:49.609857: step 6763, loss 0.386093, acc 0.875, learning_rate 0.000246747\n","2023-06-13T17:54:52.134179: step 6764, loss 0.893028, acc 0.71875, learning_rate 0.000246675\n","2023-06-13T17:54:54.466458: step 6765, loss 0.697149, acc 0.8125, learning_rate 0.000246604\n","2023-06-13T17:54:56.845740: step 6766, loss 1.01746, acc 0.71875, learning_rate 0.000246533\n","2023-06-13T17:54:58.941097: step 6767, loss 0.683512, acc 0.6875, learning_rate 0.000246462\n","2023-06-13T17:55:00.369887: step 6768, loss 0.404961, acc 0.84375, learning_rate 0.000246391\n","2023-06-13T17:55:01.814202: step 6769, loss 0.448827, acc 0.78125, learning_rate 0.00024632\n","2023-06-13T17:55:03.254477: step 6770, loss 0.335427, acc 0.90625, learning_rate 0.000246249\n","2023-06-13T17:55:04.713010: step 6771, loss 0.264464, acc 0.9375, learning_rate 0.000246178\n","2023-06-13T17:55:06.154696: step 6772, loss 0.610412, acc 0.84375, learning_rate 0.000246107\n","2023-06-13T17:55:07.611684: step 6773, loss 0.626768, acc 0.78125, learning_rate 0.000246037\n","2023-06-13T17:55:09.451504: step 6774, loss 0.567013, acc 0.84375, learning_rate 0.000245966\n","2023-06-13T17:55:11.982717: step 6775, loss 0.44021, acc 0.84375, learning_rate 0.000245895\n","2023-06-13T17:55:14.322602: step 6776, loss 0.363252, acc 0.875, learning_rate 0.000245824\n","2023-06-13T17:55:16.549073: step 6777, loss 0.55972, acc 0.78125, learning_rate 0.000245754\n","2023-06-13T17:55:18.577285: step 6778, loss 0.551657, acc 0.78125, learning_rate 0.000245683\n","2023-06-13T17:55:20.007796: step 6779, loss 0.513256, acc 0.84375, learning_rate 0.000245612\n","2023-06-13T17:55:21.414859: step 6780, loss 0.292104, acc 0.90625, learning_rate 0.000245542\n","2023-06-13T17:55:22.865341: step 6781, loss 0.258301, acc 0.9375, learning_rate 0.000245471\n","2023-06-13T17:55:24.282598: step 6782, loss 0.631348, acc 0.8125, learning_rate 0.000245401\n","2023-06-13T17:55:25.703445: step 6783, loss 0.580055, acc 0.8125, learning_rate 0.00024533\n","2023-06-13T17:55:27.153677: step 6784, loss 0.411241, acc 0.84375, learning_rate 0.00024526\n","2023-06-13T17:55:28.926583: step 6785, loss 0.368777, acc 0.90625, learning_rate 0.000245189\n","2023-06-13T17:55:31.585304: step 6786, loss 0.392201, acc 0.84375, learning_rate 0.000245119\n","2023-06-13T17:55:34.344624: step 6787, loss 0.495326, acc 0.84375, learning_rate 0.000245048\n","2023-06-13T17:55:36.861589: step 6788, loss 0.360884, acc 0.84375, learning_rate 0.000244978\n","2023-06-13T17:55:39.419077: step 6789, loss 0.551942, acc 0.9375, learning_rate 0.000244908\n","2023-06-13T17:55:41.848755: step 6790, loss 0.483497, acc 0.84375, learning_rate 0.000244838\n","2023-06-13T17:55:44.225335: step 6791, loss 0.615293, acc 0.84375, learning_rate 0.000244767\n","2023-06-13T17:55:45.824333: step 6792, loss 0.772214, acc 0.6875, learning_rate 0.000244697\n","2023-06-13T17:55:47.223253: step 6793, loss 0.526445, acc 0.84375, learning_rate 0.000244627\n","2023-06-13T17:55:48.648282: step 6794, loss 0.500791, acc 0.84375, learning_rate 0.000244557\n","2023-06-13T17:55:50.053852: step 6795, loss 0.369592, acc 0.875, learning_rate 0.000244487\n","2023-06-13T17:55:51.459430: step 6796, loss 0.951901, acc 0.71875, learning_rate 0.000244417\n","2023-06-13T17:55:52.879233: step 6797, loss 0.681913, acc 0.71875, learning_rate 0.000244347\n","2023-06-13T17:55:55.330320: step 6798, loss 0.471257, acc 0.78125, learning_rate 0.000244277\n","2023-06-13T17:55:57.724128: step 6799, loss 0.775538, acc 0.75, learning_rate 0.000244207\n","\n","Evaluation:\n","2023-06-13T17:56:26.537327: step 6800, loss 2.86154, acc 0.384965\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6800\n","\n","2023-06-13T17:56:28.151460: step 6800, loss 0.553964, acc 0.78125, learning_rate 0.000244137\n","2023-06-13T17:56:29.585630: step 6801, loss 0.347319, acc 0.90625, learning_rate 0.000244067\n","2023-06-13T17:56:31.062180: step 6802, loss 0.50198, acc 0.875, learning_rate 0.000243997\n","2023-06-13T17:56:32.511987: step 6803, loss 1.2078, acc 0.65625, learning_rate 0.000243927\n","2023-06-13T17:56:35.088484: step 6804, loss 0.424213, acc 0.8125, learning_rate 0.000243857\n","2023-06-13T17:56:37.353177: step 6805, loss 0.201065, acc 0.96875, learning_rate 0.000243788\n","2023-06-13T17:56:39.687482: step 6806, loss 0.26046, acc 0.90625, learning_rate 0.000243718\n","2023-06-13T17:56:42.014540: step 6807, loss 0.261601, acc 0.90625, learning_rate 0.000243648\n","2023-06-13T17:56:43.480128: step 6808, loss 0.545584, acc 0.875, learning_rate 0.000243579\n","2023-06-13T17:56:44.936513: step 6809, loss 0.46061, acc 0.84375, learning_rate 0.000243509\n","2023-06-13T17:56:46.359249: step 6810, loss 0.247232, acc 0.96875, learning_rate 0.000243439\n","2023-06-13T17:56:47.783327: step 6811, loss 0.61713, acc 0.8125, learning_rate 0.00024337\n","2023-06-13T17:56:49.216366: step 6812, loss 0.412068, acc 0.84375, learning_rate 0.0002433\n","2023-06-13T17:56:50.665709: step 6813, loss 0.533041, acc 0.8125, learning_rate 0.000243231\n","2023-06-13T17:56:52.080988: step 6814, loss 0.747963, acc 0.71875, learning_rate 0.000243161\n","2023-06-13T17:56:54.557389: step 6815, loss 0.576227, acc 0.8125, learning_rate 0.000243092\n","2023-06-13T17:56:56.963982: step 6816, loss 0.600625, acc 0.78125, learning_rate 0.000243023\n","2023-06-13T17:56:59.291440: step 6817, loss 0.378116, acc 0.84375, learning_rate 0.000242953\n","2023-06-13T17:57:01.477819: step 6818, loss 0.474502, acc 0.875, learning_rate 0.000242884\n","2023-06-13T17:57:02.945209: step 6819, loss 0.730054, acc 0.78125, learning_rate 0.000242815\n","2023-06-13T17:57:04.387686: step 6820, loss 0.555818, acc 0.6875, learning_rate 0.000242745\n","2023-06-13T17:57:05.839824: step 6821, loss 0.411439, acc 0.84375, learning_rate 0.000242676\n","2023-06-13T17:57:07.296371: step 6822, loss 0.437985, acc 0.84375, learning_rate 0.000242607\n","2023-06-13T17:57:08.718418: step 6823, loss 0.798958, acc 0.75, learning_rate 0.000242538\n","2023-06-13T17:57:10.149428: step 6824, loss 1.0609, acc 0.75, learning_rate 0.000242469\n","2023-06-13T17:57:11.923593: step 6825, loss 0.448775, acc 0.875, learning_rate 0.0002424\n","2023-06-13T17:57:14.377363: step 6826, loss 0.664489, acc 0.8125, learning_rate 0.000242331\n","2023-06-13T17:57:16.761218: step 6827, loss 0.624598, acc 0.71875, learning_rate 0.000242262\n","2023-06-13T17:57:18.890660: step 6828, loss 0.531256, acc 0.84375, learning_rate 0.000242193\n","2023-06-13T17:57:20.994085: step 6829, loss 0.434364, acc 0.90625, learning_rate 0.000242124\n","2023-06-13T17:57:22.456136: step 6830, loss 0.39278, acc 0.875, learning_rate 0.000242055\n","2023-06-13T17:57:23.875420: step 6831, loss 0.549509, acc 0.78125, learning_rate 0.000241986\n","2023-06-13T17:57:25.308345: step 6832, loss 0.303589, acc 0.96875, learning_rate 0.000241917\n","2023-06-13T17:57:26.757143: step 6833, loss 0.530581, acc 0.8125, learning_rate 0.000241848\n","2023-06-13T17:57:28.223819: step 6834, loss 0.649632, acc 0.78125, learning_rate 0.000241779\n","2023-06-13T17:57:29.688136: step 6835, loss 0.670011, acc 0.71875, learning_rate 0.000241711\n","2023-06-13T17:57:31.444002: step 6836, loss 0.897387, acc 0.71875, learning_rate 0.000241642\n","2023-06-13T17:57:33.988541: step 6837, loss 0.630543, acc 0.78125, learning_rate 0.000241573\n","2023-06-13T17:57:36.153281: step 6838, loss 0.538137, acc 0.78125, learning_rate 0.000241505\n","2023-06-13T17:57:38.579446: step 6839, loss 0.719081, acc 0.6875, learning_rate 0.000241436\n","2023-06-13T17:57:40.673294: step 6840, loss 0.617247, acc 0.8125, learning_rate 0.000241368\n","2023-06-13T17:57:42.161836: step 6841, loss 0.532731, acc 0.875, learning_rate 0.000241299\n","2023-06-13T17:57:43.603416: step 6842, loss 0.38443, acc 0.9375, learning_rate 0.00024123\n","2023-06-13T17:57:45.045014: step 6843, loss 0.5134, acc 0.8125, learning_rate 0.000241162\n","2023-06-13T17:57:46.468369: step 6844, loss 0.511239, acc 0.75, learning_rate 0.000241094\n","2023-06-13T17:57:47.908766: step 6845, loss 0.471684, acc 0.84375, learning_rate 0.000241025\n","2023-06-13T17:57:49.328844: step 6846, loss 0.631447, acc 0.75, learning_rate 0.000240957\n","2023-06-13T17:57:50.901904: step 6847, loss 0.503538, acc 0.8125, learning_rate 0.000240888\n","2023-06-13T17:57:53.361164: step 6848, loss 0.456541, acc 0.8125, learning_rate 0.00024082\n","2023-06-13T17:57:55.607970: step 6849, loss 0.380484, acc 0.90625, learning_rate 0.000240752\n","2023-06-13T17:57:57.712863: step 6850, loss 0.656197, acc 0.75, learning_rate 0.000240684\n","2023-06-13T17:58:00.162752: step 6851, loss 0.537932, acc 0.8125, learning_rate 0.000240615\n","2023-06-13T17:58:01.605232: step 6852, loss 0.522333, acc 0.8125, learning_rate 0.000240547\n","2023-06-13T17:58:03.033079: step 6853, loss 0.614462, acc 0.78125, learning_rate 0.000240479\n","2023-06-13T17:58:04.476108: step 6854, loss 0.292549, acc 0.90625, learning_rate 0.000240411\n","2023-06-13T17:58:05.876939: step 6855, loss 0.416401, acc 0.84375, learning_rate 0.000240343\n","2023-06-13T17:58:07.289879: step 6856, loss 0.261926, acc 0.9375, learning_rate 0.000240275\n","2023-06-13T17:58:08.712186: step 6857, loss 0.39723, acc 0.875, learning_rate 0.000240207\n","2023-06-13T17:58:10.165649: step 6858, loss 0.468372, acc 0.8125, learning_rate 0.000240139\n","2023-06-13T17:58:12.537091: step 6859, loss 0.519002, acc 0.84375, learning_rate 0.000240071\n","2023-06-13T17:58:15.175533: step 6860, loss 0.36328, acc 0.90625, learning_rate 0.000240003\n","2023-06-13T17:58:17.917616: step 6861, loss 0.436148, acc 0.8125, learning_rate 0.000239935\n","2023-06-13T17:58:20.497892: step 6862, loss 0.466834, acc 0.84375, learning_rate 0.000239867\n","2023-06-13T17:58:23.003905: step 6863, loss 0.532906, acc 0.8125, learning_rate 0.000239799\n","2023-06-13T17:58:25.330072: step 6864, loss 0.456856, acc 0.875, learning_rate 0.000239732\n","2023-06-13T17:58:27.442735: step 6865, loss 0.494666, acc 0.84375, learning_rate 0.000239664\n","2023-06-13T17:58:28.902022: step 6866, loss 0.599609, acc 0.78125, learning_rate 0.000239596\n","2023-06-13T17:58:30.340967: step 6867, loss 0.307677, acc 0.9375, learning_rate 0.000239528\n","2023-06-13T17:58:31.827690: step 6868, loss 0.429695, acc 0.84375, learning_rate 0.000239461\n","2023-06-13T17:58:33.276145: step 6869, loss 0.430163, acc 0.90625, learning_rate 0.000239393\n","2023-06-13T17:58:35.428432: step 6870, loss 0.391012, acc 0.8125, learning_rate 0.000239326\n","2023-06-13T17:58:37.895041: step 6871, loss 0.402891, acc 0.84375, learning_rate 0.000239258\n","2023-06-13T17:58:40.381171: step 6872, loss 0.794068, acc 0.875, learning_rate 0.00023919\n","2023-06-13T17:58:42.855519: step 6873, loss 0.523911, acc 0.84375, learning_rate 0.000239123\n","2023-06-13T17:58:44.377227: step 6874, loss 0.305787, acc 0.90625, learning_rate 0.000239055\n","2023-06-13T17:58:45.801661: step 6875, loss 0.236147, acc 0.9375, learning_rate 0.000238988\n","2023-06-13T17:58:47.216081: step 6876, loss 0.417008, acc 0.875, learning_rate 0.000238921\n","2023-06-13T17:58:48.662587: step 6877, loss 0.300406, acc 0.90625, learning_rate 0.000238853\n","2023-06-13T17:58:50.115694: step 6878, loss 0.967328, acc 0.75, learning_rate 0.000238786\n","2023-06-13T17:58:51.545550: step 6879, loss 0.507204, acc 0.84375, learning_rate 0.000238719\n","2023-06-13T17:58:52.978609: step 6880, loss 0.654648, acc 0.75, learning_rate 0.000238651\n","2023-06-13T17:58:55.351983: step 6881, loss 0.403038, acc 0.90625, learning_rate 0.000238584\n","2023-06-13T17:58:57.754269: step 6882, loss 0.429741, acc 0.84375, learning_rate 0.000238517\n","2023-06-13T17:59:00.278949: step 6883, loss 0.654733, acc 0.84375, learning_rate 0.00023845\n","2023-06-13T17:59:02.576417: step 6884, loss 0.650267, acc 0.75, learning_rate 0.000238383\n","2023-06-13T17:59:04.033242: step 6885, loss 0.477094, acc 0.84375, learning_rate 0.000238316\n","2023-06-13T17:59:05.480111: step 6886, loss 0.829493, acc 0.8125, learning_rate 0.000238249\n","2023-06-13T17:59:06.940230: step 6887, loss 0.488817, acc 0.84375, learning_rate 0.000238182\n","2023-06-13T17:59:08.388119: step 6888, loss 0.432259, acc 0.8125, learning_rate 0.000238115\n","2023-06-13T17:59:09.824964: step 6889, loss 0.496307, acc 0.875, learning_rate 0.000238048\n","2023-06-13T17:59:11.238061: step 6890, loss 0.518306, acc 0.8125, learning_rate 0.000237981\n","2023-06-13T17:59:12.823055: step 6891, loss 0.590584, acc 0.8125, learning_rate 0.000237914\n","2023-06-13T17:59:15.364020: step 6892, loss 0.372876, acc 0.875, learning_rate 0.000237847\n","2023-06-13T17:59:17.806443: step 6893, loss 0.491051, acc 0.875, learning_rate 0.00023778\n","2023-06-13T17:59:20.127365: step 6894, loss 0.765062, acc 0.75, learning_rate 0.000237713\n","2023-06-13T17:59:22.232535: step 6895, loss 0.559839, acc 0.875, learning_rate 0.000237646\n","2023-06-13T17:59:23.672045: step 6896, loss 0.440426, acc 0.84375, learning_rate 0.00023758\n","2023-06-13T17:59:25.113074: step 6897, loss 0.684701, acc 0.78125, learning_rate 0.000237513\n","2023-06-13T17:59:26.551363: step 6898, loss 0.324543, acc 0.84375, learning_rate 0.000237446\n","2023-06-13T17:59:28.001235: step 6899, loss 0.424328, acc 0.84375, learning_rate 0.00023738\n","\n","Evaluation:\n","2023-06-13T17:59:58.635681: step 6900, loss 2.90159, acc 0.382085\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-6900\n","\n","2023-06-13T18:00:01.290140: step 6900, loss 0.34063, acc 0.875, learning_rate 0.000237313\n","2023-06-13T18:00:02.766201: step 6901, loss 0.338504, acc 0.875, learning_rate 0.000237246\n","2023-06-13T18:00:05.215456: step 6902, loss 0.592613, acc 0.75, learning_rate 0.00023718\n","2023-06-13T18:00:07.678024: step 6903, loss 0.464378, acc 0.875, learning_rate 0.000237113\n","2023-06-13T18:00:10.243896: step 6904, loss 0.431641, acc 0.84375, learning_rate 0.000237047\n","2023-06-13T18:00:12.907470: step 6905, loss 0.69267, acc 0.84375, learning_rate 0.00023698\n","2023-06-13T18:00:15.377442: step 6906, loss 0.821548, acc 0.71875, learning_rate 0.000236914\n","2023-06-13T18:00:17.849193: step 6907, loss 0.488788, acc 0.875, learning_rate 0.000236848\n","2023-06-13T18:00:20.275227: step 6908, loss 0.730546, acc 0.78125, learning_rate 0.000236781\n","2023-06-13T18:00:22.046642: step 6909, loss 0.601392, acc 0.8125, learning_rate 0.000236715\n","2023-06-13T18:00:23.496162: step 6910, loss 0.579119, acc 0.78125, learning_rate 0.000236649\n","2023-06-13T18:00:24.928091: step 6911, loss 0.347754, acc 0.90625, learning_rate 0.000236582\n","2023-06-13T18:00:26.378315: step 6912, loss 0.205568, acc 0.9375, learning_rate 0.000236516\n","2023-06-13T18:00:27.846475: step 6913, loss 0.538173, acc 0.75, learning_rate 0.00023645\n","2023-06-13T18:00:29.299626: step 6914, loss 0.971896, acc 0.75, learning_rate 0.000236384\n","2023-06-13T18:00:30.786552: step 6915, loss 0.4658, acc 0.875, learning_rate 0.000236318\n","2023-06-13T18:00:33.136027: step 6916, loss 0.468381, acc 0.84375, learning_rate 0.000236252\n","2023-06-13T18:00:35.769509: step 6917, loss 0.215906, acc 0.90625, learning_rate 0.000236186\n","2023-06-13T18:00:38.248958: step 6918, loss 0.412849, acc 0.84375, learning_rate 0.00023612\n","2023-06-13T18:00:40.475872: step 6919, loss 0.420915, acc 0.875, learning_rate 0.000236054\n","2023-06-13T18:00:41.958968: step 6920, loss 0.553854, acc 0.78125, learning_rate 0.000235988\n","2023-06-13T18:00:43.413826: step 6921, loss 0.399639, acc 0.84375, learning_rate 0.000235922\n","2023-06-13T18:00:44.857775: step 6922, loss 0.58279, acc 0.84375, learning_rate 0.000235856\n","2023-06-13T18:00:46.302398: step 6923, loss 0.593082, acc 0.75, learning_rate 0.00023579\n","2023-06-13T18:00:47.758026: step 6924, loss 0.462008, acc 0.84375, learning_rate 0.000235724\n","2023-06-13T18:00:49.220394: step 6925, loss 0.678382, acc 0.78125, learning_rate 0.000235658\n","2023-06-13T18:00:51.117132: step 6926, loss 0.581799, acc 0.8125, learning_rate 0.000235592\n","2023-06-13T18:00:53.656958: step 6927, loss 0.505797, acc 0.875, learning_rate 0.000235527\n","2023-06-13T18:00:56.052106: step 6928, loss 0.478358, acc 0.875, learning_rate 0.000235461\n","2023-06-13T18:00:58.538595: step 6929, loss 0.587662, acc 0.71875, learning_rate 0.000235395\n","2023-06-13T18:01:00.272406: step 6930, loss 0.461421, acc 0.875, learning_rate 0.00023533\n","2023-06-13T18:01:01.786334: step 6931, loss 0.962773, acc 0.8125, learning_rate 0.000235264\n","2023-06-13T18:01:03.279138: step 6932, loss 0.251572, acc 0.9375, learning_rate 0.000235198\n","2023-06-13T18:01:04.749498: step 6933, loss 0.269581, acc 0.90625, learning_rate 0.000235133\n","2023-06-13T18:01:06.230404: step 6934, loss 0.520609, acc 0.8125, learning_rate 0.000235067\n","2023-06-13T18:01:07.681681: step 6935, loss 1.0822, acc 0.65625, learning_rate 0.000235002\n","2023-06-13T18:01:09.146468: step 6936, loss 0.259668, acc 0.90625, learning_rate 0.000234936\n","2023-06-13T18:01:11.548868: step 6937, loss 0.434584, acc 0.8125, learning_rate 0.000234871\n","2023-06-13T18:01:13.999584: step 6938, loss 0.786842, acc 0.75, learning_rate 0.000234806\n","2023-06-13T18:01:16.399925: step 6939, loss 0.474646, acc 0.8125, learning_rate 0.00023474\n","2023-06-13T18:01:18.724243: step 6940, loss 0.453044, acc 0.84375, learning_rate 0.000234675\n","2023-06-13T18:01:20.186799: step 6941, loss 0.640612, acc 0.78125, learning_rate 0.00023461\n","2023-06-13T18:01:21.612698: step 6942, loss 0.519461, acc 0.8125, learning_rate 0.000234544\n","2023-06-13T18:01:23.078264: step 6943, loss 0.482297, acc 0.875, learning_rate 0.000234479\n","2023-06-13T18:01:24.518947: step 6944, loss 0.845465, acc 0.75, learning_rate 0.000234414\n","2023-06-13T18:01:25.986082: step 6945, loss 0.426402, acc 0.90625, learning_rate 0.000234349\n","2023-06-13T18:01:27.460022: step 6946, loss 0.270406, acc 0.96875, learning_rate 0.000234284\n","2023-06-13T18:01:29.114739: step 6947, loss 0.501107, acc 0.875, learning_rate 0.000234218\n","2023-06-13T18:01:31.747017: step 6948, loss 0.259517, acc 0.90625, learning_rate 0.000234153\n","2023-06-13T18:01:34.117313: step 6949, loss 0.29235, acc 0.90625, learning_rate 0.000234088\n","2023-06-13T18:01:36.420077: step 6950, loss 0.415902, acc 0.84375, learning_rate 0.000234023\n","2023-06-13T18:01:38.530936: step 6951, loss 0.570503, acc 0.8125, learning_rate 0.000233958\n","2023-06-13T18:01:40.002583: step 6952, loss 0.530852, acc 0.875, learning_rate 0.000233893\n","2023-06-13T18:01:41.466169: step 6953, loss 0.503507, acc 0.84375, learning_rate 0.000233828\n","2023-06-13T18:01:42.957696: step 6954, loss 0.486227, acc 0.84375, learning_rate 0.000233764\n","2023-06-13T18:01:44.428278: step 6955, loss 0.797872, acc 0.84375, learning_rate 0.000233699\n","2023-06-13T18:01:45.896020: step 6956, loss 0.374729, acc 0.9375, learning_rate 0.000233634\n","2023-06-13T18:01:47.310455: step 6957, loss 0.487779, acc 0.8125, learning_rate 0.000233569\n","2023-06-13T18:01:49.260824: step 6958, loss 0.245643, acc 0.90625, learning_rate 0.000233504\n","2023-06-13T18:01:51.841972: step 6959, loss 0.417042, acc 0.84375, learning_rate 0.00023344\n","2023-06-13T18:01:54.263314: step 6960, loss 0.381913, acc 0.8125, learning_rate 0.000233375\n","2023-06-13T18:01:56.622133: step 6961, loss 0.713222, acc 0.75, learning_rate 0.00023331\n","2023-06-13T18:01:58.228628: step 6962, loss 0.589678, acc 0.8125, learning_rate 0.000233246\n","2023-06-13T18:01:59.659650: step 6963, loss 0.993311, acc 0.75, learning_rate 0.000233181\n","2023-06-13T18:02:01.190536: step 6964, loss 0.291742, acc 0.90625, learning_rate 0.000233116\n","2023-06-13T18:02:02.666690: step 6965, loss 0.403518, acc 0.90625, learning_rate 0.000233052\n","2023-06-13T18:02:04.095246: step 6966, loss 0.653752, acc 0.75, learning_rate 0.000232987\n","2023-06-13T18:02:05.521309: step 6967, loss 0.554671, acc 0.8125, learning_rate 0.000232923\n","2023-06-13T18:02:06.953167: step 6968, loss 0.780146, acc 0.75, learning_rate 0.000232858\n","2023-06-13T18:02:09.409066: step 6969, loss 0.286357, acc 0.90625, learning_rate 0.000232794\n","2023-06-13T18:02:11.879663: step 6970, loss 0.375022, acc 0.84375, learning_rate 0.00023273\n","2023-06-13T18:02:14.245742: step 6971, loss 0.477892, acc 0.875, learning_rate 0.000232665\n","2023-06-13T18:02:16.529904: step 6972, loss 0.528904, acc 0.78125, learning_rate 0.000232601\n","2023-06-13T18:02:17.984948: step 6973, loss 0.467329, acc 0.8125, learning_rate 0.000232537\n","2023-06-13T18:02:19.492880: step 6974, loss 0.344372, acc 0.9375, learning_rate 0.000232472\n","2023-06-13T18:02:20.938047: step 6975, loss 0.470599, acc 0.9375, learning_rate 0.000232408\n","2023-06-13T18:02:22.399612: step 6976, loss 0.476147, acc 0.875, learning_rate 0.000232344\n","2023-06-13T18:02:23.819946: step 6977, loss 0.33641, acc 0.9375, learning_rate 0.00023228\n","2023-06-13T18:02:25.287464: step 6978, loss 0.500412, acc 0.84375, learning_rate 0.000232216\n","2023-06-13T18:02:26.996670: step 6979, loss 0.340167, acc 0.90625, learning_rate 0.000232151\n","2023-06-13T18:02:29.607970: step 6980, loss 0.625876, acc 0.84375, learning_rate 0.000232087\n","2023-06-13T18:02:32.071189: step 6981, loss 0.358847, acc 0.875, learning_rate 0.000232023\n","2023-06-13T18:02:34.383602: step 6982, loss 0.479235, acc 0.78125, learning_rate 0.000231959\n","2023-06-13T18:02:36.400067: step 6983, loss 0.715013, acc 0.71875, learning_rate 0.000231895\n","2023-06-13T18:02:37.894660: step 6984, loss 0.819171, acc 0.78125, learning_rate 0.000231831\n","2023-06-13T18:02:39.341299: step 6985, loss 0.321196, acc 0.875, learning_rate 0.000231767\n","2023-06-13T18:02:40.795468: step 6986, loss 0.401662, acc 0.84375, learning_rate 0.000231704\n","2023-06-13T18:02:42.261818: step 6987, loss 0.702567, acc 0.75, learning_rate 0.00023164\n","2023-06-13T18:02:44.628713: step 6988, loss 0.408262, acc 0.8125, learning_rate 0.000231576\n","2023-06-13T18:02:47.208231: step 6989, loss 0.778385, acc 0.6875, learning_rate 0.000231512\n","2023-06-13T18:02:49.944221: step 6990, loss 0.481643, acc 0.84375, learning_rate 0.000231448\n","2023-06-13T18:02:52.642299: step 6991, loss 0.222216, acc 0.9375, learning_rate 0.000231385\n","2023-06-13T18:02:55.222061: step 6992, loss 0.353104, acc 0.8125, learning_rate 0.000231321\n","2023-06-13T18:02:57.668015: step 6993, loss 0.473836, acc 0.84375, learning_rate 0.000231257\n","2023-06-13T18:02:59.789508: step 6994, loss 0.517916, acc 0.84375, learning_rate 0.000231194\n","2023-06-13T18:03:01.236137: step 6995, loss 0.179302, acc 0.96875, learning_rate 0.00023113\n","2023-06-13T18:03:02.720330: step 6996, loss 0.40154, acc 0.8125, learning_rate 0.000231066\n","2023-06-13T18:03:04.187256: step 6997, loss 0.57413, acc 0.8125, learning_rate 0.000231003\n","2023-06-13T18:03:05.650492: step 6998, loss 0.477736, acc 0.8125, learning_rate 0.000230939\n","2023-06-13T18:03:07.115369: step 6999, loss 0.9357, acc 0.78125, learning_rate 0.000230876\n","\n","Evaluation:\n","2023-06-13T18:03:38.625307: step 7000, loss 2.92297, acc 0.382843\n","\n","Saved model checkpoint to /content/gdrive/MyDrive/LightWeight/train/runs/1686664319/checkpoints/model-7000\n","\n","2023-06-13T18:03:40.879508: step 7000, loss 0.489024, acc 0.84375, learning_rate 0.000230812\n","2023-06-13T18:03:42.351713: step 7001, loss 0.495416, acc 0.78125, learning_rate 0.000230749\n","2023-06-13T18:03:43.901567: step 7002, loss 0.997757, acc 0.78125, learning_rate 0.000230685\n","2023-06-13T18:03:45.415200: step 7003, loss 0.609752, acc 0.8125, learning_rate 0.000230622\n","2023-06-13T18:03:46.910145: step 7004, loss 0.359466, acc 0.90625, learning_rate 0.000230559\n","2023-06-13T18:03:48.374273: step 7005, loss 0.63692, acc 0.78125, learning_rate 0.000230495\n","2023-06-13T18:03:49.855188: step 7006, loss 0.883866, acc 0.71875, learning_rate 0.000230432\n","2023-06-13T18:03:52.203671: step 7007, loss 0.794651, acc 0.75, learning_rate 0.000230369\n","2023-06-13T18:03:54.504873: step 7008, loss 0.311246, acc 0.90625, learning_rate 0.000230306\n","2023-06-13T18:03:56.877437: step 7009, loss 0.486755, acc 0.8125, learning_rate 0.000230243\n","2023-06-13T18:03:59.293345: step 7010, loss 0.789116, acc 0.78125, learning_rate 0.000230179\n","2023-06-13T18:04:00.930885: step 7011, loss 0.55429, acc 0.78125, learning_rate 0.000230116\n","2023-06-13T18:04:02.399556: step 7012, loss 0.661827, acc 0.8125, learning_rate 0.000230053\n","2023-06-13T18:04:03.861697: step 7013, loss 0.394265, acc 0.84375, learning_rate 0.00022999\n","2023-06-13T18:04:05.302323: step 7014, loss 0.548441, acc 0.8125, learning_rate 0.000229927\n","2023-06-13T18:04:06.752380: step 7015, loss 0.379479, acc 0.84375, learning_rate 0.000229864\n","2023-06-13T18:04:08.221062: step 7016, loss 0.47453, acc 0.875, learning_rate 0.000229801\n","2023-06-13T18:04:09.662684: step 7017, loss 0.369663, acc 0.84375, learning_rate 0.000229738\n","2023-06-13T18:04:11.993976: step 7018, loss 0.7862, acc 0.78125, learning_rate 0.000229675\n","2023-06-13T18:04:14.474805: step 7019, loss 0.250705, acc 0.9375, learning_rate 0.000229612\n","2023-06-13T18:04:16.873248: step 7020, loss 0.317857, acc 0.84375, learning_rate 0.00022955\n","2023-06-13T18:04:19.172435: step 7021, loss 0.306901, acc 0.9375, learning_rate 0.000229487\n","2023-06-13T18:04:20.750330: step 7022, loss 0.242931, acc 0.9375, learning_rate 0.000229424\n","2023-06-13T18:04:22.184375: step 7023, loss 0.371907, acc 0.90625, learning_rate 0.000229361\n","2023-06-13T18:04:23.622280: step 7024, loss 1.27255, acc 0.71875, learning_rate 0.000229299\n","2023-06-13T18:04:25.089639: step 7025, loss 0.470428, acc 0.84375, learning_rate 0.000229236\n","2023-06-13T18:04:26.554054: step 7026, loss 0.380412, acc 0.90625, learning_rate 0.000229173\n","2023-06-13T18:04:27.987937: step 7027, loss 0.395915, acc 0.875, learning_rate 0.000229111\n","2023-06-13T18:04:29.442725: step 7028, loss 0.42914, acc 0.84375, learning_rate 0.000229048\n","2023-06-13T18:04:31.858102: step 7029, loss 0.626663, acc 0.78125, learning_rate 0.000228985\n","2023-06-13T18:04:34.317020: step 7030, loss 0.56658, acc 0.875, learning_rate 0.000228923\n","2023-06-13T18:04:36.994695: step 7031, loss 0.5449, acc 0.8125, learning_rate 0.00022886\n","2023-06-13T18:04:39.749304: step 7032, loss 0.625333, acc 0.78125, learning_rate 0.000228798\n","2023-06-13T18:04:42.331924: step 7033, loss 0.351791, acc 0.875, learning_rate 0.000228735\n","2023-06-13T18:04:44.652694: step 7034, loss 0.552167, acc 0.84375, learning_rate 0.000228673\n","2023-06-13T18:04:47.178650: step 7035, loss 0.530438, acc 0.84375, learning_rate 0.000228611\n","2023-06-13T18:04:48.792673: step 7036, loss 0.756444, acc 0.8125, learning_rate 0.000228548\n","2023-06-13T18:04:50.277679: step 7037, loss 0.58467, acc 0.8125, learning_rate 0.000228486\n","2023-06-13T18:04:51.756339: step 7038, loss 1.00955, acc 0.65625, learning_rate 0.000228424\n","2023-06-13T18:04:53.519120: step 7039, loss 0.586278, acc 0.8125, learning_rate 0.000228361\n","2023-06-13T18:04:56.085738: step 7040, loss 0.315315, acc 0.84375, learning_rate 0.000228299\n","2023-06-13T18:04:58.428952: step 7041, loss 0.375755, acc 0.875, learning_rate 0.000228237\n","2023-06-13T18:05:00.844020: step 7042, loss 0.527463, acc 0.84375, learning_rate 0.000228175\n","2023-06-13T18:05:02.985475: step 7043, loss 0.529607, acc 0.875, learning_rate 0.000228113\n","2023-06-13T18:05:04.455682: step 7044, loss 0.602123, acc 0.84375, learning_rate 0.00022805\n","2023-06-13T18:05:05.972515: step 7045, loss 0.442059, acc 0.75, learning_rate 0.000227988\n","2023-06-13T18:05:07.471458: step 7046, loss 0.897213, acc 0.75, learning_rate 0.000227926\n","2023-06-13T18:05:09.005429: step 7047, loss 0.487794, acc 0.84375, learning_rate 0.000227864\n","2023-06-13T18:05:10.484169: step 7048, loss 0.594583, acc 0.84375, learning_rate 0.000227802\n","2023-06-13T18:05:12.000628: step 7049, loss 0.388209, acc 0.875, learning_rate 0.00022774\n","2023-06-13T18:05:14.124684: step 7050, loss 0.697834, acc 0.75, learning_rate 0.000227678\n","2023-06-13T18:05:16.647265: step 7051, loss 0.374272, acc 0.84375, learning_rate 0.000227616\n","2023-06-13T18:05:19.012942: step 7052, loss 0.436231, acc 0.875, learning_rate 0.000227555\n","2023-06-13T18:05:21.370307: step 7053, loss 0.483848, acc 0.8125, learning_rate 0.000227493\n","Traceback (most recent call last):\n","  File \"singleADAM_LW_train.py\", line 233, in <module>\n","    train_step(x_batch, y_batch, learning_rate)\n","  File \"singleADAM_LW_train.py\", line 192, in train_step\n","    feed_dict)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n","    options, feed_dict, fetch_list, target_list, run_metadata)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"]}]}]}